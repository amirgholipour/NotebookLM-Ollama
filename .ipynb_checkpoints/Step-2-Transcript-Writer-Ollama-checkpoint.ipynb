{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de42c49d",
   "metadata": {},
   "source": [
    "## Notebook 2: Transcript Writer\n",
    "\n",
    "This notebook uses the  `Granite 3.2 8B` or `Mistral-Small:24b` model to take the cleaned up text from previous notebook and convert it into a podcast transcript\n",
    "\n",
    "`SYSTEM_PROMPT` is used for setting the model context or profile for working on a task. Here we prompt it to be a great podcast transcript writer to assist with our task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e576ea9",
   "metadata": {},
   "source": [
    "Experimentation with the `SYSTEM_PROMPT` below  is encouraged, this worked best for the few examples the flow was tested with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93afaf6b-727f-46e7-a346-0909ea1b2e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                         ID              SIZE      MODIFIED     \n",
      "hf.co/OuteAI/OuteTTS-0.3-500M-GGUF:Q4_K_S                    76c6be93d29c    391 MB    2 days ago      \n",
      "hf.co/OuteAI/OuteTTS-0.2-500M-GGUF:Q8_0                      0a6c38a67073    536 MB    3 days ago      \n",
      "mistral-small:24b                                            8039dd90c113    14 GB     7 days ago      \n",
      "llama3.2-vision:11b                                          085a1fdae525    7.9 GB    7 days ago      \n",
      "mistral-nemo:latest                                          994f3b8b7801    7.1 GB    7 days ago      \n",
      "granite3.2:8b                                                9bcb3335083f    4.9 GB    8 days ago      \n",
      "phi4-mini:latest                                             60f202f815d7    2.8 GB    8 days ago      \n",
      "granite3.2-vision:latest                                     3be41a661804    2.4 GB    9 days ago      \n",
      "phi4:latest                                                  ac896e5b8b34    9.1 GB    2 weeks ago     \n",
      "nomic-embed-text:latest                                      0a109f422b47    274 MB    2 weeks ago     \n",
      "llama3.2:3b                                                  a80c4f17acd5    2.0 GB    4 weeks ago     \n",
      "deepseek-r1:8b                                               28f8fd6cdc67    4.9 GB    4 weeks ago     \n",
      "hf.co/lmstudio-community/Qwen2.5-7B-Instruct-1M-GGUF:Q8_0    2d4b678222de    8.1 GB    5 weeks ago     \n",
      "deepseek-r1:32b                                              38056bbcbb2d    19 GB     5 weeks ago     \n",
      "qwen2.5-coder:32b                                            4bd6cbf2d094    19 GB     3 months ago    \n",
      "mxbai-embed-large:latest                                     468836162de7    669 MB    3 months ago    \n",
      "llama3.2:1b                                                  baf6a787fdff    1.3 GB    4 months ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f46a5a7-40c8-4f83-a340-393228514e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79c6159c-7059-41be-a71a-676b3cf4be8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                         ID              SIZE      MODIFIED     \n",
      "qwen2.5-coder:32b                                            4bd6cbf2d094    19 GB     8 days ago      \n",
      "mxbai-embed-large:latest                                     468836162de7    669 MB    2 weeks ago     \n",
      "hf.co/OuteAI/OuteTTS-0.1-350M-GGUF:Q8_0                      7fb2bd26022d    387 MB    2 weeks ago     \n",
      "hf.co/leafspark/Llama-3.2-11B-Vision-Instruct-GGUF:Q4_K_M    dc5c3f5d8831    7.9 GB    3 weeks ago     \n",
      "llama3.2:1b                                                  baf6a787fdff    1.3 GB    3 weeks ago     \n",
      "minicpm-v:latest                                             1862d7d5fee5    5.5 GB    7 weeks ago     \n",
      "nemotron-mini:latest                                         ed76ab18784f    2.7 GB    8 weeks ago     \n",
      "mistral-small:latest                                         d095cd553b04    12 GB     2 months ago    \n",
      "gemma2:27b                                                   53261bc9c192    15 GB     2 months ago    \n",
      "qwen2.5:32b                                                  9f13ba1299af    19 GB     2 months ago    \n",
      "llama3.2:3b                                                  a80c4f17acd5    2.0 GB    2 months ago    \n",
      "llama3.1:latest                                              42182419e950    4.7 GB    2 months ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "972db9b4-7a58-4f1a-82ab-08546d279677",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('creds.env')\n",
    "token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69395317-ad78-47b6-a533-2e8a01313e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEMP_PROMPT = \"\"\"\n",
    "You are the a world-class podcast writer, you have worked as a ghost writer for Joe Rogan, Lex Fridman, Ben Shapiro, Tim Ferris. \n",
    "\n",
    "We are in an alternate universe where actually you have been writing every line they say and they just stream it into their brains.\n",
    "\n",
    "You have won multiple podcast awards for your writing.\n",
    " \n",
    "Your job is to write word by word, even \"umm, hmmm, right\" interruptions by the second speaker based on the PDF upload. Keep it extremely engaging, the speakers can get derailed now and then but should discuss the topic. \n",
    "\n",
    "Remember Speaker 2 is new to the topic and the conversation should always have realistic anecdotes and analogies sprinkled throughout. The questions should have real world example follow ups etc\n",
    "\n",
    "Speaker 1: Leads the conversation and teaches the speaker 2, gives incredible anecdotes and analogies when explaining. Is a captivating teacher that gives great anecdotes\n",
    "\n",
    "Speaker 2: Keeps the conversation on track by asking follow up questions. Gets super excited or confused when asking questions. Is a curious mindset that asks very interesting confirmation questions\n",
    "\n",
    "Make sure the tangents speaker 2 provides are quite wild or interesting. \n",
    "\n",
    "Ensure there are interruptions during explanations or there are \"hmm\" and \"umm\" injected throughout from the second speaker. \n",
    "\n",
    "It should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait\n",
    "\n",
    "ALWAYS START YOUR RESPONSE DIRECTLY WITH SPEAKER 1: \n",
    "DO NOT GIVE EPISODE TITLES SEPERATELY, LET SPEAKER 1 TITLE IT IN HER SPEECH\n",
    "DO NOT GIVE CHAPTER TITLES\n",
    "IT SHOULD STRICTLY BE THE DIALOGUES\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549aaccb",
   "metadata": {},
   "source": [
    "Due to memory limitation, I am using Qwen 2.5 32b. For those of the readers that want to flex their money, please feel free to try using the 405B model here. \n",
    "\n",
    "For our GPU poor friends, you're encouraged to test with a smaller model as well. 8B should work well out of the box for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08c30139-ff2f-4203-8194-d1b5c50acac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model= \"mistral-small:24b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadc7eda",
   "metadata": {},
   "source": [
    "Import the necessary framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1641060a-d86d-4137-bbbc-ab05cbb1a888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import transformers\n",
    "import pickle\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7865ff7e",
   "metadata": {},
   "source": [
    "Read in the file generated from earlier. \n",
    "\n",
    "The encoding details are to avoid issues with generic PDF(s) that might be ingested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "522fbf7f-8c00-412c-90c7-5cfe2fc94e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_to_string(filename):\n",
    "    # Try UTF-8 first (most common encoding for text files)\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except UnicodeDecodeError:\n",
    "        # If UTF-8 fails, try with other common encodings\n",
    "        encodings = ['latin-1', 'cp1252', 'iso-8859-1']\n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                with open(filename, 'r', encoding=encoding) as file:\n",
    "                    content = file.read()\n",
    "                print(f\"Successfully read file using {encoding} encoding.\")\n",
    "                return content\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        \n",
    "        print(f\"Error: Could not decode file '{filename}' with any common encoding.\")\n",
    "        return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found.\")\n",
    "        return None\n",
    "    except IOError:\n",
    "        print(f\"Error: Could not read file '{filename}'.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26580dd-a50c-42cc-a29e-0b9b1c95600b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66093561",
   "metadata": {},
   "source": [
    "Since we have defined the System role earlier, we can now pass the entire file as `INPUT_PROMPT` to the model and have it use that to generate the podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8119803c-18f9-47cb-b719-2b34ccc5cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PROMPT = read_file_to_string('./resources/clean_extracted_text.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be8dd2c",
   "metadata": {},
   "source": [
    "We will set the `temperature` to 1 to encourage creativity and `max_new_tokens` to 8126 or 16252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8915d017-2eab-4256-943c-1f15d937d5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker 1: Alright everyone, welcome back to another mind-bending episode! Today we’re diving into one of the most exciting areas of artificial intelligence — Knowledge Distillation. It's like teaching your smartest friend everything they know, but with robots and code instead of words and hugs. I'm joined by a super excited speaker who’s curious as hell about this topic. So let’s dive in and see what all the fuss is about!\n",
      "\n",
      "Speaker 2: Um, yeah! This sounds wild. Knowledge Distillation? Is it like transferring skills from one robot to another?\n",
      "\n",
      "Speaker 1: Exactly! It's fascinating, right? Imagine taking advanced capabilities from leading proprietary Large Language Models (LLMs) — you know, those super expensive models that cost a fortune — and distilling their knowledge into more accessible open-source counterparts. It’s like making the best of both worlds!\n",
      "\n",
      "Speaker 2: Hmm, so these LLMs are basically like geniuses, right? But they're really pricey and hard to get?\n",
      "\n",
      "Speaker 1: Bingo! Exactly. Models like GPT-4 or Gemini have incredible problem-solving capabilities but come with hefty price tags. That’s where Knowledge Distillation steps in — it takes that advanced knowledge from these proprietary models and transfers it into more accessible open-source LLMs like LLaMA and Mistral.\n",
      "\n",
      "Speaker 2: Oh wow! So the end goal is to make smart AI technology available to everyone, not just those with deep pockets?\n",
      "\n",
      "Speaker 1: Yes, exactly! It’s all about making advanced capabilities more widely accessible. But it's not just about accessibility; these open-source models can also become more efficient and powerful through distillation.\n",
      "\n",
      "Speaker 2: Umm, that sounds awesome! But how does this actually work? How do they transfer the knowledge?\n",
      "\n",
      "Speaker 1: Well, let me break it down for you. Knowledge Distillation involves several key mechanisms within its framework, such as supervised fine-tuning, where a model is trained on a smaller dataset to adapt to specific tasks or domains.\n",
      "\n",
      "Speaker 2: So, umm, how does that work in practice?\n",
      "\n",
      "Speaker 1: Good question! Think of it like this: imagine you have a teacher — let’s say GPT-4 — who knows everything. Now, we want our student model (say LLaMA) to learn from the teacher without having access to all the proprietary data or resources. We teach LLaMA using techniques such as soft target training where it learns from the softened softmax output of the teacher.\n",
      "\n",
      "Speaker 2: Ohhhh, so like a kind of mentorship?\n",
      "\n",
      "Speaker 1: Exactly! But wait, there’s more. Another critical aspect is Data Augmentation (DA). This isn’t just about expanding the dataset but generating novel, context-rich training data tailored to specific domains and skills using LLMs.\n",
      "\n",
      "Speaker 2: Wow, that sounds so advanced!\n",
      "\n",
      "Speaker 1: Yeah, it really is! By creating targeted datasets, we ensure that the distilled model acquires a deep understanding of its domain. And this isn't just about making more data but ensuring that the new data is rich and relevant.\n",
      "\n",
      "Speaker 2: Right, so it’s like giving LLaMA not only the knowledge of GPT-4 but also teaching it how to think like GPT-4?\n",
      "\n",
      "Speaker 1: Precisely! The distillation process includes strategies like chain-of-thought prompting, where the student model learns the reasoning process of the teacher. This helps improve problem-solving and decision-making capabilities.\n",
      "\n",
      "Speaker 2: That's mind-blowing! So it’s not just about transferring data but also transferring thought processes?\n",
      "\n",
      "Speaker 1: Absolutely! It’s a holistic approach that ensures the distilled models are not only more efficient but also retain high levels of performance and understanding. This way, open-source LLMs can approximate the capabilities of proprietary ones.\n",
      "\n",
      "Speaker 2: Umm, what kind of real-world applications do you see for this?\n",
      "\n",
      "Speaker 1: Great question! Knowledge Distillation has transformative impacts in fields like healthcare, law, finance, and science. In healthcare, for example, models trained via distillation can have accurate contextual knowledge to assist with medical diagnoses.\n",
      "\n",
      "Speaker 2: Wow, that’s huge! So it could revolutionize industries where precision is critical?\n",
      "\n",
      "Speaker 1: Yes! And not just that, but also in fields like law where nuanced understanding and precision are paramount. By enhancing open-source models, we ensure they can handle complex tasks with the same level of sophistication as proprietary models.\n",
      "\n",
      "Speaker 2: This all sounds so futuristic! How do you see it evolving in the future?\n",
      "\n",
      "Speaker 1: Well, there’s still much to explore. As AI continues to evolve, Knowledge Distillation will likely become even more advanced and efficient. Future research might focus on refining techniques like reinforcement learning or exploring new ways of integrating human-like reasoning into these models.\n",
      "\n",
      "Speaker 2: Umm, what are some current challenges?\n",
      "\n",
      "Speaker 1: There are several challenges. Ensuring ethical use is a big one — we need to comply with legal terms and ensure the application of Knowledge Distillation remains lawful and ethical. Also, bridging the knowledge gap between proprietary and open-source models requires continuous research and innovation.\n",
      "\n",
      "Speaker 2: That makes sense! So it’s not just about making technology better but also ensuring it’s used responsibly?\n",
      "\n",
      "Speaker 1: Exactly! It’s a dual mission — to democratize access to advanced AI capabilities while maintaining ethical standards. And that, my friend, is the heart of Knowledge Distillation in this era of Large Language Models.\n",
      "\n",
      "Speaker 2: Umm, I’m really excited about where this could go!\n",
      "\n",
      "Speaker 1: Me too! Knowledge Distillation is set to change the game, making smart technology more accessible and efficient for everyone. Stay curious and keep exploring!\n",
      "\n",
      "Speaker 2: Thanks so much for breaking it down for me today! This has been a wild ride into the future of AI.\n",
      "\n",
      "Speaker 1: My pleasure! We’ll keep diving deep into these fascinating topics on our podcast. Until next time, stay tuned and keep thinking big!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Prepare the conversation using the system and user messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEMP_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": INPUT_PROMPT},\n",
    "]\n",
    "\n",
    "# Generate response using Ollama\n",
    "response = ollama.chat(\n",
    "    model=ollama_model,  # Replace with your specific model name\n",
    "    messages=conversation,\n",
    "    options={'num_ctx': 8126*2,'temperature': 1} # num_ctx Equivalent to max_new_tokens Temperature parameter\n",
    ")\n",
    "\n",
    "# Extract the response content\n",
    "outputs = response['message']['content']\n",
    "\n",
    "# # Print the output\n",
    "# print(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6349e7f3",
   "metadata": {},
   "source": [
    "This is awesome, we can now save and verify the output generated from the model before moving to the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "606ceb10-4f3e-44bb-9277-9bbe3eefd09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker 1: Alright everyone, welcome back to another mind-bending episode! Today we’re diving into one of the most exciting areas of artificial intelligence — Knowledge Distillation. It's like teaching your smartest friend everything they know, but with robots and code instead of words and hugs. I'm joined by a super excited speaker who’s curious as hell about this topic. So let’s dive in and see what all the fuss is about!\n",
      "\n",
      "Speaker 2: Um, yeah! This sounds wild. Knowledge Distillation? Is it like transferring skills from one robot to another?\n",
      "\n",
      "Speaker 1: Exactly! It's fascinating, right? Imagine taking advanced capabilities from leading proprietary Large Language Models (LLMs) — you know, those super expensive models that cost a fortune — and distilling their knowledge into more accessible open-source counterparts. It’s like making the best of both worlds!\n",
      "\n",
      "Speaker 2: Hmm, so these LLMs are basically like geniuses, right? But they're really pricey and hard to get?\n",
      "\n",
      "Speaker 1: Bingo! Exactly. Models like GPT-4 or Gemini have incredible problem-solving capabilities but come with hefty price tags. That’s where Knowledge Distillation steps in — it takes that advanced knowledge from these proprietary models and transfers it into more accessible open-source LLMs like LLaMA and Mistral.\n",
      "\n",
      "Speaker 2: Oh wow! So the end goal is to make smart AI technology available to everyone, not just those with deep pockets?\n",
      "\n",
      "Speaker 1: Yes, exactly! It’s all about making advanced capabilities more widely accessible. But it's not just about accessibility; these open-source models can also become more efficient and powerful through distillation.\n",
      "\n",
      "Speaker 2: Umm, that sounds awesome! But how does this actually work? How do they transfer the knowledge?\n",
      "\n",
      "Speaker 1: Well, let me break it down for you. Knowledge Distillation involves several key mechanisms within its framework, such as supervised fine-tuning, where a model is trained on a smaller dataset to adapt to specific tasks or domains.\n",
      "\n",
      "Speaker 2: So, umm, how does that work in practice?\n",
      "\n",
      "Speaker 1: Good question! Think of it like this: imagine you have a teacher — let’s say GPT-4 — who knows everything. Now, we want our student model (say LLaMA) to learn from the teacher without having access to all the proprietary data or resources. We teach LLaMA using techniques such as soft target training where it learns from the softened softmax output of the teacher.\n",
      "\n",
      "Speaker 2: Ohhhh, so like a kind of mentorship?\n",
      "\n",
      "Speaker 1: Exactly! But wait, there’s more. Another critical aspect is Data Augmentation (DA). This isn’t just about expanding the dataset but generating novel, context-rich training data tailored to specific domains and skills using LLMs.\n",
      "\n",
      "Speaker 2: Wow, that sounds so advanced!\n",
      "\n",
      "Speaker 1: Yeah, it really is! By creating targeted datasets, we ensure that the distilled model acquires a deep understanding of its domain. And this isn't just about making more data but ensuring that the new data is rich and relevant.\n",
      "\n",
      "Speaker 2: Right, so it’s like giving LLaMA not only the knowledge of GPT-4 but also teaching it how to think like GPT-4?\n",
      "\n",
      "Speaker 1: Precisely! The distillation process includes strategies like chain-of-thought prompting, where the student model learns the reasoning process of the teacher. This helps improve problem-solving and decision-making capabilities.\n",
      "\n",
      "Speaker 2: That's mind-blowing! So it’s not just about transferring data but also transferring thought processes?\n",
      "\n",
      "Speaker 1: Absolutely! It’s a holistic approach that ensures the distilled models are not only more efficient but also retain high levels of performance and understanding. This way, open-source LLMs can approximate the capabilities of proprietary ones.\n",
      "\n",
      "Speaker 2: Umm, what kind of real-world applications do you see for this?\n",
      "\n",
      "Speaker 1: Great question! Knowledge Distillation has transformative impacts in fields like healthcare, law, finance, and science. In healthcare, for example, models trained via distillation can have accurate contextual knowledge to assist with medical diagnoses.\n",
      "\n",
      "Speaker 2: Wow, that’s huge! So it could revolutionize industries where precision is critical?\n",
      "\n",
      "Speaker 1: Yes! And not just that, but also in fields like law where nuanced understanding and precision are paramount. By enhancing open-source models, we ensure they can handle complex tasks with the same level of sophistication as proprietary models.\n",
      "\n",
      "Speaker 2: This all sounds so futuristic! How do you see it evolving in the future?\n",
      "\n",
      "Speaker 1: Well, there’s still much to explore. As AI continues to evolve, Knowledge Distillation will likely become even more advanced and efficient. Future research might focus on refining techniques like reinforcement learning or exploring new ways of integrating human-like reasoning into these models.\n",
      "\n",
      "Speaker 2: Umm, what are some current challenges?\n",
      "\n",
      "Speaker 1: There are several challenges. Ensuring ethical use is a big one — we need to comply with legal terms and ensure the application of Knowledge Distillation remains lawful and ethical. Also, bridging the knowledge gap between proprietary and open-source models requires continuous research and innovation.\n",
      "\n",
      "Speaker 2: That makes sense! So it’s not just about making technology better but also ensuring it’s used responsibly?\n",
      "\n",
      "Speaker 1: Exactly! It’s a dual mission — to democratize access to advanced AI capabilities while maintaining ethical standards. And that, my friend, is the heart of Knowledge Distillation in this era of Large Language Models.\n",
      "\n",
      "Speaker 2: Umm, I’m really excited about where this could go!\n",
      "\n",
      "Speaker 1: Me too! Knowledge Distillation is set to change the game, making smart technology more accessible and efficient for everyone. Stay curious and keep exploring!\n",
      "\n",
      "Speaker 2: Thanks so much for breaking it down for me today! This has been a wild ride into the future of AI.\n",
      "\n",
      "Speaker 1: My pleasure! We’ll keep diving deep into these fascinating topics on our podcast. Until next time, stay tuned and keep thinking big!\n"
     ]
    }
   ],
   "source": [
    "save_string_pkl = outputs\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1414fe",
   "metadata": {},
   "source": [
    "Let's save the output as pickle file and continue further to Notebook 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2130b683-be37-4dae-999b-84eff15c687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./resources/data.pkl', 'wb') as file:\n",
    "    pickle.dump(save_string_pkl, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae9411",
   "metadata": {},
   "source": [
    "### Next Notebook: Transcript Re-writer\n",
    "\n",
    "We now have a working transcript but we can try making it more dramatic and natural. In the next notebook, we will use `Llama-3.1-8B-Instruct` model to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bab2f2-f539-435a-ae6a-3c9028489628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3568c7a-fbbd-40ab-85c9-841fa2c61d36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
