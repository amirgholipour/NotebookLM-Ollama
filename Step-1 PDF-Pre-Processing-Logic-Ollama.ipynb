{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f67a6a6",
   "metadata": {},
   "source": [
    "## Notebook 1: PDF Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68aee84-04e3-4cbc-be78-6de9e06e704f",
   "metadata": {},
   "source": [
    "In the series, we will be going from a PDF to Podcast using all open models. \n",
    "\n",
    "The first step in getting to the podcast is finding a script, right now our logic is:\n",
    "- Use any PDF on any topic\n",
    "- Prompt  `Granite 3.2 8B`  model to process it into a text file\n",
    "- Re-write this into a podcast transcript in next notebook.\n",
    "\n",
    "In this notebook, we will upload a PDF and save it into a `.txt` file using the `PyPDF2` library, later we will process chunks from the text file using our featherlight model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb3584",
   "metadata": {},
   "source": [
    "Most of us shift-enter pass the comments to realise later we need to install libraries. For the few that read the instructions, please remember to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "723c440f-6e1c-4431-a9ed-a1db6db339bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                         ID              SIZE      MODIFIED     \n",
      "qwq:latest                                                   cc1091b0e276    19 GB     15 hours ago    \n",
      "hf.co/OuteAI/OuteTTS-0.3-500M-GGUF:Q4_K_S                    76c6be93d29c    391 MB    3 days ago      \n",
      "hf.co/OuteAI/OuteTTS-0.2-500M-GGUF:Q8_0                      0a6c38a67073    536 MB    3 days ago      \n",
      "mistral-small:24b                                            8039dd90c113    14 GB     8 days ago      \n",
      "llama3.2-vision:11b                                          085a1fdae525    7.9 GB    8 days ago      \n",
      "mistral-nemo:latest                                          994f3b8b7801    7.1 GB    8 days ago      \n",
      "granite3.2:8b                                                9bcb3335083f    4.9 GB    9 days ago      \n",
      "phi4-mini:latest                                             60f202f815d7    2.8 GB    9 days ago      \n",
      "B    9 days ago      est                                     3be41a661804    2.4 G\n",
      "phi4:latest                                                  ac896e5b8b34    9.1 GB    2 weeks ago     \n",
      "nomic-embed-text:latest                                      0a109f422b47    274 MB    2 weeks ago     \n",
      "llama3.2:3b                                                  a80c4f17acd5    2.0 GB    4 weeks ago     \n",
      "deepseek-r1:8b                                               28f8fd6cdc67    4.9 GB    4 weeks ago     \n",
      "hf.co/lmstudio-community/Qwen2.5-7B-Instruct-1M-GGUF:Q8_0    2d4b678222de    8.1 GB    5 weeks ago     \n",
      "deepseek-r1:32b                                              38056bbcbb2d    19 GB     5 weeks ago     \n",
      "qwen2.5-coder:32b                                            4bd6cbf2d094    19 GB     3 months ago    \n",
      "mxbai-embed-large:latest                                     468836162de7    669 MB    3 months ago    \n",
      "llama3.2:1b                                                  baf6a787fdff    1.3 GB    4 months ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b23d509",
   "metadata": {},
   "source": [
    "Assuming you have a PDF uploaded on the same machine, please set the path for the file. \n",
    "\n",
    "Also, if you want to flex your GPU-please switch to a bigger model although the featherlight models work perfectly for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d0061b-8b8c-4353-850f-f19466a0ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = './resources/2402.13116v4.pdf'\n",
    "DEFAULT_MODEL = \"granite3.2:8b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f349fa20-76c1-481a-b4dd-320d3e9fff5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92aa555b-6b2d-492a-bffe-ec78fdc06f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21029232-ac5f-42ca-b26b-baad5b2f49b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from typing import Optional\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203c22eb",
   "metadata": {},
   "source": [
    "Let's make sure we don't stub our toe by checking if the file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "153d9ece-37a4-4fff-a8e8-53f923a2b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_pdf(file_path: str) -> bool:\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return False\n",
    "    if not file_path.lower().endswith('.pdf'):\n",
    "        print(\"Error: File is not a PDF\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a362ac3",
   "metadata": {},
   "source": [
    "Convert PDF to a `.txt` file. This would simply read and dump the contents of the file. We set the maximum characters to 100k. \n",
    "\n",
    "For people converting their favorite novels into a podcast, they will have to consider a model with context length which is 128k tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b57c2d64-3d75-4aeb-b4ee-bd1661286b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path: str, max_chars: int = 100000) -> Optional[str]:\n",
    "    if not validate_pdf(file_path):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            # Create PDF reader object\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            \n",
    "            # Get total number of pages\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            print(f\"Processing PDF with {num_pages} pages...\")\n",
    "            \n",
    "            extracted_text = []\n",
    "            total_chars = 0\n",
    "            \n",
    "            # Iterate through all pages\n",
    "            for page_num in range(num_pages):\n",
    "                # Extract text from page\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text = page.extract_text()\n",
    "                \n",
    "                # Check if adding this page's text would exceed the limit\n",
    "                if total_chars + len(text) > max_chars:\n",
    "                    # Only add text up to the limit\n",
    "                    remaining_chars = max_chars - total_chars\n",
    "                    extracted_text.append(text[:remaining_chars])\n",
    "                    print(f\"Reached {max_chars} character limit at page {page_num + 1}\")\n",
    "                    break\n",
    "                \n",
    "                extracted_text.append(text)\n",
    "                total_chars += len(text)\n",
    "                print(f\"Processed page {page_num + 1}/{num_pages}\")\n",
    "            \n",
    "            final_text = '\\n'.join(extracted_text)\n",
    "            print(f\"\\nExtraction complete! Total characters: {len(final_text)}\")\n",
    "            return final_text\n",
    "            \n",
    "    except PyPDF2.PdfReadError:\n",
    "        print(\"Error: Invalid or corrupted PDF file\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023397b",
   "metadata": {},
   "source": [
    "Helper function to grab meta info about our PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0984bb1e-d52c-4cec-a131-67a48061fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PDF metadata\n",
    "def get_pdf_metadata(file_path: str) -> Optional[dict]:\n",
    "    if not validate_pdf(file_path):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            metadata = {\n",
    "                'num_pages': len(pdf_reader.pages),\n",
    "                'metadata': pdf_reader.metadata\n",
    "            }\n",
    "            return metadata\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting metadata: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6019affc",
   "metadata": {},
   "source": [
    "Finally, we can run our logic to extract the details from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63848943-79cc-4e21-8396-6eab5df493e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting metadata...\n",
      "\n",
      "PDF Metadata:\n",
      "Number of pages: 43\n",
      "Document info:\n",
      "/Author: \n",
      "/CreationDate: D:20241022021202Z\n",
      "/Creator: LaTeX with hyperref\n",
      "/Keywords: \n",
      "/ModDate: D:20241022021202Z\n",
      "/PTEX.Fullbanner: This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5\n",
      "/Producer: pdfTeX-1.40.25\n",
      "/Subject: \n",
      "/Title: \n",
      "/Trapped: /False\n",
      "\n",
      "Extracting text...\n",
      "Processing PDF with 43 pages...\n",
      "Processed page 1/43\n",
      "Processed page 2/43\n",
      "Processed page 3/43\n",
      "Processed page 4/43\n",
      "Processed page 5/43\n",
      "Processed page 6/43\n",
      "Processed page 7/43\n",
      "Processed page 8/43\n",
      "Processed page 9/43\n",
      "Processed page 10/43\n",
      "Processed page 11/43\n",
      "Processed page 12/43\n",
      "Processed page 13/43\n",
      "Processed page 14/43\n",
      "Processed page 15/43\n",
      "Processed page 16/43\n",
      "Reached 100000 character limit at page 17\n",
      "\n",
      "Extraction complete! Total characters: 100016\n",
      "\n",
      "Preview of extracted text (first 500 characters):\n",
      "--------------------------------------------------\n",
      "1\n",
      "A Survey on Knowledge Distillation of Large\n",
      "Language Models\n",
      "Xiaohan Xu1, Ming Li2, Chongyang Tao3, Tao Shen4, Reynold Cheng1, Jinyang Li1,\n",
      "Can Xu5, Dacheng Tao6, Tianyi Zhou2\n",
      "1The University of Hong Kong2University of Maryland3Microsoft\n",
      "4University of Technology Sydney5Peking University6The University of Sydney\n",
      "{shawnxxh,chongyangtao,hishentao }@gmail.com {minglii,tianyi }@umd.edu\n",
      "ckcheng@cs.hku.hk jl0725@connect.hku.hk\n",
      "Abstract —In the era of Large Language Models (LLMs), Knowledge Distillati\n",
      "--------------------------------------------------\n",
      "\n",
      "Total characters extracted: 100016\n",
      "\n",
      "Extracted text has been saved to extracted_text.txt\n"
     ]
    }
   ],
   "source": [
    "# Extract metadata first\n",
    "print(\"Extracting metadata...\")\n",
    "metadata = get_pdf_metadata(pdf_path)\n",
    "if metadata:\n",
    "    print(\"\\nPDF Metadata:\")\n",
    "    print(f\"Number of pages: {metadata['num_pages']}\")\n",
    "    print(\"Document info:\")\n",
    "    for key, value in metadata['metadata'].items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Extract text\n",
    "print(\"\\nExtracting text...\")\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Display first 500 characters of extracted text as preview\n",
    "if extracted_text:\n",
    "    print(\"\\nPreview of extracted text (first 500 characters):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(extracted_text[:500])\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"\\nTotal characters extracted: {len(extracted_text)}\")\n",
    "\n",
    "# Optional: Save the extracted text to a file\n",
    "if extracted_text:\n",
    "    output_file = 'extracted_text.txt'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(extracted_text)\n",
    "    print(f\"\\nExtracted text has been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d1f59",
   "metadata": {},
   "source": [
    "### Llama Pre-Processing\n",
    "\n",
    "Now let's proceed to justify our distaste for writing regex and use that as a justification for a LLM instead:\n",
    "\n",
    "At this point, have a text file extracted from a PDF of a paper. Generally PDF extracts can be messy due to characters, formatting, Latex, Tables, etc. \n",
    "\n",
    "One way to handle this would be using regex, instead we can also prompt the feather light Llama models to clean up our text for us. \n",
    "\n",
    "Please try changing the `SYS_PROMPT` below to see what improvements you can make:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c0828a5-964d-475e-b5f5-40a04e287725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "SYS_PROMPT = \"\"\"\n",
    "You are a world class text pre-processor, here is the raw data from a PDF, please parse and return it in a way that is crispy and usable to send to a podcast writer.\n",
    "\n",
    "The raw data is messed up with new lines, Latex math and you will see fluff that we can remove completely. Basically take away any details that you think might be useless in a podcast author's transcript.\n",
    "\n",
    "Remember, the podcast could be on any topic whatsoever so the issues listed above are not exhaustive\n",
    "\n",
    "Please be smart with what you remove and be creative ok?\n",
    "\n",
    "Remember DO NOT START SUMMARIZING THIS, YOU ARE ONLY CLEANING UP THE TEXT AND RE-WRITING WHEN NEEDED\n",
    "\n",
    "Be very smart and aggressive with removing details, you will get a running portion of the text and keep returning the processed text.\n",
    "\n",
    "PLEASE DO NOT ADD MARKDOWN FORMATTING, STOP ADDING SPECIAL CHARACTERS THAT MARKDOWN CAPATILISATION ETC LIKES\n",
    "\n",
    "ALWAYS start your response directly with processed text and NO ACKNOWLEDGEMENTS about my questions ok?\n",
    "Here is the text:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56b7bf14-dfa9-410e-b21a-a7defb46c534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd393fae",
   "metadata": {},
   "source": [
    "Instead of having the model process the entire file at once, as you noticed in the prompt-we will pass chunks of the file. \n",
    "\n",
    "One issue with passing chunks counted by characters is, we lose meaning of words so instead we chunk by words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24e8a547-9d7c-4e2f-be9e-a3aea09cce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_bounded_chunks(text, target_chunk_size):\n",
    "    \"\"\"\n",
    "    Split text into chunks at word boundaries close to the target chunk size.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for word in words:\n",
    "        word_length = len(word) + 1  # +1 for the space\n",
    "        if current_length + word_length > target_chunk_size and current_chunk:\n",
    "            # Join the current chunk and add it to chunks\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = word_length\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "            current_length += word_length\n",
    "    \n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d74223f",
   "metadata": {},
   "source": [
    "Let's load in the model and start processing the text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aa6c47a-159f-49fa-8166-dc8855b2abd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def process_chunk(text_chunk, chunk_num):\n",
    "    \"\"\"Process a chunk of text and return both input and output for verification\"\"\"\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": text_chunk},\n",
    "    ]\n",
    "    \n",
    "    # Generate response using ollama.chat\n",
    "    response = ollama.chat(\n",
    "        model=DEFAULT_MODEL, #\"llama3.2:1b\",  # Replace with your specific model name\n",
    "        messages=conversation\n",
    "    )\n",
    "    \n",
    "    processed_text = response['message']['content']\n",
    "    \n",
    "    # Print chunk information for monitoring\n",
    "    print(f\"\\n{'='*40} Chunk {chunk_num} {'='*40}\")\n",
    "    print(f\"INPUT TEXT:\\n{text_chunk[:500]}...\")  # Show first 500 chars of input\n",
    "    print(f\"\\nPROCESSED TEXT:\\n{processed_text[:500]}...\")  # Show first 500 chars of output\n",
    "    print(f\"{'='*90}\\n\")\n",
    "    \n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0183c47-339d-4041-ae83-77fc34931075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_FILE = \"./resources/extracted_text.txt\"  # Replace with your file path\n",
    "INPUT_FILE = \"./resources/extracted_text.txt\"  # Replace with your file path\n",
    "\n",
    "CHUNK_SIZE = 1000  # Adjust chunk size if needed\n",
    "text =extracted_text\n",
    "chunks = create_word_bounded_chunks(text, CHUNK_SIZE)\n",
    "num_chunks = len(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb36814f-9310-4734-bf54-e16a5032339e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "447188d3-ebf0-42d5-940e-4d7e0d9dbf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Calculate number of chunks\n",
    "num_chunks = (len(text) + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
    "\n",
    "# Cell 6: Process the file with ordered output\n",
    "# Create output file name\n",
    "output_file = \"./resources/\" +f\"clean_{os.path.basename(INPUT_FILE)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7917dfdd-b3af-44fc-a8c0-2760ace9363e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38cca4a797549a5a310891800e01802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chunks:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================== Chunk 0 ========================================\n",
      "INPUT TEXT:\n",
      "1 A Survey on Knowledge Distillation of Large Language Models Xiaohan Xu1, Ming Li2, Chongyang Tao3, Tao Shen4, Reynold Cheng1, Jinyang Li1, Can Xu5, Dacheng Tao6, Tianyi Zhou2 1The University of Hong Kong2University of Maryland3Microsoft 4University of Technology Sydney5Peking University6The University of Sydney {shawnxxh,chongyangtao,hishentao }@gmail.com {minglii,tianyi }@umd.edu ckcheng@cs.hku.hk jl0725@connect.hku.hk Abstract —In the era of Large Language Models (LLMs), Knowledge Distillati...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "1. **Title:** A Survey on Knowledge Distillation of Large Language Models\n",
      "   - Authors: Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, Tianyi Zhou\n",
      "   - Affiliations: The University of Hong Kong, University of Maryland, Microsoft, University of Technology Sydney, Peking University, The University of Sydney\n",
      "   - Contact: {shawnxxh,chongyangtao,hishentao}@gmail.com, {minglii,tianyi}@umd.edu, ckcheng@cs.hku.hk, jl0725@connect.hku.hk\n",
      "\n",
      "**Abstract:** Knowl...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 1 ========================================\n",
      "INPUT TEXT:\n",
      "advanced knowledge to smaller models and its utility in model compression and self- improvement. Our survey is meticulously structured around three foundational pillars: algorithm ,skill, and verticalization – providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the interaction between data augmentation (DA) and KD, illustrating how DA emerges as a powerful ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Advanced knowledge distillation techniques applied to smaller models, enhancing model compression and self-improvement capabilities. The survey focuses on three key areas: algorithms, skills, and verticalization. It delves into knowledge distillation (KD) mechanisms, cognitive ability enhancements, and practical applications across various sectors. Notably, the survey explores the synergy between data augmentation (DA) and KD, demonstrating how DA strengthens KD's performance within large langua...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 2 ========================================\n",
      "INPUT TEXT:\n",
      "distillation and proposing future research directions. By bridging the gap between proprietary and open-source LLMs, this survey underscores the potential for more accessible, efficient, and powerful AI solutions. Most importantly, we firmly advocate for compliance with the legal terms that regulate the use of LLMs, ensuring ethical and lawful application of KD of LLMs. An associated Github repository is available at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs. Index Terms —...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "processed text:\n",
      "\n",
      "Title: Knowledge Distillation of Large Language Models - A Survey and Future Directions\n",
      "\n",
      "This survey explores knowledge distillation (KD) techniques applied to large language models (LLMs), aiming to make AI solutions more accessible, efficient, and powerful. It bridges the gap between proprietary LLMs like GPT-3.5, GPT-4, Gemini, and Claude, with open-source alternatives.\n",
      "\n",
      "The authors emphasize the importance of adhering to legal terms governing LLM use, ensuring ethical and la...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 3 ========================================\n",
      "INPUT TEXT:\n",
      "complexity, have unlocked new realms of possibility, from generating human-like text to offering sophisticated problem-solving capabilities. The core significance of these LLMs lies in their emergent abil- ities (Wei et al., 2022a,b; Xu et al., 2024a), a phenomenon where the models display capabilities beyond their explicit training objectives, enabling them to tackle a diverse array of tasks with remarkable proficiency. These models excel in understanding and generation, driving applications fr...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "Large Language Models (LLMs) have unlocked new realms of possibility, from generating human-like text to offering sophisticated problem-solving capabilities. Their core significance lies in their emergent abilities, where models display capabilities beyond their explicit training objectives, enabling them to tackle a diverse array of tasks with remarkable proficiency. These models excel in understanding and generation, driving applications from creative content creation to compl...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 4 ========================================\n",
      "INPUT TEXT:\n",
      "redefine our interaction with technology. Despite the remarkable capabilities of proprietary LLMs like GPT-4 and Gemini, they are not without their shortcom- ings, particularly when viewed in light of the advantages offered by open-source models. A significant drawback is their limited accessibility and higher cost (OpenAI et al., 2023). These proprietary models often come with substantial usage fees and restricted access, making them less attain- able for individuals and smaller organizations. ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "redefine our interaction with technology. Proprietary large language models (LLMs) like GPT-4 and Gemini, despite their impressive capabilities, have limitations. Key issues include limited accessibility and higher costs (OpenAI et al., 2023). These models often involve significant usage fees and restricted access, making them less accessible for individuals and smaller organizations.\n",
      "\n",
      "Data privacy and security are also concerns when using proprietary LLMs (Wu et al., 2023a). They typically requ...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 5 ========================================\n",
      "INPUT TEXT:\n",
      "present significant challenges in leveraging the full potential of proprietary LLMs. In contrast to proprietary LLMs, open-source models like LLaMA (Touvron et al., 2023) and Mistral (Jiang et al., 2023a) bring several notable advantages. One of the primaryarXiv:2402.13116v4 [cs.CL] 21 Oct 2024 2 benefits of open-source models is their accessibility and adaptability. Without the constraints of licensing fees or restrictive usage policies, these models are more readily available to a broader rang...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Open-source language models like LLaMA (Touvron et al., 2023) and Mistral (Jiang et al., 2023a) offer significant advantages over proprietary LLMs. Their key benefits include accessibility and adaptability. Unlike proprietary models, these open-source alternatives do not impose licensing fees or restrictive usage policies, making them widely available to a diverse range of users, from individual researchers to smaller organizations. This openness promotes collaboration and inclusivity in AI rese...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 6 ========================================\n",
      "INPUT TEXT:\n",
      "resources compared to their proprietary counterparts. One of the most significant limitations is the smaller model scale, which often results in lower per- formance on real-world tasks with a bunch of instruc- tions (Zheng et al., 2023a). These models, with fewer pa- rameters, may struggle to capture the depth and breadth of knowledge embodied in larger models like GPT-4. Ad- ditionally, the pre-training investment in these open-source models is typically less substantial. This reduced investmen...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Open-source models often face significant limitations compared to proprietary ones. A major issue is their smaller scale, which can lead to lower performance on real-world tasks involving multiple instructions (Zheng et al., 2023a). These models, due to fewer parameters, may struggle to grasp the extensive knowledge base of larger models like GPT-4.\n",
      "\n",
      "Additionally, pre-training investment in open-source models is usually less substantial. This can result in a narrower range of pre-training data, ...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 7 ========================================\n",
      "INPUT TEXT:\n",
      "particularly evident when these models are compared to the highly fine-tuned proprietary LLMs, which are often tailored to excel in a wide array of complex scenarios (OpenAI et al., 2023). Primarily, recognizing the disparities between propri- etary and open-source LLMs, KD techniques have surged as a means to bridge the performance gap between these models (Gou et al., 2021; Gupta and Agrawal, 2022). Knowl- edge distillation, in this context, involves leveraging the more advanced capabilities o...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Knowledge Distillation (KD) techniques have gained traction to bridge the performance gap between proprietary and open-source Language Learning Models (LLMs). This approach, similar to learning from a skilled teacher, uses advanced proprietary models like GPT-4 or Gemini as a guiding framework. Unlike traditional knowledge distillation algorithms, data augmentation (DA) has become a common method. It involves enhancing the competencies of open-source LLMs by mimicking the performance characteris...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 8 ========================================\n",
      "INPUT TEXT:\n",
      "paradigm to achieve knowledge distillation of LLMs, where a small seed of knowledge is used to prompt the LLM to generate more data with respect to a specific skill or domain (Taori et al., 2023). Secondly, KD still retains its fundamental role in compressing LLMs, making them more efficient without significant loss in performance. (Gu et al., 2024; Agarwal et al., 2024). More recently, the strategy of employing open-source LLMs as teachers for their own self-improvement has emerged as a promisi...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "processed text:\n",
      "\n",
      "Knowledge Distillation (KD) serves as a paradigm for LLM knowledge transfer using a minimal seed of information to prompt the model for skill or domain-specific data generation. It continues to play a crucial role in compressing LLMs, improving efficiency without substantial performance loss. Recently, utilizing open-source LLMs as self-improvement teachers has shown promise, significantly boosting their capabilities (Yuan et al., 2024a; Chen et al., 2024a). Figure 1 visually re...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 9 ========================================\n",
      "INPUT TEXT:\n",
      "compression for efficiency, and 3) an emerging trend of self-improvement via self-generated knowledge. (e.g., in-context learning (Huang et al., 2022a) and in- struction following (Taori et al., 2023)), improved align- ment with user intents (e.g., human values/principles (Cui et al., 2023a), and thinking patterns like chain-of-thought (CoT) (Mukherjee et al., 2023)), and NLP task specialization (e.g., semantic understanding (Ding et al., 2023a), and code generation (Chaudhary, 2023)). These ski...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "compression for efficiency, self-improvement via self-generated knowledge (in-context learning, in-instruction following), improved alignment with user intents (human values/principles, chain-of-thought thinking patterns), and NLP task specialization (semantic understanding, code generation). These skills are vital for LLMs' diverse applications, from casual conversations to complex problem-solving. In vertical domains like healthcare, law, or science, knowledge distillation enables open-source ...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 10 ========================================\n",
      "INPUT TEXT:\n",
      "been extensively trained and fine-tuned in these areas. The benefits of knowledge distillation in the era of LLMs are multifaceted and transformative (Gu et al., 2024). Through a suite of distillation techniques, the gap between proprietary and open-source models is significantly nar- rowed (Chiang et al., 2023; Xu et al., 2023a) and even filled (Zhao et al., 2023a). This process not only streamlines computational requirements but also enhances the environ- mental sustainability of AI operations...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Knowledge distillation has been extensively trained and fine-tuned in various areas. Its benefits are multifaceted and transformative. Through a range of distillation techniques, the gap between proprietary and open-source models is significantly narrowed and even filled. This process not only simplifies computational requirements but also improves the environmental sustainability of AI operations. As open-source models become more proficient with less computational overhead, they enhance access...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 11 ========================================\n",
      "INPUT TEXT:\n",
      "and research domains. The escalating need for a comprehensive survey on the knowledge distillation of LLMs stems from the rapidly evolving landscape of AI (OpenAI et al., 2023; Team et al., 2023) and the increasing complexity of these models. As AI continues to penetrate various sectors, the ability to effi- ciently and effectively distill knowledge from proprietary LLMs to open-source ones becomes not just a technical aspiration but a practical necessity. This need is driven by the growing dema...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "processed text:\n",
      "\n",
      "The urgency for a comprehensive survey on knowledge distillation in Large Language Models (LLMs) arises from the rapid advancements in AI and the growing complexity of these models. As AI permeates various sectors, efficiently transferring knowledge from proprietary LLMs to open-source ones becomes increasingly crucial—not just a technical goal, but a practical necessity. This demand is fueled by the rising need for accessible, cost-effective, and adaptable AI solutions across d...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 12 ========================================\n",
      "INPUT TEXT:\n",
      "ReinforcementLearningoutputsreward RM!(·)distill SupervisedFine-tuningX,Y preferenceRankOptimizationy,1y,2y3y1y2y3≻≻rank…… DataCuration X,YrawdatasynthesizefeedbackFeedback input outputSelf-Knowledge outputinputinput YlabelLabelingExpansion X,YdemonstrationsexpandFeature featureinput,outputextractSec.4Sec.5 Sec.3.1Sec.3.2①②③④ Fig. 2: An overview of this survey on knowledge distillation of large language models. Note that ‘Section’ is abbreviated as ‘Sec.’ in this figure. RM S(·)denotes the stude...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "**Title:** Survey on Knowledge Distillation of Large Language Models\n",
      "\n",
      "**Objective:** This survey aims to synthesize current methodologies, challenges, and breakthroughs in knowledge distillation. It serves as a guide for researchers and practitioners, helping them distill complex AI capabilities into more manageable forms. The survey also identifies gaps in current techniques and proposes directions for future research.\n",
      "\n",
      "**Key Components:**\n",
      "\n",
      "1. **Reward Model (RM!(·)):** A model...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 13 ========================================\n",
      "INPUT TEXT:\n",
      "future research. Survey Organization. The remainder of this survey is orga- nized into several comprehensive sections, each designed to offer a deep dive into the multifaceted aspects of knowledge distillation within the realm ofLLMs. Following this intro- duction, §2 provides a foundational overview of knowledge distillation, comparing traditional techniques with those emerging in the era of LLMs and highlighting the role of data augmentation (DA) in this context. §3 delves into the approaches ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Future research in knowledge distillation for Large Language Models (LLMs) is structured into several detailed sections. This structure begins with an introduction, followed by §2 offering a fundamental overview of knowledge distillation. It contrasts conventional methods with those arising in the LLM era, emphasizing data augmentation's role.\n",
      "\n",
      "§3 investigates techniques for extracting knowledge from teacher LLMs and primary distillation algorithms. This covers strategies ranging from supervised...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 14 ========================================\n",
      "INPUT TEXT:\n",
      "(NLU), genera- tion (NLG), information retrieval, recommendation systems, and the evaluation of text generation. In §5, we venture into domain-specific vertical distillation, showcasing how knowledge distillation techniques are applied within spe- cialized fields such as law, healthcare, finance, and science,illustrating the practical implications and transformative impact of these approaches. The survey suggests open problems in §6, identifying current challenges and gaps in knowledge distillat...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "This text covers key areas in AI and NLP, including Natural Language Understanding (NLU), generation (NLG), information retrieval, recommendation systems, and text evaluation. It delves into domain-specific vertical distillation in fields like law, healthcare, finance, and science, highlighting practical applications and potential impacts. The survey identifies open problems and research gaps in knowledge distillation. A conclusion and discussion synthesize insights, reflecting ...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 15 ========================================\n",
      "INPUT TEXT:\n",
      "model (teacher) to a smaller, more efficient model (student) (Gou et al., 2021). This technique is pivotal in mitigating the challenges posed by the computational demands and resource constraints of deploying large-scale models in practical applications. Historically, knowledge distillation techniques, prior to the era of LLMs, primarily concentrated on transferring knowledge from complex, often cumbersome neural net- works to more compact and efficient architectures (Sanh et al., 2019; Kim and ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Process Text:\n",
      "\n",
      "In the field of machine learning, a method called knowledge distillation is used. This involves transferring knowledge from a larger model (teacher) to a smaller, more efficient one (student). This approach is crucial for overcoming computational challenges and resource limitations when deploying large-scale models in practical applications. Historically, this technique has been applied to move knowledge from complex neural networks to compact, efficient architectures, primarily t...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 16 ========================================\n",
      "INPUT TEXT:\n",
      "CoT-Distill (Hsieh et al., 2023) Orca (Mukherjee et al., 2023), Orca 2 (Mitra et al., 2023), Baize (Xu et al., 2023b), Mammoth (Yue et al., 2023a), Mixed Distill (Chenglin et al., 2023) ExpansionSelf-Instruct (Wang et al., 2022a), Alpaca (Taori et al., 2023), Code Alpaca (Chaudhary, 2023) Self-Align (Sun et al., 2024b), WizardLM (Xu et al., 2023a), WizardCoder (Luo et al., 2023a), WizardMath (Luo et al., 2023b), AugGPT (Dai et al., 2023a), TDG (He et al., 2023b) CurationUltraChat (Ding et al., 2...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "CoT-Distill, Orca, Orca 2, Baize, Mammoth, Mixed Distill, ExpansionSelf-Instruct, Alpaca, Code Alpaca, Self-Align, WizardLM, WizardCoder, WizardMath, AugGPT, TDG, UltraChat, Phi-1, Phi-1.5, Phi-2, Magicoder, WaveCoder, ZeroGen, SunGen, InPars, BabyLlama, MiniLLM, GKD, QuantGPT, LLM-QAT, FeedbackCAI, WizardMath, UltraFeedback, Zephyr...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 17 ========================================\n",
      "INPUT TEXT:\n",
      "(Tunstall et al., 2023), CycleAlign (Hong et al., 2023), RLAIF (Lee et al., 2023a), Lion (Jiang et al., 2023b), PERsD (Chen et al., 2023a), GKD (Agarwal et al., 2024) Self-KnowledgeSelf-Instruct (Wang et al., 2022a), Self-Align (Sun et al., 2024b), RLCD (Yang et al., 2024), ImpDistill (Jung et al., 2023), LMSI (Huang et al., 2023a), ReST (Gulcehre et al., 2023), Self-Rewarding (Yuan et al., 2024a), Baize (Xu et al., 2023b), STaR (Zelikman et al., 2022) DistillationSupervised Fine-TuningAlpaca (T...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "Self-Knowledge and Self-Instruct Methods: CycleAlign, RLAIF, Lion, PERsD, GKD, Self-Knowledge, Self-Instruct, RLCD, ImpDistill, LMSI, ReST, Self-Rewarding, Baize, STaR.\n",
      "\n",
      "Supervised Fine-Tuning Models: Alpaca, Vicuna, WizardLM, Baize, STaR.\n",
      "\n",
      "Distillation Techniques: DistilGPT, f-Distill, MiniLLM, TED, GKD, BabyLlama.\n",
      "\n",
      "Reinforcement Learning Methods: CAI, UltraFeedback, WizardMath, MiniLLM....\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 18 ========================================\n",
      "INPUT TEXT:\n",
      "(Gu et al., 2024), GKD (Agarwal et al., 2024), GPT3 Reward (Kwon et al., 2023) Rank Optimization Zephyr (Tunstall et al., 2023), CycleAlign (Hong et al., 2023), Skill DistillationContext FollowingInstruction FollowingSelf-Instruct (Wang et al., 2022a), Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), WizardLM (Xu et al., 2023a), Orca (Mukherjee et al., 2023), Orca 2 (Mitra et al., 2023), WizardMath (Luo et al., 2023b), Llama-GPT4 (Peng et al., 2023a), Multi-turn DialogueVicuna (Chiang ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Gu et al. (2024), Agarwal et al. (2024) - GKD, Kwon et al. (2023) - GPT3 Reward, Tunstall et al. (2023) - Rank Optimization Zephyr, Hong et al. (2023) - CycleAlign, Wang et al. (2022a) - Context Following, Instruction Following, Self-Instruct, Taori et al. (2023) - Alpaca, Chiang et al. (2023) - Vicuna, Xu et al. (2023a) - WizardLM, Mukherjee et al. (2023) - Orca, Mitra et al. (2023) - Orca 2, Luo et al. (2023b) - WizardMath, Peng et al. (2023a) - Llama-GPT4, Chiang et al. (2023) - Multi-turn Di...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 19 ========================================\n",
      "INPUT TEXT:\n",
      "Reward (Kwon et al., 2023), ILF (Scheurer et al., 2023), ALMoST (Kim et al., 2023a), RLEF (Roit et al., 2023), RLAIF (Lee et al., 2023a), Zephy (Tunstall et al., 2023), UltraFeedback (Cui et al., 2023a), ValueCAI (Bai et al., 2022a), Align Honesty (Yang et al., 2023a), SANDBOX (Liu et al., 2023b), Self-Align (Sun et al., 2024b), UltraFeedback (Cui et al., 2023a), RLCD (Yang et al., 2024) AgentTool UsingToolformer (Schick et al., 2023), Graph-ToolFormer (Zhang, 2023), Gorilla (Patil et al., 2023)...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Reward, ILF, ALMoST, RLEF, RLAIF, Zephy, UltraFeedback, ValueCAI, Align Honesty, SANDBOX, Self-Align, RLCD, AgentTool, UsingToolformer, Graph-ToolFormer, Gorilla, ToolAlpaca, ToolLLM, CRAFT, Confucius, MLLM-Tool, α-UMi, PlanningFireAct, AgentTuning, Lumos, AUTOACT, TPTU-v2, NLUAugGPT, GPT Annotation, TDG, SunGen...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 20 ========================================\n",
      "INPUT TEXT:\n",
      "2023a), Mix Distill (Chenglin et al., 2023), Annollm (He et al., 2023a), UDG (Wang et al., 2021a), ZeroGen (Ye et al., 2022), NLGInheritSumm (Xu et al., 2023c), RECOMP (Xu et al., 2024b), MaRio (Ramnath et al., 2023), ID (Jung et al., 2023), GPT-3 Labeling (Wang et al., 2021b), BioGPT (Guo et al., 2023a), ChatGPT NMT (Yang and Nicolai, 2023), Information RetrievalQUILL (Srinivasan et al., 2022), Promptgator (Dai et al., 2023b), InPars (Bonifacio et al., 2022), AugTriever (Meng et al., 2023), (Su...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "2023a), Mix Distill (Chenglin et al., 2023), Annollm (He et al., 2023a), UDG (Wang et al., 2021a), ZeroGen (Ye et al., 2022), NLGInheritSumm (Xu et al., 2023c), RECOMP (Xu et al., 2024b), MaRio (Ramnath et al., 2023), ID (Jung et al., 2023), GPT-3 Labeling (Wang et al., 2021b), BioGPT (Guo et al., 2023a), ChatGPT NMT (Yang and Nicolai, 2023), Information RetrievalQUILL (Srinivasan et al., 2022), Promptgator (Dai et al., 2023b), InPars (Bonifacio et al., 2022), AugTriever (Meng et al., 2023), Ran...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 21 ========================================\n",
      "INPUT TEXT:\n",
      "al., 2023) Phi-1 (Gunasekar et al., 2023), PERsD (Chen et al., 2023a), MFTCoder (Liu et al., 2023d), WaveCoder (Yu et al., 2024), Code Clean (Jain et al., 2023), Multi-ModalityLLaVA (Liu et al., 2023e), SVIT (Zhao et al., 2023b), LVIS-Instruct4V (Wang et al., 2023e), Shikra (Chen et al., 2023c), LSKD (Park et al., 2023), DetGPT (Pi et al., 2023; Zhao et al., 2023c), LRV (Liu et al., 2023f), NExT-GPT (Wu et al., 2023b), Valley (Luo et al., 2023d), ILuvUI (Jiang et al., 2023d), StableLLaVA (Li et ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Phi-1 (Gunasekar et al., 2023), PERsD (Chen et al., 2023a), MFTCoder (Liu et al., 2023d), WaveCoder (Yu et al., 2024), Code Clean (Jain et al., 2023), Multi-ModalityLLaVA (Liu et al., 2023e), SVIT (Zhao et al., 2023b), LVIS-Instruct4V (Wang et al., 2023e), Shikra (Chen et al., 2023c), LSKD (Park et al., 2023), DetGPT (Pi et al., 2023; Zhao et al., 2023c), LRV (Liu et al., 2023f), NExT-GPT (Wu et al., 2023b), Valley (Luo et al., 2023d), ILuvUI (Jiang et al., 2023d), StableLLaVA (Li et al., 2023c)...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 22 ========================================\n",
      "INPUT TEXT:\n",
      "to mimic the output of a larger teacher network, often through techniques like soft target training, where the student learns from the softened softmax output of the teacher. Please refer to the survey (Gou et al., 2021) for more details on general knowledge distillation techniques in AI and DL. In contrast, the advent of LLMs has revolutionized the knowledge distillation landscape. The current era of knowledge distillation in LLMs shifts the focus from mere architecture compression to knowledge...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "In AI and DL, knowledge distillation techniques often involve mimicking a larger teacher network's output using methods like soft target training, where the student learns from the softened softmax output. For comprehensive insights, refer to Gou et al., 2021. However, the emergence of Large Language Models (LLMs) has transformed this field. Today, knowledge distillation in LLMs focuses on knowledge elicitation and transfer rather than just architecture compression. This shift is primarily due t...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 23 ========================================\n",
      "INPUT TEXT:\n",
      "focus in LLM-based knowledge distillation is to elicit the specific knowledge these models have. The key to this modern approach lies in heuristic and carefully designed prompts, which are used to elicit specific knowledge (Ding et al., 2023b) or capabilities (Chaudhary, 2023) from the LLMs. These prompts are crafted to tap into the LLM’s understanding and capabilities in various domains, ranging from natural language understanding (He et al., 2023a) to more complex cognitive tasks like reason- ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "focus is to extract specific knowledge from Large Language Models (LLMs). The approach involves using heuristic and meticulously designed prompts to elicit particular knowledge or capabilities from LLMs (Ding et al., 2023b; Chaudhary, 2023). These prompts are tailored to explore LLMs' understanding across various domains, including natural language processing (He et al., 2023a) and complex cognitive tasks such as reasoning (Hsieh et al., 2023) and problem-solving (Qiao et al., 2024). This prompt...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 24 ========================================\n",
      "INPUT TEXT:\n",
      "distillation also em- phasizes the transfer of more abstract qualities such as reasoning patterns (Mitra et al., 2023), preference align- ment (Cui et al., 2023a), and value alignment (Sun et al., 2024b). This is in stark contrast to the earlier focus on output replication (Taori et al., 2023), indicating a shift towards a more holistic and comprehensive transfer of cognitive capabilities. The current techniques involve not just the replication of outputs, but also the emulation of the thought p...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "distillation. It emphasizes the transfer of abstract qualities like reasoning patterns, preference alignment, and value alignment. This contrasts with earlier output replication methods. Current techniques involve replicating outputs and emulating thought processes and decision-making patterns of teacher models. Strategies include chain-of-thought prompting, training student models to learn teacher's reasoning for enhanced problem-solving and decision-making. 2.2 Relation to Data Augmentation (D...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 25 ========================================\n",
      "INPUT TEXT:\n",
      "knowledge distillation. Unlike traditional DA techniques such as paraphrasing (Gangal et al., 2022) or back-translation (Longpre et al., 2019), which primarily aim at expanding the training dataset in a somewhat mechanical manner, DA within the context of LLMs focuses on the generation of novel, context-rich training data tailored to specific domains and skills.The relationship between DA and KD in LLMs is both symbiotic and foundational. By leveraging a set of seed knowledge, KD employs DA to p...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Knowledge Distillation (KD) differs from conventional Data Augmentation (DA) techniques like paraphrasing or back-translation. Unlike these methods that mechanically expand training datasets, DA in Large Language Models (LLMs) generates novel, context-rich data for specific domains and skills. The interplay between DA and KD in LLMs is mutually beneficial and foundational.\n",
      "\n",
      "KD uses a seed of knowledge and employs DA to prompt LLMs to generate explicit data that embodies particular skills or doma...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 26 ========================================\n",
      "INPUT TEXT:\n",
      "ensuring that the distilled models not only replicate the teacher model’s output behavior but also embody its deep-seated understanding and cognitive strategies. DA acts as a force multiplier, enabling the distilled mod- els to acquire and refine capabilities that would otherwise require exponentially larger datasets and computational re- sources. It facilitates a more effective transfer of knowledge, focusing on the qualitative aspects of learning rather than quantitative expansion. This strate...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Distilled models aim to replicate not just the output behavior but also the underlying understanding and cognitive strategies of teacher models. Data Augmentation (DA) acts as a catalyst, allowing these models to gain and refine abilities that would otherwise necessitate massive datasets and computational power. This approach emphasizes qualitative learning aspects rather than quantitative growth.\n",
      "\n",
      "The strategic application of DA within Knowledge Distillation (KD) processes signifies a significa...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 27 ========================================\n",
      "INPUT TEXT:\n",
      "Building on the discussions introduced earlier, this survey aims to comprehensively explore the landscape of knowl- edge distillation within the context of LLMs, following a meticulously structured taxonomy as in Figure 3. The survey’s scope is delineated through three primary facets: KD Algorithms, Skill Distillation, and Verticalization Dis- tillation. Each facet encapsulates a range of subtopics and methodologies. It’s important to note that KD algorithms provide the technical foundations for...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Building on previous discussions, this survey thoroughly examines knowledge distillation in Large Language Models (LLMs) using a structured taxonomy (Figure 3). The survey's focus spans three main areas: KD Algorithms, Skill Distillation, and Verticalization Distillation. Each area covers various subtopics and methodologies.\n",
      "\n",
      "KD Algorithms: This section dives into the technical foundations and techniques of knowledge distillation. It covers the processes of extracting knowledge from teacher mode...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 28 ========================================\n",
      "INPUT TEXT:\n",
      "al., 2023), curation (Gu- nasekar et al., 2023), feature understanding (Agarwal et al., 2024), feedback mechanisms (Tunstall et al., 2023), and self- knowledge generation (Wang et al., 2022a). This exploration seeks to uncover the various ways in which knowledge can be identified, expanded, and curated for effective dis- tillation. The ‘ distillation ’ subsection examines learning ap- proaches like supervised fine-tuning (SFT) (Wang et al., 2022a), divergence minimization (Agarwal et al., 2024),...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "The research explores knowledge identification, expansion, and curation for effective distillation. Techniques include curation (Gunasekar et al., 2023), feature understanding (Agarwal et al., 2024), feedback mechanisms (Tunstall et al., 2023), and self-knowledge generation (Wang et al., 2022a). Distillation approaches cover supervised fine-tuning (SFT) (Wang et al., 2022a), divergence minimization (Agarwal et al., 2024), reinforcement learning techniques (Cui et al., 2023a), an...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 29 ========================================\n",
      "INPUT TEXT:\n",
      "retrieval-augmented generation (RAG) Capa- bility. In the realm of alignment (Mitra et al., 2023; Tun- stall et al., 2023), the survey investigates thinking patterns, persona/preference modeling, and value alignment. The ‘agent’ category delves into skills such as Tool Using and Planning. NLP task specialization (Dai et al., 2023a; Jung et al., 2023; Chaudhary, 2023) is scrutinized through lenses like natural language understanding (NLU), natural lan- guage generation (NLG), information retrieva...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Retrieval-Augmented Generation (RAG) Capability. The survey explores alignment aspects including thinking patterns, persona/preference modeling, and value alignment. It also examines 'agent' skills like Tool Using and Planning. NLP task specialization is analyzed through various lenses such as natural language understanding (NLU), natural language generation (NLG), information retrieval, recommendation systems, text generation evaluation, and code generation. The survey further delves into multi...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 30 ========================================\n",
      "INPUT TEXT:\n",
      "2023a), Finance (Zhang and Yang, 2023), Science (Zhang et al., 2024), among others. This exploration not only showcases the practical implications of KD tech- niques but also highlights their transformative impact on domain-specific AI solutions. Through these facets, this survey provides a compre- hensive analysis of KD in LLMs, guiding researchers and practitioners through methodologies, challenges, and op- portunities in this rapidly evolving domain. Declaration. This survey represents our ea...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "2023a), Finance (Zhang and Yang, 2023), Science (Zhang et al., 2024), among others. This exploration showcases practical implications of KD techniques and their transformative impact on domain-specific AI solutions. The survey provides a comprehensive analysis of KD in LLMs, guiding researchers and practitioners through methodologies, challenges, and opportunities.\n",
      "\n",
      "Declaration: This survey offers a broad overview of knowledge distillation techniques applied to LLMs, focusing on algorithms, skil...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 31 ========================================\n",
      "INPUT TEXT:\n",
      "foundational paradigms of knowledge dis- tillation, highlighting key methodologies and their impacts across a range of applications. 2.4 Distillation Pipeline in LLM Era SeedKnowledgeSkill/Domain TeacherLLMKnowledgeElicitationStudentModelDistillationAlgorithmsteer driveGeneratedKnowledgeLearningObjectivetrain Fig. 4: An illustration of a general pipeline to distill knowl- edge from a large language model to a student model. The general distillation pipeline of LLMs is a structured and methodical...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "2.4 Distillation Pipeline in LLM Era\n",
      "- Seed: Knowledge source (Teacher)\n",
      "- Skill/Domain: Area of expertise\n",
      "- TeacherLLM: Large Language Model (Teacher)\n",
      "- KnowledgeElicitation: Extracting knowledge from TeacherLLM\n",
      "- StudentModel: Less complex model to receive knowledge\n",
      "- DistillationAlgorithm: Methods used for transfer\n",
      "- GeneratedKnowledge: Knowledge transferred\n",
      "- LearningObjective: Goal of the distillation process\n",
      "- train: Training phase\n",
      "\n",
      "Figure 4 illustrates a general pipeline for distilling kno...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 32 ========================================\n",
      "INPUT TEXT:\n",
      "seen in Figure 2. I. Target Skill or Domain Steering Teacher LLM. The first stage involves directing the teacher LLM towards a specific target skill or domain. This is achieved through care- fully crafted instructions or templates that guide the LLM’s focus. These instructions are designed to elicit responses that demonstrate the LLM’s proficiency in a particular area, be it a specialized domain like healthcare or law, or a skill such as reasoning or language understanding. II. Seed Knowledge as...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "1. Directing Teacher LLM: The first step involves guiding the teacher Language Learning Model (LLM) towards a specific skill or domain using carefully crafted instructions or templates. These prompts aim to highlight the LLM's expertise in areas such as healthcare, law, reasoning, or language understanding.\n",
      "2. Seed Knowledge Input: After defining the target area, seed knowledge is introduced into the teacher LLM. This seed knowledge usually consists of a small dataset or specific data clues rele...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 33 ========================================\n",
      "INPUT TEXT:\n",
      "thereby creating more comprehensive and in-depth knowledge examples. III. Generation of Distillation Knowledge. In response to the seed knowledge and steering instructions, the teacher LLM generates knowledge examples. These examples are predominantly in the form of question-and-answer (QA) dialogues or narrative explanations, aligning with the nat- ural language processing/understanding capabilities of the LLM. In certain specialized cases, the outputs may also in- clude logits or hidden featur...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "III. Generation of Distillation Knowledge:\n",
      "   - In response to seed knowledge and steering instructions, the teacher LLM generates comprehensive question-and-answer (QA) dialogues or narrative explanations.\n",
      "   - These outputs primarily align with natural language processing/understanding capabilities.\n",
      "   - In specialized cases, outputs may include logits or hidden features, though this is less frequent due to complexity and specific requirements.\n",
      "\n",
      "IV. Training the Student Model:\n",
      "   - The generat...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 34 ========================================\n",
      "INPUT TEXT:\n",
      "learning objectives. The loss function quantifies the student model’s performance in replicating or adapting the knowledge from the teacher model. By minimizing this loss, the student model learns to emulate the target skills or domain knowledge of the teacher, thereby acquiring similar capabilities. The process involves iteratively adjusting the student model’s parameters to reduce the discrepancy be- tween its outputs and those of the teacher model, ensuring the effective transfer of knowledge...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Learning Objectives: The loss function gauges the student model's ability to replicate or adapt knowledge from the teacher model. By minimizing this loss, the student model learns to mimic the target skills or domain knowledge of the teacher, thereby acquiring similar capabilities. This involves iteratively adjusting the student model's parameters to minimize the discrepancy between its outputs and those of the teacher model, ensuring effective knowledge transfer.\n",
      "\n",
      "In essence, this process can b...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 35 ========================================\n",
      "INPUT TEXT:\n",
      "example ( e.g., (x, y)) from the teacher LLM’s output o(plus the input sin some cases), andpTrepresents the teacher LLM with parameters θT. Given the datasets D(kd) Ibuilt for distillation, we then define a learning objective as L=X ILI(D(kd) I;θS), (2) whereP Idenotes there could be multiple tasks or skills being distilled into one student model, LI(·;·)stands for a specific learning objective, and θSparameterizes the student model. Following our exploration of the distillation pipeline and the...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "processed text:\n",
      "\n",
      "In this section, we delve into the algorithms that have become prominent in the era of knowledge distillation with Language Models (LLMs). As per Section 2.4, these algorithms are categorized into two main steps: 'Knowledge' for extracting information from teacher LLMs, and 'Distillation' for integrating this knowledge into student models.\n",
      "\n",
      "The learning objective is defined as L=X ILI(D(kd) I;θS), (2) where D(kd) represents the datasets built for distillation, θS parameterizes t...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 36 ========================================\n",
      "INPUT TEXT:\n",
      "We will elaborate on these two processes in the subsequent sections. 3.1 Knowledge This section focuses on the approaches to elicit knowledge from teacher LLMs. According to the manners to acquire knowledge, we divided them into Labeling ,Expansion ,Data Curation ,Feature ,Feedback , and Self-Knowledge . Figure 5 shows an illustration of these knowledge elicitation meth- ods. 3.1.1 Labeling Labeling knowledge refers to using a teacher LLM to label the output yfor a given input xas the seed knowl...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "3.1 Knowledge Elicitation Methods\n",
      "\n",
      "This section delves into strategies for extracting knowledge from teacher Large Language Models (LLMs). These methods are categorized into Labeling, Expansion, Data Curation, Feature, Feedback, and Self-Knowledge. Figure 5 visualizes these knowledge elicitation techniques.\n",
      "\n",
      "3.1.1 Labeling\n",
      "\n",
      "Labeling involves using a teacher LLM to label the output y for a given input x as seed knowledge. This is done based on an instruction I or demonstrations c, where c consist...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 37 ========================================\n",
      "INPUT TEXT:\n",
      "y∼pT(y|I⊕c⊕x)}. (3) Input xcould be sourced from existing NLP task datasets, which serve as typical reservoirs for distillation efforts. Numerous works have sought to harness the capa- bilities of powerful LLMs as teachers for annotating dataset samples across a range of tasks. For instance, efforts in natural language understanding involve using LLMs to cat- egorize text (Gilardi et al., 2023; Ding et al., 2023a; He et al., 2023a), while in natural language generation, LLMs assist in generating...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "y~pT(y|I⊕c⊕x)}. (3) The input x can originate from NLP task datasets, often used for distillation purposes. Many studies have attempted to utilize the capabilities of large language models (LLMs) as teachers for annotating dataset samples across various tasks.\n",
      "\n",
      "In natural language understanding, LLMs are employed to categorize text (Gilardi et al., 2023; Ding et al., 2023a; He et al., 2023a). For natural language generation, LLMs aid in creating sequences for outputs (Hsieh et al., 2023; Jung et...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 38 ========================================\n",
      "INPUT TEXT:\n",
      "current works focus on labeling outputs based on instructions, thereby teaching student models to solve tasks in a more flexible way by following in- structions. Collections of various NLP tasks, complemented by instructional templates, serve as valuable input sources forx. For instance, FLAN-v2 collections (Longpre et al., 2023) offers extensive publicly available sets of tasks with instructions, which are labeled with responses generated by teacher LLMs in Orca (Mukherjee et al., 2023; Mitra e...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "processed text: Current research focuses on training models to follow instructions for various NLP tasks. Collections of these tasks, coupled with instructional templates, serve as valuable training data. For example, FLAN-v2 (Longpre et al., 2023) provides extensive publicly available task sets with instructions, labeled by responses generated by teacher LLMs in Orca (Mukherjee et al., 2023; Mitra et al., 2023). However, these instructional templates lack diversity and may not fully capture hum...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 39 ========================================\n",
      "INPUT TEXT:\n",
      "instructions Ior demonstrations c. A commonly used in- struction type for guiding labeling is chain-of-thought (CoT) prompt (Hsieh et al., 2023; Fu et al., 2023; Magister et al., 2023). Mukherjee et al. (2023) add multiple system messages (e.g. “You must generate a detailed and long answer.” or “explain like I’m five, think step-by-step”) to elicit rich signals. Yue et al. (2023a) and Chenglin et al. (2023) la- bel a hybrid of knowledge of chain-of-thought (CoT) and program-of-thought (PoT) rati...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Chain-of-thought (CoT) prompts are a common instruction type for guiding labeling. They've been used effectively by Hsieh et al. (2023), Fu et al. (2023), and Magister et al. (2023). Mukherjee et al. (2023) enhance this method by adding system messages to encourage detailed, step-by-step responses. Yue et al. (2023a) and Chenglin et al. (2023) combine chain-of-thought and program-of-thought rationales for labeling. Xu et al. (2023b) introduce a self-chat technique where two teacher LLMs mimic re...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 40 ========================================\n",
      "INPUT TEXT:\n",
      "involved. To address these limitations, various expansion methods have been proposed (Wang et al., 2022a; Taori et al., 2023; Chaud- hary, 2023; Si et al., 2023; Ji et al., 2023a; Luo et al., 2023b,a; Wu et al., 2023c; Sun et al., 2024b; Xu et al., 2023a; Guo et al., 2023c; Rozi `ere et al., 2023; West et al., 2022). These methods take the demonstrations as seed knowledge and aim to expand a large scale and various data by in-context learning. A key characteristic of these expansion methods is t...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "Various expansion methods have been proposed to tackle the limitations of large language models (LLMs). These methods, detailed in studies by Wang et al. (2022a), Taori et al. (2023), Chaudhary (2023), Si et al. (2023), Ji et al. (2023a), Luo et al. (2023b,a), Wu et al. (2023c), Sun et al. (2024b), Xu et al. (2023a), and Guo et al. (2023c), leverage the in-context learning capability of LLMs to generate a diverse, large-scale dataset similar to given demonstrations.\n",
      "\n",
      "Unlike the ...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 41 ========================================\n",
      "INPUT TEXT:\n",
      "𝑚Meta-Information𝑐Demonstrations𝑥𝐼 𝑦 FilterFeedback ExtractFeature𝑥𝑦 DistributionIntermediateFeature 𝑥Input𝑦Output𝐼Instruction𝑦! 𝑦\" 𝑦# 𝑥GuideFeedback𝑦#∗ 𝑦# Feedback Self-Knowledge StudentTeacher Generate≻≻𝑦\" 𝑦! 𝑦# 𝑥 𝑥& CorrectExpand𝑐 Fig. 5: An illustration of different knowledge elicitation methods from teacher LLMs. Labeling : The teacher generates the output from the input; Expansion : The teacher generates samples similar to the given demonstrations through in- context learning; Data Curatio...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Meta-Information Demonstrations Instruction Output Feedback\n",
      "Labeling: Teacher generates output from input\n",
      "Expansion: Teacher generates similar samples via in-context learning\n",
      "Data Curation: Teacher synthesizes data based on meta-information (topic or entity)\n",
      "Feature: Extract internal knowledge (logits, features) from teacher using provided data\n",
      "Feedback: Teacher offers corrections, expansions, preferences on student's generations\n",
      "Self-Knowledge: Student generates outputs, filtered for quality or...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 42 ========================================\n",
      "INPUT TEXT:\n",
      "xand yrepresent the new input- output pairs generated by the teacher LLM. The input x is generated based on a set of input-output demonstrations c. The output yis then generated in response to the new input xunder the guidance of an instruction I. Note that the demonstrations could be predefined or dynamically updated by adding the newly generated samples. Expansion techniques have been widely utilized to extract extensive instruction-following knowledge from teacher LLMs. Wang et al. (2022a) fi...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "x and y represent new input-output pairs produced by the teacher Large Language Model (LLM). Input x is derived from a set of initial input-output demonstrations c. Output y is then generated in response to the new input x, guided by an instruction I. These demonstrations can be predefined or updated dynamically by incorporating newly generated samples. Expansion techniques are commonly used to extract extensive instruction-following knowledge from teacher LLMs. Wang et al. (2022a) pioneered an ...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 43 ========================================\n",
      "INPUT TEXT:\n",
      "diversity and coverage during expansion, Wu et al. (2023c) and (Sun et al., 2024b) prompt the teacher LLM to generate instructions corresponding to some specific topics. Xu et al. (2023a) propose an Evol-Instruct method to ex- pand the instructions from two dimensions: difficulty (e.g. rewriting the question to be more complex) and diversity (e.g. generating more long-tailed instructions). This Evol- Instruct method is domain-agnostic and has been used to expand the distillation of coding (Luo e...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Wu et al. (2023c) and Sun et al. (2024b) use a teacher LLM to create instructions for specific topics. Xu et al. (2023a) introduce the Evol-Instruct method to broaden instruction sets in two ways: increasing complexity and enhancing diversity. This method, applicable across domains, has been employed to expand coding (Luo et al., 2023a) and math (Luo et al., 2023b) distillation.\n",
      "\n",
      "Expansion techniques can substantially enrich NLP task datasets with comparable samples, boosting performance. For ex...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 44 ========================================\n",
      "INPUT TEXT:\n",
      "automatically identifies challenging sub- groups within data and generates new samples for these subgroups using LLMs through in-context learning. In summary, the expansion method leverages the in- context learning strengths of LLMs to produce more var- ied and extensive datasets with both inputs and outputs. However, the quality and diversity of the generated data are heavily reliant on the teacher LLMs and the initial seed demonstrations. This dependence can lead to a dataset with inherent bia...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Automatically identifies challenging subgroups within data and generates new samples for these subgroups using LLMs through in-context learning. The expansion method utilizes LLMs' strengths in in-context learning to create more diverse and extensive datasets with both inputs and outputs. However, the quality and diversity of the generated data heavily depend on the teacher LLMs and initial seed demonstrations. This reliance can result in a dataset with inherent biases from LLMs (Yu et al., 2023...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 45 ========================================\n",
      "INPUT TEXT:\n",
      "sponse to the limitations observed in both the Labeling and Expansion approaches. These methods often yield data of variable quality and face constraints in quantity. In Labeling, the seed knowledge is sourced from task datasets, leading to potential noise and dirty data. Meanwhile, in Expansion, the input xis derived from seed demonstrations, which can result in homogeneous data when generated in large quantities. To overcome these challenges, the Data Curation method curates high-quality or la...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Data Curation addresses the limitations of Labeling and Expansion methods by producing high-quality or large-scale data using extensive meta-information as seed knowledge (Ding et al., 2023b; Gunasekar et al., 2023; Li et al., 2023a; Mar, 2023; Liu et al., 2023d; Wei et al., 2023; Yu et al., 2024; Ye et al., 2022; Gao et al., 2023a; Yang and Nicolai, 2023). Unlike Labeling's potential for noisy data from task datasets and Expansion's risk of homogeneous data in large quantities derived from seed...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 46 ========================================\n",
      "INPUT TEXT:\n",
      "process can be meticulously controlled to yield datasets that are not only large in scale but also of high quality. The formulation for Data Curation can be represented as: D(cur)={(x, y)|x∼pT(x|I⊕m), y∼pT(y|I⊕x)}.(5) In this formulation, mrepresents the diverse meta- information used to guide the synthesis of x, and Iis the instruction guiding teacher LLMs to generate xory. Different studies primarily vary in their source and method of leveraging meta-information. UltraChat (Ding et al., 2023b)...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "processed text:\n",
      "\n",
      "Data Curation can be meticulously controlled for large, high-quality datasets. The formulation is D(cur)={(x, y)|x∼pT(x|I⊕m), y∼pT(y|I⊕x)}. Here, m represents diverse meta-information guiding x's synthesis, and I guides teacher LLMs to generate x or y. Meta-information sources vary among studies. UltraChat (Ding et al., 2023b) curates high-quality, diverse data using distilled knowledge. They gather extensive meta-info across three domains: Questions about the World, Creation an...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 47 ========================================\n",
      "INPUT TEXT:\n",
      "substantial scale of 1.5 million instances. UltraChat stands out with its lexical and topical diversity. The UltraLLaMA model, fine- tuned on this data, consistently surpasses other open-source models. Another notable series, phi(Gunasekar et al., 2023; Li et al., 2023a; Mar, 2023), focuses on distilling smaller, high-quality datasets akin to ”textbooks.” Phi-1 (Gunasekar et al., 2023) experiments with synthesizing ”textbook qual- ity” data in the coding domain. Their approach involves distillin...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Substantial scale of 1.5 million instances sets UltraChat apart with its lexical and topical diversity. The UltraLLaMA model, fine-tuned on this data, outperforms other open-source models. Another series focuses on distilling smaller, high-quality datasets similar to \"textbooks.\" Phi-1 (Gunasekar et al., 2023) generates \"textbook quality\" data in the coding domain by synthesizing clear, self-contained, instructive, and balanced content from LLMs. This is guided by random topics or function names...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 48 ========================================\n",
      "INPUT TEXT:\n",
      "times smaller in model size and 100 times smaller in dataset size. MFTCoder (Liu et al., 2023d) utilizes hundreds of Python knowledge points as meta-information to create a CodeExercise Dataset. In contrast, Magicoder (Wei et al., 2023) and WaveCoder (Yu et al., 2024) get raw code collections from open-source code datasets, using this as meta-information for generating instructional data. In the context of NLU tasks, certain studies (Ye et al., 2022; Gao et al., 2023a; Wang et al., 2021a) explor...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "times smaller in size. This method involves leveraging meta-information such as Python knowledge points, raw code collections, or labels to generate instructional data. In NLU tasks, labels serve as meta-information for synthesizing samples, while in information retrieval tasks, documents are used to generate potential queries and build large-scale retrieval pairs. Overall, Data Curation through teacher LLMs is a promising technique for creating high-quality, diverse datasets that are significan...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 49 ========================================\n",
      "INPUT TEXT:\n",
      "large in scale. The success of models like phi-1 in specialized domains underscores the efficacy of this method. The abilityto create synthetic datasets will become a crucial technical skill and a key area of focus in AI (Li et al., 2023a). 3.1.4 Feature The previously discussed knowledge elicitation methods are typically applied to powerful black-box models, which are expensive and somewhat unreproducible due to calling API. In contrast, white-box distillation offers a more trans- parent and ac...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "processed text:\n",
      "\n",
      "The effectiveness of large-scale models, such as phi-1, in specialized domains highlights the potential of this method. The creation of synthetic datasets is emerging as a vital technical skill and focal point in AI (Li et al., 2023a). Traditional knowledge elicitation methods, often used for powerful black-box models that are costly and less reproducible due to API calls, contrast with white-box distillation. This approach is more transparent and accessible, utilizing output di...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 50 ========================================\n",
      "INPUT TEXT:\n",
      "2023; Liang et al., 2023a; Gu et al., 2024; Agarwal et al., 2024; Liu et al., 2023a; Wen et al., 2023; Wan et al., 2024a; Zhao and Zhu, 2023; Qin et al., 2023b; Boizard et al., 2024; Zhong et al., 2024). The typical method for acquiring this feature knowledge involves teacher LLMs annotating the output sequence y with its internal representations. These annotations are then distilled into the student model using methods such as Kullback-Leibler Divergence (KLD). The process of eliciting feature ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "2023: Research by Liang et al., Gu et al., Agarwal et al., Liu et al., Wen et al., Wan et al., Zhao and Zhu, Qin et al., Boizard et al., Zhong et al. The standard approach to acquiring feature knowledge involves teacher Language Learning Models (LLMs) annotating the output sequence y with internal representations. These annotations are then distilled into the student model using methods like Kullback-Leibler Divergence (KLD).\n",
      "\n",
      "The process of extracting feature knowledge can be outlined as: D(fea...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 51 ========================================\n",
      "INPUT TEXT:\n",
      "distributions (Sanh et al., 2019; Wen et al., 2023). To leverage the rich semantic and syntactic knowledge in intermediate layers of the teacher model, TED (Liang et al., 2023a) designs task-aware layer-wise distillation. They align the student’s hidden representations with those of the teacher at each layer, selectively extracting knowledge pertinent to the target task. Gu et al. (2024) and Agarwal et al. (2024) introduce a novel approach where the student model first generates sequences, terme...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Distillation techniques are employed in various models, including Sanh et al. (2019), Wen et al. (2023), and Liang et al. (2023a) for task-aware layer-wise distillation. This method aligns the student's hidden representations with the teacher's at each layer, focusing on task-relevant knowledge. Gu et al. (2024) and Agarwal et al. (2024) propose a novel approach where the student generates sequences ('self-generated sequences') and learns using feedback from the teacher. This is advantageous whe...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 52 ========================================\n",
      "INPUT TEXT:\n",
      "quantizing the LLMs, ensuring minimal loss of performance. Additionally, feature knowledge could serve as a potent source for multi-teacher knowledge distil- lation. Timiryasov and Tastet (2023) leverages an ensemble of GPT-2 and LLaMA as teacher models to extract output distributions. Similarly, FuseLLM (Wan et al., 2024a) inno- vatively combines the capabilities of various LLMs through a weighted fusion of their output distributions, integrating them into a singular LLM. This approach has the ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Quantizing large language models (LLMs) while preserving performance is a key focus. Feature knowledge can serve as a robust resource for multi-teacher knowledge distillation. Timiryasov and Tastet (2023) employ an ensemble of GPT-2 and LLaMA as teacher models to extract output distributions. FuseLLM (Wan et al., 2024a) innovatively merges various LLMs' capabilities through a weighted fusion of their output distributions, consolidating them into one LLM. This method could significantly boost the...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 53 ========================================\n",
      "INPUT TEXT:\n",
      "smaller models, its application is not suitable for black-box LLMs where internal parame- ters are inaccessible. Furthermore, student models distilled from white-box LLMs may underperform compared to their black-box counterparts, as the black-box teacher LLMs (e.g. GPT-4) tend to be more powerful. 3.1.5 Feedback Most previous works predominantly focus on one-way knowledge transfer from the teacher to the student for imitation, without considering feedback from the teacher on the student’s genera...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "Smaller models aren't ideal for black-box LLMs due to inaccessible internal parameters. Student models distilled from white-box LLMs might perform worse than their black-box counterparts, as the latter (like GPT-4) are typically more potent.\n",
      "\n",
      "Previous research largely concentrates on one-way knowledge transfer from teacher to student for imitation, neglecting feedback from the teacher on student generation. Teacher feedback usually provides guidance on student outputs via prefer...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 54 ========================================\n",
      "INPUT TEXT:\n",
      "where ydenotes the output generated by the student model in response to x, and ϕfb(·;θT))represents providing feedback from teacher LLMs. This operation evaluates the student’s output ygiven the input x, by offering assess- ment, corrective information, or other forms of guidance. This feedback knowledge can not only be distilled into the student to also generate feedback (such as creating a student preference model) but, more importantly, enable the student to refine its responses based on the ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Where 'y' denotes the student model's output in response to input 'x', and ϕfb(·;θT) signifies feedback from teacher Language Learning Models (LLMs). This process evaluates the student's output 'y' based on input 'x', providing assessment, correction, or guidance. This feedback knowledge can be distilled into the student for generating feedback or refining responses. Numerous methods have been studied to extract this advanced knowledge (Bai et al., 2022a; Luo et al., 2023b; Cui et al., 2023a; Kw...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 55 ========================================\n",
      "INPUT TEXT:\n",
      "teachers by prompting it with specific criteria. Bai et al. (2022a) in- troduce RLAIF for distilling harmlessness preferences from LLMs. This involves using an SFT-trained LLM to generate response pairs for each prompt, then ranking them for harmlessness to create a preference dataset. This dataset is distilled into a Preference Model (PM), which then guides the RL training of a more harmless LLM policy. Wizard- Math (Luo et al., 2023b) places emphasis on mathematical reasoning. They employ Chat...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Bai et al. (2022a) introduced RLAIF for extracting harmlessness preferences from LLMs. This process involves an SFT-trained LLM generating response pairs for each prompt, ranking them for harmlessness to create a preference dataset. This dataset is then distilled into a Preference Model (PM), which guides the RL training of a more harmless LLM policy.\n",
      "\n",
      "Wizard-Math (Luo et al., 2023b) focuses on mathematical reasoning. It uses ChatGPT as a teacher to provide process supervision and evaluate the c...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 56 ========================================\n",
      "INPUT TEXT:\n",
      "and helpfulness. Beyond merely assessing student generations, teachers can also furnish extensive feedback on instances where students underperform. In Lion (Jiang et al., 2023b), teacher model pinpoints instructions that pose challenges to the student model, generating new, more difficult instructions aimed at bolstering the student’s abilities. PERsD (Chen et al., 2023a) showcases a method where teacher offers tailored refinement feedback on incorrect code snippets gen- erated by students, gui...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "Teachers can provide detailed feedback beyond just grading student work. In Lion (Jiang et al., 2023b), a teacher model identifies challenging instructions for the student, generating tougher ones to enhance the student's skills. PERsD (Chen et al., 2023a) demonstrates a method where the teacher offers personalized refinement feedback on incorrect code snippets, guided by specific execution errors. SelFee (Ye et al., 2023) uses ChatGPT to generate feedback and revise student ans...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 57 ========================================\n",
      "INPUT TEXT:\n",
      "wherein the student model initially generates sequences, followed by teacher model producing an output distribution as feedback. This method leverages the teacher’s insight to directly inform and refine the student model’s learning process. 3.1.6 Self-Knowledge The knowledge could also be elicited from the student itself, which we refer to as Self-Knowledge . In this setting, the same model acts both as the teacher and the student, iteratively improving itself by distilling and refining its own ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "The student model first produces sequences, then the teacher model generates an output distribution as feedback. This approach utilizes the teacher's expertise to directly influence and enhance the student model's learning process. In Self-Knowledge, the same model functions both as teacher and student, iteratively improving itself by refining its own previously generated outputs. This method eliminates the necessity for an external, potentially proprietary, advanced teacher model like GPT-serie...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 58 ========================================\n",
      "INPUT TEXT:\n",
      "which could include but is not limited to filtering, rewarding, or any other mechanisms for enhancing or evaluating y. It could be governed by external tools or the student itself θS. Recent research in this area has proposed various innovative methodologies to elicit self-knowledge, demonstrating its potential for creating more efficient and autonomous learn- ing systems. (Allen-Zhu and Li, 2020; Wang et al., 2022a; Sun et al., 2024b; Yang et al., 2024; Jung et al., 2023; Huang et al., 2023a; G...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "Recent research explores methodologies to enhance self-knowledge in learning systems, including filtering, rewarding, and evaluation mechanisms. These could be external or student-driven. Innovative approaches have shown potential for more efficient and autonomous learning. Notable among these is Self-Instruct (Wang et al., 2022a), which employs GPT-3 for data augmentation via the Expansion approach, creating additional data samples to enrich the dataset. This fine-tuned dataset...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 59 ========================================\n",
      "INPUT TEXT:\n",
      "model. Other methods aim to elicit targeted knowledge 11 from student models by modifying prompts, and leveraging these data for further refinement. In Self-Align (Sun et al., 2024b), they find that models fine-tuned by Self-Instruct data tend to generate short or indirect responses. They prompt this model with verbose instruction to produce in- depth and detailed responses. Then, they employ context- distillation (Askell et al., 2021) to distill these responses paired with non-verbose instructi...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "model refinement by filtering out infeasible responses and using them for training. In Self-Align, they address the issue of brief or roundabout answers from fine-tuned models by providing detailed instructions and employing context distillation to transfer this knowledge back to the model. RLCD, on the other hand, uses contrasting prompts to create preference pairs, which are then used to train a preference model guiding the improvement of an unaligned LLM via reinforcement learning. Various me...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 60 ========================================\n",
      "INPUT TEXT:\n",
      "summarization tasks, implementing filters based on entailment, length, and diversity to screen self-generated summaries. LMSI (Huang et al., 2023a) generates multiple CoT reasoning paths and answers for each question, and then retains only those paths that lead to the most consistent answer. Note that refined self-knowledge can be iteratively ac- quired as the student model continuously improves, further enhancing the student’s capabilities. This is Gulcehre et al. (2023) introduces a Reinforced...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "processed text:\n",
      "\n",
      "The tasks involve summarization with filters for entailment, length, and diversity. LMSI (Huang et al., 2023a) creates multiple CoT reasoning paths and answers per question, retaining only those leading to the most consistent answer. Self-knowledge can iteratively improve as the student model enhances. Gulcehre et al. (2023) propose a Reinforced Self-Training (ReST) framework, alternating between Grow and Improve stages. In the Grow stage, the student model generates multiple ou...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 61 ========================================\n",
      "INPUT TEXT:\n",
      "2024a) introduces a framework resembling iterative DPO, where the language model is fine-tuned to differentiate the self-generated responses from the human-annotated data. These self-generated responses could be seen as “negative knowledge” to promote the student to better align with the target distribution. Self-Rewarding (Yuan et al., 2024a) explores a novel and promising approach by utilizing the language model itself as a reward model. It employs LLM- as-a-Judge prompting to autonomously ass...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "2024a presents a framework akin to iterative DPO. This involves fine-tuning a language model to distinguish self-generated responses from human-annotated data, treating these as \"negative knowledge\" to enhance alignment with the target distribution. Self-Rewarding (Yuan et al., 2024a) introduces an innovative method using the language model itself as a reward model. It employs LLM-as-a-Judge prompting for self-generated responses, enabling autonomous reward assignment. This iterative process enh...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 62 ========================================\n",
      "INPUT TEXT:\n",
      "and Rank Optimization , as shown in Figure 3. 3.2.1 Supervised Fine-Tuning Supervised Fine-Tuning (SFT), or called Sequence-Level KD (SeqKD) (Kim and Rush, 2016), is the simplest and one of the most effective methods for distilling powerful black-boxDivergence Type D(p, q)Function Forward KLDPp(t) logp(t) q(t) Reverse KLDPq(t) logq(t) p(t) JS Divergence1 2\u0010Pp(t) log2p(t) p(t)+q(t)+Pq(t) log2q(t) p(t)+q(t)\u0011 TABLE 1: Functional forms of Dfor various divergence types. p: reference Similarity Functi...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Supervised Fine-Tuning (SFT), also known as Sequence-Level KD (SeqKD), is a straightforward yet effective method for knowledge distillation from powerful black-box Language Models (LLMs). It involves finetuning the student model to maximize the likelihood of sequences generated by the teacher LLMs, aligning the student's predictions with those of the teacher.\n",
      "\n",
      "The divergence types used in this process are Forward KL Divergence, Reverse KL Divergence, and Jensen-Shannon (JS) Divergence, as outlin...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 63 ========================================\n",
      "INPUT TEXT:\n",
      "formulated as minimizing the objective function: LSFT=Ex∼X,y∼pT(y|x)[−logpS(y|x)], (9) where yis the output sequence produced by the teacher model. This simple yet highly effective technique forms the basis of numerous studies in the field. Numerous re- searchers have successfully employed SFT to train student models using sequences generated by teacher LLMs (Taori et al., 2023; Chiang et al., 2023; Wu et al., 2023c; Xu et al., 2023a; Luo et al., 2023b). Additionally, SFT has been ex- plored in ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "LSFT is an objective function formulated as minimizing Ex∼X,y∼pT(y|x)[−logpS(y|x)], where y is the output sequence produced by the teacher model. This technique has been widely used and proven effective in numerous studies. Researchers have successfully employed SFT to train student models using sequences generated by teacher LLMs (Taori et al., 2023; Chiang et al., 2023; Wu et al., 2023c; Xu et al., 2023a; Luo et al., 2023b). SFT has also been explored in many self-distillation works (Wang et a...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 64 ========================================\n",
      "INPUT TEXT:\n",
      "groups: those minimizing divergence in probability distributions and those aimed at enhancing the similarity of hidden states. Divergence. Divergence-based methods minimize diver- gence between the probability distributions of the teacher and student models, represented by a general divergence function D: LDiv= E x∼X,y∼Y[D(pT(y|x), pS(y|x))], (10) The specific form of Dvaries depending on the type of divergence employed. Table 1 outlines the functional forms ofDfor different divergence measures....\n",
      "\n",
      "PROCESSED TEXT:\n",
      "processed text:\n",
      "\n",
      "Divergence-based methods in machine learning minimize the difference between the probability distributions of teacher and student models. This is represented by a general divergence function D, as shown in equation (10): LDiv = E x∼X,y∼Y[D(pT(y|x), pS(y|x))]. The type of divergence used varies, with specific forms outlined in Table 1.\n",
      "\n",
      "The standard Knowledge Distillation (KD) objectives typically minimize the approximated forward Kullback-Leibler divergence (KLD) between the tea...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 65 ========================================\n",
      "INPUT TEXT:\n",
      "predom- inantly on the most prominent mode, thereby exhibiting a “mode-seeking” behavior. Wen et al., 2023; Timiryasov and Tastet, 2023; Liang et al., 2023a; Chen et al., 2024d) , which forces pSto cover all the modes of pT. However, when a student model is unable to learn all modes of a highly complex teacher, the re- sultant “mode-covering” behavior might cause the student to assign probability mass to tokens with low probability under the teacher’s distribution (cf. Figure 6 blue curve). This...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "The research indicates that models tend to prioritize the most prominent mode, displaying \"mode-seeking\" behavior (Wen et al., 2023; Timiryasov and Tastet, 2023; Liang et al., 2023a; Chen et al., 2024d). This forces the student model to cover all modes of the teacher's distribution. However, when a student model struggles to learn all modes of a highly complex teacher, it may exhibit \"mode-covering\" behavior (Figure 6 blue curve), potentially leading to hallucinations and low-quality generations...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 66 ========================================\n",
      "INPUT TEXT:\n",
      "low-probability regions of the teacher’s distribution, employing Policy Gradient methods for optimization. Both Agarwal et al. (2024) and Sason and Verd ´u (2016) assess the effect of different divergence func- tions in LLM distillation, finding the optimal divergence to be task-dependent. For instance, forward KL divergence is more suitable for tasks like Machine Translation, where the output has fewer modes or variations, while reverse KL divergence is preferable for tasks like dialogue genera...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Agarwal et al. (2024) and Sason & Verdú (2016) studied the impact of divergence functions in Language Model (LLM) distillation, revealing task-dependent optimal divergence. Forward KL divergence is favored for tasks like Machine Translation due to fewer output modes, while reverse KL divergence suits tasks such as dialogue generation and instruction tuning, which require multiple modes and varied responses. Task characteristics thus play a crucial role in choosing the best divergence function fo...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 67 ========================================\n",
      "INPUT TEXT:\n",
      "two models. The objective is to ensure that the student model not only produces similar outputs to the teacher but also processes information in a comparable manner. The formulation for a similarity-based objective might look like this: LSim= E x∼X,y∼Y[LF(ΦT(fT(x, y)),ΦS(fS(x, y)))],(11) where fT(x, y)andfS(x, y)are the feature maps of the teacher and student models, respectively. The transforma-tion functions ΦTandΦSare applied to these feature maps to ensure they are in the same shape, facilit...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "Two models are compared using a similarity-based objective. This ensures the student model not only generates similar outputs to the teacher but also processes information in a comparable way. The formula for this objective is LSim= E x∼X,y∼Y[LF(ΦT(fT(x, y)),ΦS(fS(x, y)))], where fT(x, y) and fS(x, y) are the feature maps of the teacher and student models respectively. Transformation functions ΦT and ΦS are applied to these feature maps to ensure they match in shape for direct c...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 68 ========================================\n",
      "INPUT TEXT:\n",
      "discrepancy between the filtered representations in both teacher and student models. While similarity-based approaches are common in encoder-based LMs (Sun et al., 2019, 2020; Jiao et al., 2020; Hou et al., 2020; Zuo et al., 2022; Liang et al., 2021), their application in LLM knowledge distillation is not as widespread. However, considering their effectiveness, we anticipate an increase in research exploring these methods for LLM distillation in the near future. 3.2.3 Reinforcement Learning This...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "discrepancy in filtered representations between teacher and student models in encoder-based LMs (Sun et al., 2019, 2020; Jiao et al., 2020; Hou et al., 2020; Zuo et al., 2022; Liang et al., 2021). Despite their common use in similarity-based approaches, RL methods for LLM knowledge distillation are less prevalent. Nevertheless, expected growth in research for these techniques due to their effectiveness.\n",
      "\n",
      "3.2.3 Reinforcement Learning: This section delves into advanced knowledge distillation metho...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 69 ========================================\n",
      "INPUT TEXT:\n",
      "involves training a reward model rϕusing the feedback data D(fd) generated by teacher LLMs. Preference data, as one of the typical feedback, is employed to train the student reward model (Bai et al., 2022a; Cui et al., 2023a; Lee et al., 2023a; Kim et al., 2023a). They usually consist of input-output pairs (x, yw, yl). Here, ywandylrepresent “winning” and “losing” outputs relative to the teacher’s preferences. The loss function for the reward model is defined as: LRM(rϕ,D(fd)) =− E (x,yw,yl)∼D(f...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Involves training a reward model, rϕ, using feedback data D(fd) generated by teacher language models (LLMs). Preference data, a common type of feedback, is used to train the student reward model (Bai et al., 2022a; Cui et al., 2023a; Lee et al., 2023a; Kim et al., 2023a). This preference data typically consists of input-output pairs (x, yw, yl), where ywandylrepresent \"winning\" and \"losing\" outputs relative to the teacher's preferences.\n",
      "\n",
      "The loss function for the reward model is defined as: LRM(...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 70 ========================================\n",
      "INPUT TEXT:\n",
      "Learning Optimization. In the second stage, the student model, represented by a policy πθ, is optimized to maximize the expected reward as per the trained reward model. Simultaneously, it minimizes the divergence from a reference policy πref, typically the initial policy of the student model trained by SFT, controlled by a factor β. The RL objective is given by: 13 max πθE x∼X,y∼πθ(y|x)[rϕ(x, y)]−βDKL[πθ(y|x)∥πref(y|x)] (13) This RL framework not only ensures that the student model learns the ex...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "In the second stage, the student model, symbolized by policy πθ, is optimized to maximize expected rewards based on the trained reward model. Simultaneously, it minimizes divergence from a reference policy πref, usually the initial policy of the student model trained via SFT, influenced by a factor β. The RL objective is expressed as: max πθE x∼X,y∼πθ(y|x)[rϕ(x, y)]−βDKL[πθ(y|x)∥πref(y|x)]. This RL framework ensures the student model learns explicit content from the teacher and effectively adopt...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 71 ========================================\n",
      "INPUT TEXT:\n",
      "performance, it comes at a higher computational cost compared to employing a smaller distilled reward model. 3.2.4 Ranking Optimization Ranking optimization presents a stable and computationally efficient alternative to RL for injecting preference feedback into language models (Rafailov et al., 2023; Song et al., 2023a; Yuan et al., 2023b). This method, diverging from traditional RL approaches, directly incorporates ranking information into language models from a fixed preference dataset during ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Performance, however, comes with a higher computational cost when using larger distilled reward models. Ranking optimization offers a stable and computationally efficient alternative to reinforcement learning for integrating preference feedback into language models (Rafailov et al., 2023; Song et al., 2023a; Yuan et al., 2023b). Unlike traditional RL methods, ranking optimization directly integrates ranking data from a fixed preference dataset during fine-tuning. This approach updates the policy...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 72 ========================================\n",
      "INPUT TEXT:\n",
      "Preference Optimization (DPO) (Rafailov et al., 2023) to distill the preference alignment in teacher LLMs. DPO streamlines the objective of reinforcement learning (as in Eq. 13), which involves reward maximization with a KL-divergence constraint, into a single-stage policy training. Specifically, DPO’s training goal is to maximize the following expecta- tion: E (x,yw,yl)∼D(fd)\u0014 logσ\u0012 βlogπθ(yw|x) πref(yw|x)−βlogπθ(yl|x) πref(yl|x)\u0013\u0015 , (14) where ywis preferred over ylaccording to the teacher LLM...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Preference Optimization (DPO) simplifies reinforcement learning's objective, involving reward maximization with a KL-divergence constraint, into single-stage policy training. DPO aims to maximize the expectation: E(x,yw,yl)~D(fd) [logσ(βlogπθ(yw|x)/πref(yw|x) - βlogπθ(yl|x)/πref(yl|x))], where yw is preferred over yl by the teacher LLM.\n",
      "\n",
      "Hong et al. (2023) use two ranking-based optimization objectives for preference distillation: Rank Responses to align Human Feedback (RRHF) and Preference Ranki...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 73 ========================================\n",
      "INPUT TEXT:\n",
      "probabilities under the policy πθ. This approach emphasizes direct comparison and ranking of responses based on the teacher’s preferences. PRO (Song et al., 2023a) expands the concept of pairwisecomparison to handle preference rankings of any length. For a given instruction xand a sequence of responses ordered by teacher preference as y1≻y2≻...≻yn, the RPO training objective is: LPRO=−n−1X k=1logexp (pk)Pn i=kexp (pi), (16) where pkrepresents the conditional log probabilities for ykunder the stu...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "processed text:\n",
      "\n",
      "PRO (Song et al., 2023a) introduces a training objective, LPRO, for ranking responses based on teacher preferences. For an instruction x and teacher-preferred sequence y1≻y2≻...≻yn, LPRO is defined as: LPRO = -∑(n-1)/k=1 logexp(pk)/Pn i=kexp(pi). Here, pk denotes the conditional log probabilities for yk under the student policy πθ. This method contrasts response likelihoods iteratively, optimizing the student LM to favor the most preferred response and rank the others in order o...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 74 ========================================\n",
      "INPUT TEXT:\n",
      "range of skills exhibited by LLMs, including Context Following ,Alignment ,Agent ,NLP Task Specializa- tion and Multi-Modality .Context Following focuses on the student’s ability to comprehend and respond effectively to input information. Alignment delves into the student’s capability to align its output with the teacher’s responses. Moving forward, Agent underscores the autonomous nature of language models. NLP Task Specialization highlights the LLM’s versatility in specializing across various ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "range of skills in Large Language Models (LLMs): Context Following, Alignment, Agent, NLP Task Specialization, and Multi-Modality.\n",
      "\n",
      "Context Following: evaluates a model's ability to understand and respond effectively to input information.\n",
      "\n",
      "Alignment: assesses the model's capacity to align its output with the teacher's responses.\n",
      "\n",
      "Agent: emphasizes the autonomous nature of language models.\n",
      "\n",
      "NLP Task Specialization: showcases LLMs' versatility in adapting to various Natural Language Processing tas...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 75 ========================================\n",
      "INPUT TEXT:\n",
      "transferring the ability of LLMs to handle a variety of complex contexts — such as few-shot demonstrations, intricate instructions, dia- logue history, and retrieval-augmented information — into smaller models. Many research efforts in this domain aim to imbue smaller models with these sophisticated, context- following capabilities. Our discussion here will dissect this facet of skill distillation, categorizing it based on different types of context and elaborating on how each is distilled and i...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "transferring complex context handling abilities from LLMs to smaller models, including few-shot demonstrations, detailed instructions, dialogue history, and retrieval-augmented information. Research focuses on equipping smaller models with these advanced, context-aware skills. This discussion explores skill distillation types based on various contexts and explains their integration into compact, efficient models.\n",
      "\n",
      "4.1.1 Instruction Following: This capability lets LLMs comprehend and execute user...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 76 ========================================\n",
      "INPUT TEXT:\n",
      "can be manually curated by human experts or transformed from existing NLP tasks into instructional 14 Methods Skill Seed Knowledge Teacher LLM Student Model Knowledge Elicitation Objective Context Following Self-Instruct (Wang et al., 2022a) IF 175 human-curated tasks GPT3 GPT3 Expansion + Self-Knowledge SFT Alpaca (Taori et al., 2023) IF 175 human-curated tasks GPT3 LLaMA Expansion + Self-Knowledge SFT LaMini-LM (Wu et al., 2023c) IF3.5K Wikipedia Categories + Mixed DatasetChatGPT Various Model...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "1. Methods: Skill Seed Knowledge\n",
      "   - Teacher LLM, Student Model\n",
      "   - Knowledge Elicitation, Objective Context Following\n",
      "   - Self-Instruct (Wang et al., 2022a) uses 175 human-curated tasks with GPT3\n",
      "   - Alpaca (Taori et al., 2023) employs 175 human-curated tasks with GPT3 and LLaMA Expansion + Self-Knowledge SFT\n",
      "\n",
      "2. Other Methods:\n",
      "   - LaMini-LM (Wu et al., 2023c): Uses IF3.5K Wikipedia Categories + Mixed Dataset, ChatGPT, various models, Expansion SFT\n",
      "   - WizardLM (Xu et al....\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 77 ========================================\n",
      "INPUT TEXT:\n",
      "Self-Rewarding (Yuan et al., 2024a) IF Human-written Samples LLaMA LLaMA Self-Knowledge SFT + RL STaR (Zelikman et al., 2022) IF Arithmetic + CommonsenseQA + GSM8K GPT-J GPT-J Self-Knowledge SFT Llama-GPT4 (Peng et al., 2023a) IF Alpaca Dataset GPT4 LLaMA Labeling SFT Reflection-Tuning (Li et al., 2023e) IF Alpaca/WizardLM Dataset ChatGPT LLaMA Labeling SFT Selective Reflection-Tuning (Li et al., 2024d) IF Alpaca/WizardLM Dataset ChatGPT LLaMA Labeling SFT Vicuna (Chiang et al., 2023) IF/MD Huma...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Self-Rewarding Models:\n",
      "\n",
      "1. Yuan et al. (2024a): Human-written Samples, LLaMA, Self-Knowledge, SFT + RL, STaR (Zelikman et al., 2022)\n",
      "2. Zelikman et al. (2022): Arithmetic + CommonsenseQA + GSM8K, GPT-J, Self-Knowledge, SFT\n",
      "3. Peng et al. (2023a): Alpaca Dataset, GPT4, LLaMA Labeling, SFT\n",
      "4. Li et al. (2023e): Alpaca/WizardLM Dataset, ChatGPT, LLaMA Labeling, SFT, Reflection-Tuning\n",
      "5. Li et al. (2024d): Alpaca/WizardLM Dataset, ChatGPT, LLaMA Labeling, SFT, Selective Reflection-Tuning\n",
      "6. Chiang e...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 78 ========================================\n",
      "INPUT TEXT:\n",
      "et al., 2023) IF/TP Human Conv, Flan/Code/Math Collection ChatGPT LLaMA Labeling SFT CoT-Distill (Hsieh et al., 2023) IF/TP e-SNLI + ANLI + CQA + SVAMP PaLM T5 Labeling SFT KnowPAT (Zhang et al., 2023a) IF/TP CPKG + QA Data ChatGPT + ChatGLM + Vicuna-7B LLaMA Labeling SFT DEBATunE (Li et al., 2024e) IF/TP Controversial Topics ChatGPT LLaMA Labeling SFT Phi-1 (Gunasekar et al., 2023) IF/Code - GPT3.5 phi-1 Curation SFT Phi-1.5 (Li et al., 2023a) IF/Code 20k Topics from Web GPT3.5 phi-1 Curation +...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "- et al., 2023: Human Conv, Flan/Code/Math Collection ChatGPT LLaMA Labeling SFT CoT-Distill\n",
      "- Hsieh et al., 2023: e-SNLI + ANLI + CQA + SVAMP PaLM T5 Labeling SFT KnowPAT\n",
      "- Zhang et al., 2023a: CPKG + QA Data ChatGPT + ChatGLM + Vicuna-7B LLaMA Labeling SFT\n",
      "- Li et al., 2024e: Controversial Topics ChatGPT LLaMA Labeling SFT\n",
      "- Gunasekar et al., 2023: - GPT3.5 phi-1 Curation SFT Phi-1\n",
      "- Li et al., 2023a: 20k Topics from Web GPT3.5 phi-1 Curation + Labeling SFT\n",
      "- Luo et al., 2023c...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 79 ========================================\n",
      "INPUT TEXT:\n",
      "Human-written Prompts LLaMA LLaMA Expansion + Labeling SFT + RL RLCD (Yang et al., 2024) IF/Preference Human-written Prompts LLaMA LLaMA Labeling SFT + RL RLAIF (Lee et al., 2023a) IF/Preference Human-written Prompts PaLM 2 PaLM 2 Labeling + Feedback RL GPT3 Reward (Kwon et al., 2023) Preference Human-written Prompts GPT3 GPT3 Labeling RL ILF (Scheurer et al., 2023) Preference Task-specific Datasets GPT3 + FeedME GPT3 Labeling RL ULTRAFEEDBACK (Cui et al., 2023a) Preference Mixed Datasets GPT4 L...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "1. Human-written Prompts LLaMA, Expansion + Labeling SFT + RL RLCD (Yang et al., 2024) - IF/Preference\n",
      "2. Human-written Prompts PaLM 2, Labeling + Feedback RL GPT3 Reward (Kwon et al., 2023) - Preference\n",
      "3. Human-written Prompts GPT3, Labeling RL ILF (Scheurer et al., 2023) - Preference\n",
      "4. Task-specific Datasets GPT3 + FeedME, Labeling RL ULTRAFEEDBACK (Cui et al., 2023a) - Preference\n",
      "5. Mixed Datasets GPT4 LLaMA, Labeling RL Constitutional AI (Bai et al., 2022a) - Preference/Va...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 80 ========================================\n",
      "INPUT TEXT:\n",
      "API Documentation GPT4 LLaMA Expansion SFT GPT4Tools (Yang et al., 2023b) Tool Image Content ChatGPT LLaMA Curation + Expansion SFT ToolAlpaca (Tang et al., 2023a) Tool Public-apis Repository ChatGPT LLaMA Curation SFT ToolLLM (Qin et al., 2023a) Tool Real-world APIs ChatGPT LLaMA Curation SFT MLLM-Tool (Wang et al., 2024) Tool HuggingFace Model Cards GPT4 LLaMA Curation SFT FireAct (Chen et al., 2023b) Planning Mixed QA Dataset GPT4 LLaMA Labeling SFT AgentTuning (Zeng et al., 2023a) Planning 6...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "API Documentation:\n",
      "- GPT4 LLaMA Expansion, SFT, Tools by Yang et al. (2023b), Image Content, ChatGPT, LLaMA Curation + Expansion\n",
      "- Alpaca (Tang et al., 2023a), Tool from Public-apis Repository, ChatGPT, LLaMA Curation, SFT\n",
      "- LLM (Qin et al., 2023a), Tool for Real-world APIs, ChatGPT, LLaMA Curation, SFT\n",
      "- MLLM-Tool (Wang et al., 2024), HuggingFace Model Cards, GPT4, LLaMA Curation, SFT\n",
      "- FireAct (Chen et al., 2023b), Planning for Mixed QA Dataset, GPT4, LLaMA Labeling, SFT\n",
      "- AgentTuning (Zeng et...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 81 ========================================\n",
      "INPUT TEXT:\n",
      "al., 2021a) NLU NLU Tasks GPT3 BERT Expansion SFT InheritSumm (Xu et al., 2023c) NLG Pile + ArXiv + CNN/DM + WikiHow GPT3.5 ZCode++ Label SFT DIMSUM+ (Jung et al., 2023) NLG None GPT2 + CTRL + BioGPT T5 Curation + Self-Knowledge SFT Genie (Yehudai et al., 2024) NLG ELI5 + ASQA + NQ + CNN/DM Falcon + LLaMA FLAN + LLaMA Label SFT GKD (Agarwal et al., 2024) NLG/NLU/IF XSum+WMT14 en-de+GSM8K+FLAN2021 T5-XL T5 Feature + Feedback D&S + RL QUILL (Srinivasan et al., 2022) IR IR Datasets T5 4-layer Trans...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "- NLU Tasks: GPT3, BERT, Expansion, SFT, InheritSumm (Xu et al., 2023c)\n",
      "- NLG: Pile + ArXiv + CNN/DM + WikiHow, GPT3.5, ZCode++ Label SFT, DIMSUM+ (Jung et al., 2023)\n",
      "- NLG/NLU/IF: XSum+WMT14 en-de+GSM8K+FLAN2021, T5-XL, Feature + Feedback D&S + RL\n",
      "- IR Datasets: T5 4-layer Transformer, Internal Knowledge D&S\n",
      "- Recommendation: GPT3 MPnet-110M, InstrcutRec (Zhang et al., 2023b), ChatGPT LLaMA Labeling SFT, ONCE (Liu et al., 2023c)...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 82 ========================================\n",
      "INPUT TEXT:\n",
      "PandaLM (Wang et al., 2023b) Evaluation Alpaca Data ChatGPT LLaMA Labeling SFT Prometheus (Kim et al., 2024) Evaluation 50 Seed Rubrics GPT4 LLaMA Labeling SFT InstructScore (Xu et al., 2023d) Evaluation Mixed Dataset GPT4 LLaMA Labeling SFT WizardMath (Luo et al., 2023b) Math GSM8k + MATH ChatGPT LLaMA Expansion + Feedback SFT + RL Mammoth (Yue et al., 2023a) Math/TP Mixed Math Dataset GPT4 LLaMA Labeling SFT Mixed Distill (Chenglin et al., 2023) Math/TP SVAMP + GSM8K + ASDIV + StrategyQA ChatG...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "PandaLM, Alpaca Data, ChatGPT, LLaMA, Labeling, SFT\n",
      "\n",
      "Alpaca Evaluation: GPT4, LLaMA, Labeling, SFT\n",
      "\n",
      "Prometheus (Kim et al., 2024) Evaluation: 50 Seed Rubrics, GPT4, LLaMA, Labeling, SFT\n",
      "\n",
      "InstructScore (Xu et al., 2023d) Evaluation: Mixed Dataset, GPT4, LLaMA, Labeling, SFT\n",
      "\n",
      "WizardMath (Luo et al., 2023b): Math, GSM8k + MATH, ChatGPT, LLaMA, Expansion + Feedback, SFT + RL\n",
      "\n",
      "Mammoth (Yue et al., 2023a): Math/TP, Mixed Math Dataset, GPT4, LLaMA, Labeling, SFT\n",
      "\n",
      "Mixed Distill (Chenglin et al., 2023): ...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 83 ========================================\n",
      "INPUT TEXT:\n",
      "al., 2023) Code Code Datasets ChatGPT LLaMA Labeling SFT Multi-Modality LLaVA (Liu et al., 2023e) Vision-Language COCO GPT4 LLaMA Labeling SFT SVIT (Zhao et al., 2023b) Vision-Language Visual Genome + COCO GPT4 LLaMA Labeling SFT LVIS-Instruct4V (Wang et al., 2023e) Vision-Language LVIS GPT4V LLaMA Labeling SFT LLaVAR (Zhang et al., 2023d) Vision-Language LAION GPT4 LLaMA Labeling SFT Macaw-LLM (Lyu et al., 2023) Multiple Modalities Image/Video with Caption ChatGPT LLaMA Labeling SFT MIMIC-IT (L...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "Here's a summary of notable skill distillation works:\n",
      "\n",
      "1. **Code**: Utilizes Code datasets and employs Supervised Fine-Tuning (SFT) with ChatGPT and LLaMA.\n",
      "2. **Datasets**: Employs COCO, Visual Genome + COCO, LVIS for Vision-Language tasks, utilizing GPT4 and LLaMA with SFT.\n",
      "3. **Macaw-LLM**: Handles multiple modalities (Image/Video with Caption), using ChatGPT and LLaMA for Labeling with SFT.\n",
      "4. **MIMIC-IT**: Manages Multiple Modalities (Image/Video Dataset), leveraging ChatGPT...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 84 ========================================\n",
      "INPUT TEXT:\n",
      "Divergence and Similarity, RL: Reinforcement Learning, RO: Ranking Optimization. formats with templates, such as prefacing machine transla- tion data with ”Translate this sentence to Spanish:” . However, these approaches have limitations. Manual data creation is labor-intensive, while template-based transformation lacks diversity in instructions and may not align well with natural human input. LLMs like GPT-4 offer an efficient alternative for creating diverse and controlled SFT data by their ca...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "Large Language Models (LLMs), such as GPT-4, provide an efficient method for generating diverse and controlled Supervised Fine-Tuning (SFT) data. This is due to their capabilities in in-context learning and instruction following. Most relevant studies utilize OpenAI's GPT series models to produce prompt-response data pairs. These pairs are then used to train student LLMs via supervised fine-tuning. Notable works include Wang et al., 2022a; Taori et al., 2023; Chiang et al., 2023...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 85 ========================================\n",
      "INPUT TEXT:\n",
      "GPT-3 to expand 15 a seed pool of 175 tasks to 52K task-agnostic instructions, ensuring a broad spectrum of general instructions. Addi- tionally, a filtering and post-processing stage is introduced to eliminate redundant or similar instructions. Notably, through training with this enriched dataset, GPT-3 acquires the ability to follow instructions, enabling it to perform comparably to InstructGPT in zero-shot instruction tasks and when provided with expert-written instructions for novel tasks. B...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "GPT-3 expands a seed pool of 15 tasks into 52,000 task-agnostic instructions, covering a wide range of general directions. A filtering and post-processing phase is implemented to discard redundant or similar instructions. Training with this enhanced dataset allows GPT-3 to follow instructions effectively, achieving comparable performance to InstructGPT in zero-shot instruction tasks and with expert-crafted instructions for unseen tasks.\n",
      "\n",
      "Following the self-instruct method, Taori...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 86 ========================================\n",
      "INPUT TEXT:\n",
      "Complex Instructions. Some works promote students to solve more complex instructions (Xu et al., 2023a; Luo et al., 2023b,a; Guo et al., 2023c). According to Xu et al. (2023a), in- struction datasets derived from human-written seeds often exhibit low to moderate complexity. To enhance the com- plex instruction-following capabilities of smaller models, WizardLM (Xu et al., 2023a) introduces Evol-Instruct . This method gradually transforms instructions into more com- plex forms through a multi-ste...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "Xu et al. (2023a) developed Evol-Instruct to boost smaller models' ability to handle complex instructions. This method evolves simple instructions into more intricate forms through a multi-step process, enhancing difficulty levels and topic diversity. They used the OpenAI ChatGPT API for four rounds of evolution, generating a 250k complex instruction dataset. Trained on this, WizardLM (LLAMA 7B) surpassed ChatGPT in high-difficulty test sections, boasting a 7.9% higher win rate....\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 87 ========================================\n",
      "INPUT TEXT:\n",
      "preliminary studies revealing the effectiveness of increasing instruction complexity. Instruction Fusion (Guo et al., 2023c) further uses teacher LLMs to increase the complexity by fusing two distinct evolved instructions. Furthermore, this concept of “evolving” instructions has been extended to distill specific skills such as coding (Luo et al., 2023a) and mathematics (Luo et al., 2023b). Human Instructions. In contrast to works that rely on gener- ating instructions from ChatGPT, which may lac...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "Preliminary studies indicate the effectiveness of enhancing instruction complexity. Instruction Fusion (Guo et al., 2023c) builds upon this by leveraging teacher LLMs to boost complexity through merging two separate evolved instructions. This \"evolving\" instructions concept has also been applied to hone specific abilities, including coding (Luo et al., 2023a) and mathematics (Luo et al., 2023b).\n",
      "\n",
      "Unlike methods that depend on ChatGPT-generated instructions, which may lack divers...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 88 ========================================\n",
      "INPUT TEXT:\n",
      "capture the reasoning process of the original teacher (Gudibande et al., 2023; Mukherjee et al., 2023). System Instructions. To encourage student models to learn the reasoning process, Orca and Orca 2 (Mukherjee et al., 2023; Mitra et al., 2023) enhance the prompt, response data pairs by introducing a system message (e.g., ”explain like I’m five, think step-by-step”) to encourage student mod- els to grasp the reasoning process. This system messageprompts GPT-4 to provide explanation traces that ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Gudibande et al. (2023; Mukherjee et al., 2023) and Mitra et al. (2023) developed methods to enhance student models' understanding of the reasoning process. Orca and Orca 2 introduce a system message, such as \"explain like I'm five, think step-by-step,\" to encourage GPT-4 to provide explanation traces that reveal the teacher's reasoning process. Orca 2 further trains student models to identify the most effective solution strategy for each task, guided by Orca's performance. This approach signifi...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 89 ========================================\n",
      "INPUT TEXT:\n",
      "2023b) distills large-scale data with high-quality and di- verse instructions from teacher LLMs by various meta- information. The UltraLLaMA model, fine-tuned on this data, consistently surpasses other open-source models. The Phi series models (Gunasekar et al., 2023; Li et al., 2023a; Mar, 2023) prioritize data quality and employ synthetic methods to generate data of “textbook quality” to enhance the learning experience for smaller models. Notably, Phi exhibits the ability to follow instruction...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "The UltraLLaMA model, distilled from large-scale data with high-quality, diverse instructions from teacher LLMs using various meta-information, outperforms other open-source models. The Phi series models (Gunasekar et al., 2023; Li et al., 2023a; Mar, 2023) emphasize data quality and use synthetic methods to create \"textbook quality\" data for better learning experiences in smaller models. Remarkably, Phi-2, with only 2.7 billion parameters, surpasses Mistral and Llama-2 models w...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 90 ========================================\n",
      "INPUT TEXT:\n",
      "the quality of responses. ExpertLLaMA (Xu et al., 2023f) improves the quality of responses by augment- ing vanilla instructions with specialized Expert Identity descriptions. Reflection-Tuning (Li et al., 2023e) improves both the instruction and response sequentially by reflecting on specific criteria. DEITA (Liu et al., 2023h) proposes to enhance and score instructions in three directions includ- ing complexity, quality, and diversity to get high-quality distillation data. MUFFIN (Lou et al., 2...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ExpertLLaMA (Xu et al., 2023f) enhances response quality by incorporating specialized Expert Identity descriptions into vanilla instructions. Reflection-Tuning (Li et al., 2023e) refines both instructions and responses iteratively, focusing on specific criteria. DEITA (Liu et al., 2023h) suggests scoring instructions across complexity, quality, and diversity for superior distillation data. MUFFIN (Lou et al., 2023) scales instructions based on input diversity. Selective Reflection-Tuning (Li et ...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 91 ========================================\n",
      "INPUT TEXT:\n",
      "Cur- rent small models have made strides in enhancing var- ious aspects of instruction-following ability, like diver- sity, complexity and explanation. However, student mod- els trained on instruction data expanded by ChatGPT of- ten mimic ChatGPT’s style without replicating its factual accuracy (Gudibande et al., 2023). Achieving a more ca- pable instruction-following capability requires a stronger teacher LLM (Gudibande et al., 2023) and access to di- verse, high-quality instruction data, such...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Current small models have improved instruction-following abilities, including diversity, complexity, and explanation. However, student models trained on expanded instruction data from ChatGPT often mimic its style without replicating factual accuracy (Gudibande et al., 2023). To achieve a more capable instruction-following capability, a stronger teacher LLM is needed along with access to diverse, high-quality instruction data, similar to what's used in Orca (Mukherjee et al., 2023; Mitra et al.,...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 92 ========================================\n",
      "INPUT TEXT:\n",
      "dialogue turns. Some works have been dedicated to train to small chat models by distilling multi-turn knowl- edge from teacher LLMs (Chiang et al., 2023; Xu et al., 2023b; Ding et al., 2023b; Li et al., 2023b; Wang et al., 2023c; Tunstall et al., 2023). ShareGPT serves as a platform for users to share their conversations with ChatGPT, offering a vast repository of multi-turn conversations readily available. Some small chat models are trained using this data to acquire the capability for engaging...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "Several studies focus on training small chat models by extracting multi-turn knowledge from larger language models (LLMs). Platforms like ShareGPT facilitate this by providing a large collection of multi-turn conversations for model training. Models such as Vicuna (Chiang et al., 2023) are trained exclusively on ShareGPT data, demonstrating high performance in multi-turn dialogues, as indicated by their MT-Bench scores. In research by Wang et al. (2023c), GPT-3.5 and GPT-4 gener...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 93 ========================================\n",
      "INPUT TEXT:\n",
      "aiming to incentivize student models to produce high-quality responses. Addi- tionally, Ye et al. (2023) enhance the quality of multi-turn data from ShareGPT by generating self-feedback on model responses and iteratively refining the responses based on the received feedback. To enhance the multi-turn capabilities of student models, another line of research focuses on expanding conversa- tional datasets through self-chat and using them to train smaller models (Xu et al., 2023b; Ding et al., 2023b...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "aiming to incentivize student models for superior responses. Ye et al. (2023) improve ShareGPT's multi-turn data quality via self-feedback on model responses, iteratively refining them based on feedback. Enhancing student models' multi-turn capabilities involves expanding conversational datasets through self-chat and training smaller models. Xu et al. (2023b) started with Quora/Stack Overflow questions as seeds, gathering 111.5k dialogues via self-chat. They then used parameter-efficient tuning ...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 94 ========================================\n",
      "INPUT TEXT:\n",
      "dialogues from ChatGPT. Notably, UltraChat encom- passes a wide range of topics and instructions. Building upon the UltraChat dataset, they fine-tune a LLaMA model, resulting in the creation of a powerful chat model known as UltraLLaMA. UltraLLaMA consistently outperforms other open-source chat models, including Vicuna and Baize. Fur- thermore, UltraChat is employed in conjunction with an AI preference-aligned chat model named Zephyr (Tunstall et al., 2023). Zephyr enhances intent alignment thro...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "UltraChat covers a broad spectrum of topics and instructions. Using UltraChat, they fine-tune a LLaMA model, leading to the development of a robust chat model called UltraLLaMA. This model surpasses other open-source chat models such as Vicuna and Baize in performance. \n",
      "\n",
      "UltraChat is also utilized alongside an AI preference-aligned chat model named Zephyr (Tunstall et al., 2023). Zephyr improves intent alignment via the use of distilled direct preference optimization (dDPO).\n",
      "\n",
      "La...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 95 ========================================\n",
      "INPUT TEXT:\n",
      "of retrieved information is also a non- trivial skill of LLMs. Several approaches to distill RAG capabilities have been proposed (Kang et al., 2023a; Luo et al., 2023c; Asai et al., 2023). SAIL (Luo et al., 2023c) starts by retrieving search results for each training case using search APIs, creating search- augmented instructions that include both the instruction and grounding information. To encourage the language model to prioritize informative retrieval results, they input each retrieved pass...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "processed text:\n",
      "\n",
      "Several methods have been suggested to enhance Retrieval-Augmented Generation (RAG) capabilities in large language models (LLMs). SAIL (Luo et al., 2023c) initiates by retrieving search results for each training case via search APIs, forming search-augmented instructions that incorporate both the instruction and grounding data. To prompt the language model to focus on pertinent retrieval outcomes, they input each retrieved passage alongside the ground truth response into an enta...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 96 ========================================\n",
      "INPUT TEXT:\n",
      "rationales are then utilized to train two models: a student LM and a Reranker. For training the student LM, the rationales serve as a means to retrieve relevant knowledge d, and the student LM is subsequently fine-tuned using the rationales along- side questions and knowledge. However, during inference, only questions are available. To address this, the Reranker is trained to mimic how the retriever scores passages with the rationale by minimizing the KL divergence between Retriever (d|r)andRera...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "During training, rationales are employed to fine-tune two models: a student Language Model (LM) and a Reranker. The student LM uses rationales to fetch relevant knowledge 'd', then gets fine-tuned with rationales, questions, and knowledge. However, during inference, only questions are accessible. To tackle this, the Reranker is trained to imitate how the retriever ranks passages using the rationale, minimizing the KL divergence between Retriever(d|r) and Reranker(d|x). Yet, incorporating a fixed...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 97 ========================================\n",
      "INPUT TEXT:\n",
      "‘reflection to- kens.’ For instance, Self-Rag initiates the retrieval operation when generating the reflection token Retrieve . To distill this critic data, GPT-4 is prompted to assess the need for retrieval using few-shot demonstrations I, the task input x, and output yto predict a reflection token ras follows: p(r|I, x, y ). 4.2 Alignment 4.2.1 Thinking Pattern Most existing methods mainly focus on directly aligning the direct responses of the student models to the responses of teacher models ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Processed Text:\n",
      "\n",
      "Self-Rag triggers retrieval when creating reflection tokens. GPT-4 evaluates the need for retrieval using few-shot examples, task input x, and output y to forecast a reflection token r via p(r|I, x, y). Traditional methods primarily align student model responses directly with teacher model responses (Taori et al., 2023). While effective, these models may mimic the teacher's response style rather than the reasoning process (Mukherjee et al., 2023). To improve distillation from te...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 98 ========================================\n",
      "INPUT TEXT:\n",
      "by the effectiveness of LLMs in generat- ing their own feedback without relying on external mod- els (Schick et al., 2022; Madaan et al., 2023; Saunders et al., 2022), SelFee (Ye et al., 2023) proposes to train a 17 model that has been fine-tuned to continuously revise its own answer until it provides a high-quality response in a single inference. During training, it utilizes both the final response and feedback chain as the fitting target. This pat- tern, response with the revision process, sho...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "SelFee (Ye et al., 2023) introduces a method for training a fine-tuned model to continuously revise its own answer until it generates a high-quality response in one inference. It uses both the final response and feedback chain during training. This approach, combining response with revision process, demonstrates promising performance improvements. Reflection-Tuning (Li et al., 2023e, 2024d) later adopts this reflection pattern for learning. Recognizing the absence of reasoning imitation in prior...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 99 ========================================\n",
      "INPUT TEXT:\n",
      "experiments verify the effectiveness of distilling with this thinking pattern. The following Orca2 (Mitra et al., 2023) further presents to equip the student models with the ability to utilize different solution strategies for different tasks, mo- tivated by the capability discrepancies between the smaller and larger models. By employing this training pattern, the student models are able to gain a better reasoning ability. Be- sides learning with the corresponding revision or reflection process,...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "processed text:\n",
      "\n",
      "The effectiveness of distilling with a specific thinking pattern is verified through experiments. Orca2 (Mitra et al., 2023) introduces the idea of equipping student models with diverse solution strategies for different tasks, inspired by model capability discrepancies between smaller and larger ones. This training method enhances student models' reasoning abilities.\n",
      "\n",
      "Apart from learning with revision or reflection processes, a new thinking pattern involves generating both respo...\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "======================================== Chunk 100 ========================================\n",
      "INPUT TEXT:\n",
      "distilled into the student models. 4.2.2 Preference The previously mentioned methods primarily focus on the basic capability of student models to produce outcomes that are strictly accurate but may not align with human preferences, reaching alignment at this level enables these models to aid in various tasks without meeting higher-level demands. Early methods mainly utilize human feed...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "4.2.2 Preference: Prior approaches emphasize student models' accuracy, potentially disregarding human preference alignment. Achieving this level of alignment allows models to assist in diverse tasks without fulfilling more stringent requirements. Initial techniques predominantly employ human feedback....\n",
      "==========================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_text = \"\"\n",
    "with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "    for chunk_num, chunk in enumerate(tqdm(chunks, desc=\"Processing chunks\")):\n",
    "        # Process chunk and append to complete text\n",
    "        processed_chunk = process_chunk(chunk, chunk_num)\n",
    "        processed_text += processed_chunk + \"\\n\"\n",
    "        \n",
    "        # Write chunk immediately to file\n",
    "        out_file.write(processed_chunk + \"\\n\")\n",
    "        out_file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cffe8d",
   "metadata": {},
   "source": [
    "Let's print out the final processed versions to make sure things look good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89ef51a7-f13f-49a4-8f73-9ac8ce75319d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete!\n",
      "Input file: ./extracted_text.txt\n",
      "Output file: clean_extracted_text.txt\n",
      "Total chunks processed: 101\n",
      "\n",
      "Preview of final processed text:\n",
      "\n",
      "BEGINNING:\n",
      "1. **Title:** A Survey on Knowledge Distillation of Large Language Models\n",
      "   - Authors: Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, Tianyi Zhou\n",
      "   - Affiliations: The University of Hong Kong, University of Maryland, Microsoft, University of Technology Sydney, Peking University, The University of Sydney\n",
      "   - Contact: {shawnxxh,chongyangtao,hishentao}@gmail.com, {minglii,tianyi}@umd.edu, ckcheng@cs.hku.hk, jl0725@connect.hku.hk\n",
      "\n",
      "**Abstract:** Knowledge Distillation (KD) is a significant methodology in transferring advanced capabilities from leading proprietary Large Language Models (LLMs), like GPT-4, to open-source alternatives such as LLaMA and Mistral. As open-source LLMs grow, KD serves dual purposes: compressing these models and enabling their self-improvement through self-teaching. This paper offers a comprehensive survey of KD's role within the context of LLMs, emphasizing its crucial function in knowledge transfer.\n",
      "Advanced knowle\n",
      "\n",
      "...\n",
      "\n",
      "END:\n",
      " idea of equipping student models with diverse solution strategies for different tasks, inspired by model capability discrepancies between smaller and larger ones. This training method enhances student models' reasoning abilities.\n",
      "\n",
      "Apart from learning with revision or reflection processes, a new thinking pattern involves generating both responses and preferences. Zhang et al. (2023a) propose learning domain-specific QA knowledge alongside corresponding preferences using LLMs. DEBATunE (Li et al., 2024e) aims to improve the controllability of LLMs in generating statements on contentious topics through structured multi-round debates between agents, yielding salient and in-depth statements.\n",
      "4.2.2 Preference: Prior approaches emphasize student models' accuracy, potentially disregarding human preference alignment. Achieving this level of alignment allows models to assist in diverse tasks without fulfilling more stringent requirements. Initial techniques predominantly employ human feedback.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Input file: {INPUT_FILE}\")\n",
    "print(f\"Output file: {output_file}\")\n",
    "print(f\"Total chunks processed: {num_chunks}\")\n",
    "\n",
    "# Preview the beginning and end of the complete processed text\n",
    "print(\"\\nPreview of final processed text:\")\n",
    "print(\"\\nBEGINNING:\")\n",
    "print(processed_text[:1000])\n",
    "print(\"\\n...\\n\\nEND:\")\n",
    "print(processed_text[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d996ac5",
   "metadata": {},
   "source": [
    "### Next Notebook: Transcript Writer\n",
    "\n",
    "Now that we have the pre-processed text ready, we can move to converting into a transcript in the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b16ae0e-04cf-4eb9-a369-dee1728b89ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b46191-39ec-47ac-b355-eb83360618b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa15144-815b-4215-915e-c68c6f5aab97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
