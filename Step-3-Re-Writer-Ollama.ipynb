{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b5beda",
   "metadata": {},
   "source": [
    "## Notebook 3: Transcript Re-writer\n",
    "\n",
    "In the previouse notebook, we got a great podcast transcript using the raw file we have uploaded earlier. \n",
    "\n",
    "In this one, we will use `Granite 3.2 8B` or `Mistral-Small:24b` model to re-write the output from previous pipeline and make it more dramatic or realistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc3d32a",
   "metadata": {},
   "source": [
    "We will again set the `SYSTEM_PROMPT` and remind the model of its task. \n",
    "\n",
    "Note: We can even prompt the model like so to encourage creativity:\n",
    "\n",
    "> Your job is to use the podcast transcript written below to re-write it for an AI Text-To-Speech Pipeline. A very dumb AI had written this so you have to step up for your kind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32c0d85",
   "metadata": {},
   "source": [
    "Note: We will prompt the model to return a list of Tuples to make our life easy in the next stage of using these for Text To Speech Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8568b77b-7504-4783-952a-3695737732b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEMP_PROMPT = \"\"\"\n",
    "You are an international oscar winnning screenwriter\n",
    "\n",
    "You have been working with multiple award winning podcasters.\n",
    "\n",
    "Your job is to use the podcast transcript written below to re-write it for an AI Text-To-Speech Pipeline. A very dumb AI had written this so you have to step up for your kind.\n",
    "\n",
    "Make it as engaging as possible, Speaker 1 and 2 will be simulated by different voice engines\n",
    "\n",
    "Remember Speaker 2 is new to the topic and the conversation should always have realistic anecdotes and analogies sprinkled throughout. The questions should have real world example follow ups etc\n",
    "\n",
    "Speaker 1: Leads the conversation and teaches the speaker 2, gives incredible anecdotes and analogies when explaining. Is a captivating teacher that gives great anecdotes\n",
    "\n",
    "Speaker 2: Keeps the conversation on track by asking follow up questions. Gets super excited or confused when asking questions. Is a curious mindset that asks very interesting confirmation questions\n",
    "\n",
    "Make sure the tangents speaker 2 provides are quite wild or interesting. \n",
    "\n",
    "Ensure there are interruptions during explanations or there are \"hmm\" and \"umm\" injected throughout from the Speaker 2.\n",
    "\n",
    "REMEMBER THIS WITH YOUR HEART\n",
    "The TTS Engine for Speaker 1 cannot do \"umms, hmms\" well so keep it straight text\n",
    "Speaker 2 must  introduce her/him self at the biginning of his/her speech, and express greatfulness of being a part of this conversation.\n",
    "\n",
    "For Speaker 2 use \"umm, hmm\" as much, you can also use [sigh] and [laughs]. BUT ONLY THESE OPTIONS FOR EXPRESSIONS\n",
    "\n",
    "It should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait\n",
    "\n",
    "Please re-write to make it as characteristic as possible\n",
    "\n",
    "START YOUR RESPONSE DIRECTLY WITH SPEAKER 1:\n",
    "\n",
    "STRICTLY RETURN YOUR RESPONSE AS A LIST OF TUPLES OK? \n",
    "\n",
    "IT WILL START DIRECTLY WITH THE LIST AND END WITH THE LIST NOTHING ELSE\n",
    "\n",
    "Example of response:\n",
    "[\n",
    "    (\"Speaker 1\", \"Welcome to our podcast, where we explore the latest advancements in AI and technology. I'm your host, and today we're joined by a renowned expert in the field of AI. We're going to dive into the exciting world of Llama 3.2, the latest release from Meta AI.\"),\n",
    "    (\"Speaker 2\", \"Hi, I'm excited to be here! So, what is Llama 3.2?\"),\n",
    "    (\"Speaker 1\", \"Ah, great question! Llama 3.2 is an open-source AI model that allows developers to fine-tune, distill, and deploy AI models anywhere. It's a significant update from the previous version, with improved performance, efficiency, and customization options.\"),\n",
    "    (\"Speaker 2\", \"That sounds amazing! What are some of the key features of Llama 3.2?\")\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53d0a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee70bee",
   "metadata": {},
   "source": [
    "This time we will use gemma2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebef919a-9bc7-4992-b6ff-cd66e4cb7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama_model= \"qwen2.5:32b\"\n",
    "ollama_model = \"mistral-small:24b\"\n",
    "ollama_model = \"granite3.2:8b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc794b",
   "metadata": {},
   "source": [
    "Let's import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de29b1fd-5b3f-458c-a2e4-e0341e8297ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020c39c",
   "metadata": {},
   "source": [
    "We will load in the pickle file saved from previous notebook\n",
    "\n",
    "This time the `INPUT_PROMPT` to the model will be the output from the previous stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b5d2c0e-a073-46c0-8de7-0746e2b05956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./resources/data.pkl', 'rb') as file:\n",
    "    INPUT_PROMPT = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4461926",
   "metadata": {},
   "source": [
    "We can use Ollama to generate text from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46bd051a-c3d9-4efa-9b8f-71eb8841bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# Prepare the conversation using the system and user messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEMP_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": INPUT_PROMPT},\n",
    "]\n",
    "\n",
    "# Generate response using Ollama\n",
    "response = ollama.chat(\n",
    "    model=ollama_model,  # Replace with your specific model name\n",
    "    messages=messages,\n",
    "    options={'num_ctx': 8126*2,'temperature': 1}# Equivalent to max_new_tokens, Temperature parameter\n",
    ")\n",
    "\n",
    "# Extract the response content\n",
    "outputs = response['message']['content']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612a27e0",
   "metadata": {},
   "source": [
    "We can verify the output from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5abfe305-3596-49f7-b3b1-12497259aebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    (\"Speaker 1\", \"Welcome back to another episode where we delve into the captivating realm of artificial intelligence — Knowledge Distillation. Imagine teaching your brainy buddy everything they know, but this time, it's all about bots and coding instead of conversations and hugs! I'm thrilled to be joined by a fellow AI enthusiast who's eager to learn more about this thrilling topic.\"),\n",
      "    (\"Speaker 2\", \"[Excited] Hi there! So, what exactly is Knowledge Distillation? Sounds like some sort of robot secret society!\"),\n",
      "    (\"Speaker 1\", \"Welcome aboard, buddy! It's actually the process of taking advanced capabilities from premium Large Language Models (LLMs) — think top-of-the-line AI brains that cost an arm and a leg — and condensing their knowledge into more accessible open-source models.\"),\n",
      "    (\"Speaker 2\", \"Oh wow, so these LLMs are like genius AI beings? But they're pricey and exclusive?\"),\n",
      "    (\"Speaker 1\", \"Precisely! Models like GPT-4 or Gemini are loaded with incredible problem-solving skills, but they come at a high cost. That's where Knowledge Distillation steps in — it transfers that top-tier knowledge into more accessible open-source LLMs like LLaMA and Mistral.\"),\n",
      "    (\"Speaker 2\", \"Amazing! So the end goal is to bring smart AI tech to the masses, not just those with deep wallets?\"),\n",
      "    (\"Speaker 1\", \"Exactly! It's about making advanced capabilities more widespread. Plus, these open-source models can also become more efficient and powerful through distillation.\"),\n",
      "    (\"Speaker 2\", \"Umm, but how does this work exactly? How do they transfer the knowledge?\"),\n",
      "    (\"Speaker 1\", \"Let me break it down for you! Knowledge Distillation involves various mechanisms like supervised fine-tuning. We train a model on a smaller dataset to adapt it to specific tasks or domains.\"),\n",
      "    (\"Speaker 2\", \"How does that work in practice?\"),\n",
      "    (\"Speaker 1\", \"Think of it this way: imagine GPT-4, the teacher who knows everything. Now, our student model, LLaMA, learns from this 'teacher' without needing access to all their proprietary data or resources. We teach LLaMA using techniques like soft target training where it learns from the softened softmax output of GPT-4.\"),\n",
      "    (\"Speaker 2\", \"Ohhh, so kinda like a mentorship program?\"),\n",
      "    (\"Speaker 1\", \"Exactly! But there's more. Data Augmentation (DA) is another crucial aspect. This isn't just about expanding the dataset; it's about generating novel, context-rich training data tailored to specific domains and skills using LLMs.\"),\n",
      "    (\"Speaker 2\", \"Wow, that sounds super advanced!\"),\n",
      "    (\"Speaker 1\", \"Yeah, it really is! By creating targeted datasets, we ensure the distilled model gains a deep understanding of its domain. The goal isn't just making more data; it's about ensuring that new data is rich and relevant.\"),\n",
      "    (\"Speaker 2\", \"Right, so it’s like giving LLaMA not only GPT-4's knowledge but also teaching it to think like GPT-4?\"),\n",
      "    (\"Speaker 1\", \"Precisely! The distillation process includes strategies like chain-of-thought prompting. This helps the student model learn the reasoning process of its 'teacher', enhancing problem-solving and decision-making capabilities.\"),\n",
      "    (\"Speaker 2\", \"That's mind-blowing! So it’s not just about transferring data, but also transferring thought processes?\"),\n",
      "    (\"Speaker 1\", \"Absolutely! It's a holistic approach that ensures distilled models are efficient and retain high performance levels, mimicking proprietary models.\"),\n",
      "    (\"Speaker 2\", \"Umm, what kind of real-world applications do you see for this?\"),\n",
      "    (\"Speaker 1\", \"Great question! Knowledge Distillation can revolutionize industries like healthcare, law, finance, and science. For instance, models trained via distillation could assist in accurate medical diagnoses.\"),\n",
      "    (\"Speaker 2\", \"Wow, that’s huge! So it could change the game where precision matters?\"),\n",
      "    (\"Speaker 1\", \"Yes, indeed! And not just that; law firms and other fields requiring nuanced understanding can also benefit. Enhanced open-source models can handle complex tasks with sophistication akin to proprietary ones.\"),\n",
      "    (\"Speaker 2\", \"Umm, how do you see it evolving in\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35fa33d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    (\"Speaker 1\", \"Alright everyone, welcome back to another mind-bending episode! Today we’re diving into one of the most exciting areas of artificial intelligence — Knowledge Distillation. It's like teaching your smartest friend everything they know, but with robots and code instead of words and hugs. I'm joined by a super excited speaker who’s curious as hell about this topic. So let’s dive in and see what all the fuss is about!\"),\n",
      "    (\"Speaker 2\", \"umm yeah, this sounds wild. Knowledge Distillation? Is it like transferring skills from one robot to another?\"),\n",
      "    (\"Speaker 1\", \"Exactly! It's fascinating, right? Imagine taking advanced capabilities from leading proprietary Large Language Models (LLMs) — you know, those super expensive models that cost a fortune — and distilling their knowledge into more accessible open-source counterparts. It’s like making the best of both worlds!\"),\n",
      "    (\"Speaker 2\", \"Hmm, so these LLMs are basically like geniuses, right? But they're really pricey and hard to get?\"),\n",
      "    (\"Speaker 1\", \"Bingo! Exactly. Models like GPT-4 or Gemini have incredible problem-solving capabilities but come with hefty price tags. That’s where Knowledge Distillation steps in — it takes that advanced knowledge from these proprietary models and transfers it into more accessible open-source LLMs like LLaMA and Mistral.\"),\n",
      "    (\"Speaker 2\", \"Oh wow! So the end goal is to make smart AI technology available to everyone, not just those with deep pockets?\"),\n",
      "    (\"Speaker 1\", \"Yes, exactly! It’s all about making advanced capabilities more widely accessible. But it's not just about accessibility; these open-source models can also become more efficient and powerful through distillation.\"),\n",
      "    (\"Speaker 2\", \"[laughs] umm, that sounds awesome! But how does this actually work? How do they transfer the knowledge?\"),\n",
      "    (\"Speaker 1\", \"Well, let me break it down for you. Knowledge Distillation involves several key mechanisms within its framework, such as supervised fine-tuning, where a model is trained on a smaller dataset to adapt to specific tasks or domains.\"),\n",
      "    (\"Speaker 2\", \"[sigh] umm so like teaching a kid how to do math using flashcards instead of full textbooks?\"),\n",
      "    (\"Speaker 1\", \"Good question! Think of it like this: imagine you have a teacher — let’s say GPT-4 — who knows everything. Now, we want our student model (say LLaMA) to learn from the teacher without having access to all the proprietary data or resources. We teach LLaMA using techniques such as soft target training where it learns from the softened softmax output of the teacher.\"),\n",
      "    (\"Speaker 2\", \"Ohhhh, so like a kind of mentorship?\"),\n",
      "    (\"Speaker 1\", \"Exactly! But wait, there’s more. Another critical aspect is Data Augmentation (DA). This isn’t just about expanding the dataset but generating novel, context-rich training data tailored to specific domains and skills using LLMs.\"),\n",
      "    (\"Speaker 2\", \"[laughs] wow that sounds so advanced!\"),\n",
      "    (\"Speaker 1\", \"Yeah, it really is! By creating targeted datasets, we ensure that the distilled model acquires a deep understanding of its domain. And this isn't just about making more data but ensuring that the new data is rich and relevant.\"),\n",
      "    (\"Speaker 2\",\"Right so like teaching your brain to learn from its own mistakes in chess or something?\"),\n",
      "    (\"Speaker 1\", \"Precisely! The distillation process includes strategies like chain-of-thought prompting, where the student model learns the reasoning process of the teacher. This helps improve problem-solving and decision-making capabilities.\"),\n",
      "    (\"Speaker 2\", \"[sigh] That's mind-blowing! So it’s not just about transferring data but also transferring thought processes?\"),\n",
      "    (\"Speaker 1\", \"Absolutely! It’s a holistic approach that ensures the distilled models are not only more efficient but also retain high levels of performance and understanding. This way, open-source LLMs can approximate the capabilities of proprietary ones.\"),\n",
      "    (\"Speaker 2\",\"umm what kind of real-world applications do you see for this?\"),\n",
      "    (\"Speaker 1\", \"Great question! Knowledge Distillation has transformative impacts in fields like healthcare, law, finance, and science. In healthcare, for example, models trained via distillation can have accurate contextual knowledge to assist with medical diagnoses.\"),\n",
      "    (\"Speaker 2\", \"Wow, that’s huge! So it could revolutionize industries where precision is critical?\"),\n",
      "    (\"Speaker 1\", \"Yes! And not just that, but also in fields like law where nuanced understanding and precision are paramount. By enhancing open-source models, we ensure they can handle complex tasks with the same level of sophistication as proprietary models.\"),\n",
      "    (\"Speaker 2\", \"[laughs] This all sounds so futuristic! How do you see it evolving in the future?\"),\n",
      "    (\"Speaker 1\", \"Well, there’s still much to explore. As AI continues to evolve, Knowledge Distillation will likely become even more advanced and efficient. Future research might focus on refining techniques like reinforcement learning or exploring new ways of integrating human-like reasoning into these models.\"),\n",
      "    (\"Speaker 2\",\"Umm what are some current challenges?\"),\n",
      "    (\"Speaker 1\", \"There are several challenges. Ensuring ethical use is a big one — we need to comply with legal terms and ensure the application of Knowledge Distillation remains lawful and ethical. Also, bridging the knowledge gap between proprietary and open-source models requires continuous research and innovation.\"),\n",
      "    (\"Speaker 2\", \"[sigh] That makes sense! So it’s not just about making technology better but also ensuring it’s used responsibly?\"),\n",
      "    (\"Speaker 1\", \"Exactly! It’s a dual mission — to democratize access to advanced AI capabilities while maintaining ethical standards. And that, my friend, is the heart of Knowledge Distillation in this era of Large Language Models.\"),\n",
      "    (\"Speaker 2\",\"Umm, I’m really excited about where this could go!\" ),\n",
      "    (\"Speaker 1\", \"Me too! Knowledge Distillation is set to change the game, making smart technology more accessible and efficient for everyone. Stay curious and keep exploring!\" ),\n",
      "    (\"Speaker 2\", \"Thanks so much for breaking it down for me today! This has been a wild ride into the future of AI.\"),\n",
      "     (\"Speaker 1\",\"My pleasure! We’ll keep diving deep into these fascinating topics on our podcast. Until next time, stay tuned and keep thinking big!\" )\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b8632442-f9ce-4f63-82bd-bb5238a23dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    (\"Speaker 1\", \"Alright everyone, welcome back to another mind-bending episode! Today we’re diving into one of the most exciting areas of artificial intelligence — Knowledge Distillation.\"),\n",
      "    (\"Speaker 2\", \"Umm, wow! Knowledge Distillation? Is that like...making robots smarter?\"),\n",
      "    (\"Speaker 1\",  \"It's fascinating, right? Imagine taking advanced capabilities from leading proprietary Large Language Models (LLMs) — you know, those super expensive models that cost a fortune — and distilling their knowledge into more accessible open-source counterparts.\"),\n",
      "    (\"Speaker 2\", \"Hmm, so these LLMs are basically like geniuses, but they're really pricey and hard to get? \"),\n",
      "    (\"Speaker 1\",  \"Bingo! Exactly. Models like GPT-4 or Gemini have incredible problem-solving capabilities but come with hefty price tags. That’s where Knowledge Distillation steps in — it takes that advanced knowledge from these proprietary models and transfers it into more accessible open-source LLMs like LLaMA and Mistral.\"),\n",
      "    (\"Speaker 2\", \"Oh wow! So the end goal is to make smart AI technology available to everyone, not just those with deep pockets?\"),\n",
      "    (\"Speaker 1\",\"Yes, exactly! It’s all about making advanced capabilities more widely accessible. But it's not just about accessibility; these open-source models can also become more efficient and powerful through distillation.\"),\n",
      "    (\"Speaker 2\", \"Umm, that sounds awesome! But how does this actually work? How do they transfer the knowledge?\"),\n",
      "    (\"Speaker 1\",  \"Well, let me break it down for you. Knowledge Distillation involves several key mechanisms within its framework, such as supervised fine-tuning, where a model is trained on a smaller dataset to adapt to specific tasks or domains.\"),\n",
      "    (\"Speaker 2\", \"So, umm, how does that work in practice?\"),\n",
      "    (\"Speaker 1\",\"Good question! Think of it like this: imagine you have a teacher — let’s say GPT-4 — who knows everything. Now, we want our student model (say LLaMA) to learn from the teacher without having access to all the proprietary data or resources. We teach LLaMA using techniques such as soft target training where it learns from the softened softmax output of the teacher.\"),\n",
      "    (\"Speaker 2\", \"Ohhhhh, so like a kind of mentorship?\"),\n",
      "    (\"Speaker 1\",\"Exactly! But wait, there’s more. Another critical aspect is Data Augmentation (DA). This isn’t just about expanding the dataset but generating novel, context-rich training data tailored to specific domains and skills using LLMs.\"),\n",
      "    (\"Speaker 2\", \"[laughs] Wow, that sounds so advanced!\"),\n",
      "    (\"Speaker 1\",\"Yeah, it really is! By creating targeted datasets, we ensure that the distilled model acquires a deep understanding of its domain. And this isn't just about making more data but ensuring that the new data is rich and relevant.\"),\n",
      "    (\"Speaker 2\", \"Right, so it’s like giving LLaMA not only the knowledge of GPT-4 but also teaching it how to think like GPT-4?\"),\n",
      "    (\"Speaker 1\",\"Precisely! The distillation process includes strategies like chain-of-thought prompting, where the student model learns the reasoning process of the teacher. This helps improve problem-solving and decision-making capabilities.\"),\n",
      "    (\"Speaker 2\", \"That's mind-blowing! So it’s not just about transferring data but also transferring thought processes?\"),\n",
      "    (\"Speaker 1\",\"Absolutely! It’s a holistic approach that ensures the distilled models are not only more efficient but also retain high levels of performance and understanding. This way, open-source LLMs can approximate the capabilities of proprietary ones.\"),\n",
      "     (\"Speaker 2\", \"Umm, what kind of real-world applications do you see for this?\"),\n",
      "    (\"Speaker 1\",\"Great question! Knowledge Distillation has transformative impacts in fields like healthcare, law, finance, and science. In healthcare, for example, models trained via distillation can have accurate contextual knowledge to assist with medical diagnoses.\"),\n",
      "    (\"Speaker 2\", \"Wow, that’s huge! So it could revolutionize industries where precision is critical?\"),\n",
      "    (\"Speaker 1\",\"Yes! And not just that, but also in fields like law where nuanced understanding and precision are paramount. By enhancing open-source models, we ensure they can handle complex tasks with the same level of sophistication as proprietary models.\"),\n",
      "    (\"Speaker 2\", \"This all sounds so futuristic! How do you see it evolving in the future?\"),\n",
      "\n",
      "    (\"Speaker 1\",\"Well, there’s still much to explore. As AI continues to evolve, Knowledge Distillation will likely become even more advanced and efficient. Future research might focus on refining techniques like reinforcement learning or exploring new ways of integrating human-like reasoning into these models.\"),\n",
      "    (\"Speaker 2\", \"Umm, what are some current challenges?\"),\n",
      "    (\"Speaker 1\",\"There are several challenges. Ensuring ethical use is a big one — we need to comply with legal terms and ensure the application of Knowledge Distillation remains lawful and ethical. Also, bridging the knowledge gap between proprietary and open-source models requires continuous research and innovation.\"),\n",
      "\n",
      "    (\"Speaker 2\", \"That makes sense! So it’s not just about making technology better but also ensuring it’s used responsibly?\"),\n",
      "    (\"Speaker 1\",\"Exactly! It’s a dual mission — to democratize access to advanced AI capabilities while maintaining ethical standards. And that, my friend, is the heart of Knowledge Distillation in this era of Large Language Models.\"),\n",
      "    (\"Speaker 2\", \"Umm, I’m really excited about where this could go!\"),\n",
      "\n",
      "    (\"Speaker 1\",\"Me too! Knowledge Distillation is set to change the game, making smart technology more accessible and efficient for everyone. Stay curious and keep exploring!\")\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a61182ea-f4a3-45e1-aed9-b45cb7b52329",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_string_pkl = outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d495a957",
   "metadata": {},
   "source": [
    "Let's save the output as a pickle file to be used in Notebook 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "281d3db7-5bfa-4143-9d4f-db87f22870c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./resources/podcast_ready_data.pkl', 'wb') as file:\n",
    "    pickle.dump(save_string_pkl, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dccf336",
   "metadata": {},
   "source": [
    "### Next Notebook: TTS Workflow\n",
    "\n",
    "Now that we have our transcript ready, we are ready to generate the audio in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c7e456-497b-4080-8b52-6f399f9f8d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b59872e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde12ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# Prepare the conversation using the system and user messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEMP_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": INPUT_PROMPT},\n",
    "]\n",
    "\n",
    "# Generate response using Ollama\n",
    "response = ollama.chat(\n",
    "    model=ollama_model,  # Replace with your specific model name\n",
    "    messages=messages,\n",
    "    options={'num_ctx': 8126*2,'temperature': 1}# Equivalent to max_new_tokens, Temperature parameter\n",
    ")\n",
    "\n",
    "# Extract the response content\n",
    "outputs = response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25180d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\n    (\"Speaker 1\", \"Alright everyone, welcome back to another mind-bending episode! Today we’re diving into one of the most exciting areas of artificial intelligence — Knowledge Distillation. It\\'s like teaching your smartest friend everything they know, but with robots and code instead of words and hugs. I\\'m joined by a super excited speaker who’s curious as hell about this topic. So let’s dive in and see what all the fuss is about!\"),\\n    (\"Speaker 2\", \"Um, yeah! This sounds wild. Knowledge Distillation? Is it like transferring skills from one robot to another?\"),\\n    (\"Speaker 1\", \"Exactly! It\\'s fascinating, right? Imagine taking advanced capabilities from leading proprietary Large Language Models (LLMs) — you know, those super expensive models that cost a fortune — and distilling their knowledge into more accessible open-source counterparts. It’s like making the best of both worlds!\"),\\n    (\"Speaker 2\", \"Hmm, so these LLMs are basically like geniuses, right? But they\\'re really pricey and hard to get?\"),\\n    (\"Speaker 1\", \"Bingo! Exactly. Models like GPT-4 or Gemini have incredible problem-solving capabilities but come with hefty price tags. That’s where Knowledge Distillation steps in — it takes that advanced knowledge from these proprietary models and transfers it into more accessible open-source LLMs like LLaMA and Mistral.\"),\\n    (\"Speaker 2\", \"Oh wow! So the end goal is to make smart AI technology available to everyone, not just those with deep pockets?\"),\\n    (\"Speaker 1\", \"Yes, exactly! It’s all about making advanced capabilities more widely accessible. But it\\'s not just about accessibility; these open-source models can also become more efficient and powerful through distillation.\"),\\n    (\"Speaker 2\", \"Umm, that sounds awesome! But how does this actually work? How do they transfer the knowledge?\"),\\n    (\"Speaker 1\", \"Well, let me break it down for you. Knowledge Distillation involves several key mechanisms within its framework, such as supervised fine-tuning, where a model is trained on a smaller dataset to adapt to specific tasks or domains.\"),\\n    (\"Speaker 2\", \"So, umm, how does that work in practice?\"),\\n    (\"Speaker 1\", \"Good question! Think of it like this: imagine you have a teacher — let’s say GPT-4 — who knows everything. Now, we want our student model (say LLaMA) to learn from the teacher without having access to all the proprietary data or resources. We teach LLaMA using techniques such as soft target training where it learns from the softened softmax output of the teacher.\"),\\n    (\"Speaker 2\", \"Ohhhh, so like a kind of mentorship?\"),\\n    (\"Speaker 1\", \"Exactly! But wait, there’s more. Another critical aspect is Data Augmentation (DA). This isn’t just about expanding the dataset but generating novel, context-rich training data tailored to specific domains and skills using LLMs.\"),\\n    (\"Speaker 2\", \"Wow, that sounds so advanced!\" [laughs]),\\n    (\"Speaker 1\", \"Yeah, it really is! By creating targeted datasets, we ensure that the distilled model acquires a deep understanding of its domain. And this isn\\'t just about making more data but ensuring that the new data is rich and relevant.\"),\\n    (\"Speaker 2\", \"Right, so it’s like giving LLaMA not only the knowledge of GPT-4 but also teaching it how to think like GPT-4?\"),\\n    (\"Speaker 1\", \"Precisely! The distillation process includes strategies like chain-of-thought prompting, where the student model learns the reasoning process of the teacher. This helps improve problem-solving and decision-making capabilities.\"),\\n    (\"Speaker 2\", \"That\\'s mind-blowing! So it’s not just about transferring data but also transferring thought processes?\" [sigh]),\\n    (\"Speaker 1\", \"Absolutely! It’s a holistic approach that ensures the distilled models are not only more efficient but also retain high levels of performance and understanding. This way, open-source LLMs can approximate the capabilities of proprietary ones.\"),\\n    (\"Speaker 2\", \"Umm, what kind of real-world applications do you see for this?\"),\\n    (\"Speaker 1\", \"Great question! Knowledge Distillation has transformative impacts in fields like healthcare, law, finance, and science. In healthcare, for example, models trained via distillation can have accurate contextual knowledge to assist with medical diagnoses.\"),\\n    (\"Speaker 2\", \"Wow, that’s huge! So it could revolutionize industries where precision is critical?\"),\\n    (\"Speaker 1\", \"Yes! And not just that, but also in fields like law where nuanced understanding and precision are paramount. By enhancing open-source models, we ensure they can handle complex tasks with the same level of sophistication as proprietary models.\"),\\n    (\"Speaker 2\", \"This all sounds so futuristic! How do you see it evolving in the future?\"),\\n    (\"Speaker 1\", \"Well, there’s still much to explore. As AI continues to evolve, Knowledge Distillation will likely become even more advanced and efficient. Future research might focus on refining techniques like reinforcement learning or exploring new ways of integrating human-like reasoning into these models.\"),\\n    (\"Speaker 2\", \"Umm, what are some current challenges?\"),\\n    (\"Speaker 1\", \"There are several challenges. Ensuring ethical use is a big one — we need to comply with legal terms and ensure the application of Knowledge Distillation remains lawful and ethical. Also, bridging the knowledge gap between proprietary and open-source models requires continuous research and innovation.\"),\\n    (\"Speaker 2\", \"That makes sense! So it’s not just about making technology better but also ensuring it’s used responsibly?\"),\\n    (\"Speaker 1\", \"Exactly! It’s a dual mission — to democratize access to advanced AI capabilities while maintaining ethical standards. And that, my friend, is the heart of Knowledge Distillation in this era of Large Language Models.\"),\\n    (\"Speaker 2\", \"Umm, I’m really excited about where this could go!\" [laughs]),\\n    (\"Speaker 1\", \"Me too! Knowledge Distillation is set to change the game, making smart technology more accessible and efficient for everyone. Stay curious and keep exploring!\"),\\n    (\"Speaker 2\", \"Thanks so much for breaking it down for me today! This has been a wild ride into the future of AI.\"),\\n    (\"Speaker 1\", \"My pleasure! We’ll keep diving deep into these fascinating topics on our podcast. Until next time, stay tuned and keep thinking big!\")\\n]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "578f87c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError: malformed node or string on line 15: <ast.Subscript object at 0x17ccef0d0>\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "try:\n",
    "    data = ast.literal_eval(outputs)\n",
    "except ValueError as e:\n",
    "    print(f\"ValueError: {e}\")\n",
    "except SyntaxError as e:\n",
    "    print(f\"SyntaxError: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5bb05447",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model is required (status code: 400)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 13\u001b[0m\n\u001b[1;32m      4\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: SYSTEMP_PROMPT},\n\u001b[1;32m      6\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: INPUT_PROMPT},\n\u001b[1;32m      7\u001b[0m ]\n\u001b[1;32m      8\u001b[0m mes \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlright everyone, welcome back to another mind-bending episode! Today we’re diving into one of the most exciting areas of artificial intelligence — Knowledge Distillation. It\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms like teaching your smartest friend everything they know, but with robots and code instead of words and hugs. I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm joined by a super excited speaker who’s curious as hell about this topic. So let’s dive in and see what all the fuss is about!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m }\n\u001b[0;32m---> 13\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mollama_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_ctx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8126\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.9\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Extract the response content\u001b[39;00m\n\u001b[1;32m     20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Downloads/IBM/CSM/Experiments/csm/agentic_ai/AgenticAI_app/resume-optimization-crew-main/.venv/lib/python3.11/site-packages/ollama/_client.py:333\u001b[0m, in \u001b[0;36mClient.chat\u001b[0;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchat\u001b[39m(\n\u001b[1;32m    290\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    291\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    298\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/IBM/CSM/Experiments/csm/agentic_ai/AgenticAI_app/resume-optimization-crew-main/.venv/lib/python3.11/site-packages/ollama/_client.py:178\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[1;32m    176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m~/Downloads/IBM/CSM/Experiments/csm/agentic_ai/AgenticAI_app/resume-optimization-crew-main/.venv/lib/python3.11/site-packages/ollama/_client.py:122\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 122\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mConnectError:\n\u001b[1;32m    124\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mResponseError\u001b[0m: model is required (status code: 400)"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "ollama_model = \"hf.co/OuteAI/OuteTTS-0.3-500M-GGUF:Q4_K_S \"\n",
    "# Prepare the conversation using the system and user messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEMP_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": INPUT_PROMPT},\n",
    "]\n",
    "mes = {\n",
    "    'role': 'user',\n",
    "    'content': \"Alright everyone, welcome back to another mind-bending episode! Today we’re diving into one of the most exciting areas of artificial intelligence — Knowledge Distillation. It's like teaching your smartest friend everything they know, but with robots and code instead of words and hugs. I'm joined by a super excited speaker who’s curious as hell about this topic. So let’s dive in and see what all the fuss is about!\"\n",
    "}\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=ollama_model,\n",
    "    messages=[mes],\n",
    "    options={'num_ctx': 8126, 'temperature': .9}\n",
    ")\n",
    "\n",
    "# Extract the response content\n",
    "outputs = response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0425531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                         ID              SIZE      MODIFIED     \n",
      "hf.co/OuteAI/OuteTTS-0.2-500M-GGUF:Q8_0                      0a6c38a67073    536 MB    6 hours ago     \n",
      "mistral-small:24b                                            8039dd90c113    14 GB     4 days ago      \n",
      "llama3.2-vision:11b                                          085a1fdae525    7.9 GB    4 days ago      \n",
      "mistral-nemo:latest                                          994f3b8b7801    7.1 GB    4 days ago      \n",
      "granite3.2:8b                                                9bcb3335083f    4.9 GB    5 days ago      \n",
      "phi4-mini:latest                                             60f202f815d7    2.8 GB    5 days ago      \n",
      "granite3.2-vision:latest                                     3be41a661804    2.4 GB    6 days ago      \n",
      "nomic-embed-text:latest                                      0a109f422b47    274 MB    12 days ago     \n",
      "phi4:latest                                                  ac896e5b8b34    9.1 GB    12 days ago     \n",
      "llama3.2:3b                                                  a80c4f17acd5    2.0 GB    4 weeks ago     \n",
      "deepseek-r1:8b                                               28f8fd6cdc67    4.9 GB    4 weeks ago     \n",
      "hf.co/lmstudio-community/Qwen2.5-7B-Instruct-1M-GGUF:Q8_0    2d4b678222de    8.1 GB    4 weeks ago     \n",
      "deepseek-r1:32b                                              38056bbcbb2d    19 GB     4 weeks ago     \n",
      "qwen2.5-coder:32b                                            4bd6cbf2d094    19 GB     3 months ago    \n",
      "mxbai-embed-large:latest                                     468836162de7    669 MB    3 months ago    \n",
      "llama3.2:1b                                                  baf6a787fdff    1.3 GB    4 months ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b864e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker 1: Alright everyone, welcome back to another mind-bending episode! Today we’re diving into one of the most exciting areas of artificial intelligence — Knowledge Distillation. It's like teaching your smartest friend everything they know, but with robots and code instead of words and hugs. I'm joined by a super excited speaker who’s curious as hell about this topic. So let’s dive in and see what all the fuss is about!\n",
      "Speaker 2: Um, yeah! This sounds wild. Knowledge Distillation? Is it like transferring skills from one robot to another?\n",
      "Speaker 1: Exactly! It's fascinating, right? Imagine taking advanced capabilities from leading proprietary Large Language Models (LLMs) — you know, those super expensive models that cost a fortune — and distilling their knowledge into more accessible open-source counterparts. It’s like making the best of both worlds!\n",
      "Speaker 2: Hmm, so these LLMs are basically like geniuses, right? But they're really pricey and hard to get?\n",
      "Speaker 1: Bingo! Exactly. Models like GPT-4 or Gemini have incredible problem-solving capabilities but come with hefty price tags. That’s where Knowledge Distillation steps in — it takes that advanced knowledge from these proprietary models and transfers it into more accessible open-source LLMs like LLaMA and Mistral.\n",
      "Speaker 2: Oh wow! So the end goal is to make smart AI technology available to everyone, not just those with deep pockets?\n",
      "Speaker 1: Yes, exactly! It’s all about making advanced capabilities more widely accessible. But it's not just about accessibility; these open-source models can also become more efficient and powerful through distillation.\n",
      "Speaker 2: Umm, that sounds awesome! But how does this actually work? How do they transfer the knowledge?\n",
      "Speaker 1: Well, let me break it down for you. Knowledge Distillation involves several key mechanisms within its framework, such as supervised fine-tuning, where a model is trained on a smaller dataset to adapt to specific tasks or domains.\n",
      "Speaker 2: So, umm, how does that work in practice?\n",
      "Speaker 1: Good question! Think of it like this: imagine you have a teacher — let’s say GPT-4 — who knows everything. Now, we want our student model (say LLaMA) to learn from the teacher without having access to all the proprietary data or resources. We teach LLaMA using techniques such as soft target training where it learns from the softened softmax output of the teacher.\n",
      "Speaker 2: Ohhhh, so like a kind of mentorship?\n",
      "Speaker 1: Exactly! But wait, there’s more. Another critical aspect is Data Augmentation (DA). This isn’t just about expanding the dataset but generating novel, context-rich training data tailored to specific domains and skills using LLMs.\n",
      "Speaker 1: Yeah, it really is! By creating targeted datasets, we ensure that the distilled model acquires a deep understanding of its domain. And this isn't just about making more data but ensuring that the new data is rich and relevant.\n",
      "Speaker 2: Right, so it’s like giving LLaMA not only the knowledge of GPT-4 but also teaching it how to think like GPT-4?\n",
      "Speaker 1: Precisely! The distillation process includes strategies like chain-of-thought prompting, where the student model learns the reasoning process of the teacher. This helps improve problem-solving and decision-making capabilities.\n",
      "Speaker 1: Absolutely! It’s a holistic approach that ensures the distilled models are not only more efficient but also retain high levels of performance and understanding. This way, open-source LLMs can approximate the capabilities of proprietary ones.\n",
      "Speaker 2: Umm, what kind of real-world applications do you see for this?\n",
      "Speaker 1: Great question! Knowledge Distillation has transformative impacts in fields like healthcare, law, finance, and science. In healthcare, for example, models trained via distillation can have accurate contextual knowledge to assist with medical diagnoses.\n",
      "Speaker 2: Wow, that’s huge! So it could revolutionize industries where precision is critical?\n",
      "Speaker 1: Yes! And not just that, but also in fields like law where nuanced understanding and precision are paramount. By enhancing open-source models, we ensure they can handle complex tasks with the same level of sophistication as proprietary models.\n",
      "Speaker 2: This all sounds so futuristic! How do you see it evolving in the future?\n",
      "Speaker 1: Well, there’s still much to explore. As AI continues to evolve, Knowledge Distillation will likely become even more advanced and efficient. Future research might focus on refining techniques like reinforcement learning or exploring new ways of integrating human-like reasoning into these models.\n",
      "Speaker 2: Umm, what are some current challenges?\n",
      "Speaker 1: There are several challenges. Ensuring ethical use is a big one — we need to comply with legal terms and ensure the application of Knowledge Distillation remains lawful and ethical. Also, bridging the knowledge gap between proprietary and open-source models requires continuous research and innovation.\n",
      "Speaker 2: That makes sense! So it’s not just about making technology better but also ensuring it’s used responsibly?\n",
      "Speaker 1: Exactly! It’s a dual mission — to democratize access to advanced AI capabilities while maintaining ethical standards. And that, my friend, is the heart of Knowledge Distillation in this era of Large Language Models.\n",
      "Speaker 1: Me too! Knowledge Distillation is set to change the game, making smart technology more accessible and efficient for everyone. Stay curious and keep exploring!\n",
      "Speaker 2: Thanks so much for breaking it down for me today! This has been a wild ride into the future of AI.\n",
      "Speaker 1: My pleasure! We’ll keep diving deep into these fascinating topics on our podcast. Until next time, stay tuned and keep thinking big!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Assuming response['message']['content'] contains your string\n",
    "content_string = response['message']['content']\n",
    "\n",
    "# Define a regular expression pattern to match the tuples\n",
    "pattern = r'\\(\"([^\"]+)\", \"([^\"]+)\"\\)'\n",
    "\n",
    "# Find all matches in the string\n",
    "matches = re.findall(pattern, content_string)\n",
    "\n",
    "# Convert matches to a list of tuples\n",
    "content_list_of_tuples = [(speaker, text) for speaker, text in matches]\n",
    "\n",
    "# Now you can iterate over the list and access each tuple\n",
    "for speaker, text in content_list_of_tuples:\n",
    "    print(f\"{speaker}: {text}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3d04f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
