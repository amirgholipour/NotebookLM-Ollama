1. **Title:** A Survey on Knowledge Distillation of Large Language Models
   - Authors: Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, Tianyi Zhou
   - Affiliations: The University of Hong Kong, University of Maryland, Microsoft, University of Technology Sydney, Peking University, The University of Sydney
   - Contact: {shawnxxh,chongyangtao,hishentao}@gmail.com, {minglii,tianyi}@umd.edu, ckcheng@cs.hku.hk, jl0725@connect.hku.hk

**Abstract:** Knowledge Distillation (KD) is a significant methodology in transferring advanced capabilities from leading proprietary Large Language Models (LLMs), like GPT-4, to open-source alternatives such as LLaMA and Mistral. As open-source LLMs grow, KD serves dual purposes: compressing these models and enabling their self-improvement through self-teaching. This paper offers a comprehensive survey of KD's role within the context of LLMs, emphasizing its crucial function in knowledge transfer.
Advanced knowledge distillation techniques applied to smaller models, enhancing model compression and self-improvement capabilities. The survey focuses on three key areas: algorithms, skills, and verticalization. It delves into knowledge distillation (KD) mechanisms, cognitive ability enhancements, and practical applications across various sectors. Notably, the survey explores the synergy between data augmentation (DA) and KD, demonstrating how DA strengthens KD's performance within large language models (LLMs). By utilizing DA to produce context-rich, skill-specific training data, KD surpasses conventional limitations, allowing open-source models to mirror the contextual proficiency, ethical alignment, and in-depth semantic understanding of proprietary models. This research aims to serve as a valuable resource for researchers and practitioners, providing a thorough overview of current methodologies in knowledge distillation.
processed text:

Title: Knowledge Distillation of Large Language Models - A Survey and Future Directions

This survey explores knowledge distillation (KD) techniques applied to large language models (LLMs), aiming to make AI solutions more accessible, efficient, and powerful. It bridges the gap between proprietary LLMs like GPT-3.5, GPT-4, Gemini, and Claude, with open-source alternatives.

The authors emphasize the importance of adhering to legal terms governing LLM use, ensuring ethical and lawful application of KD techniques. An associated GitHub repository is available at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.

Keywords: Large language models, knowledge distillation, data augmentation, skill distillation, supervised fine-tuning.

Introduction: In the rapidly evolving field of artificial intelligence (AI), proprietary LLMs such as GPT-3.5, GPT-4, Gemini, and Claude have revolutionized natural language processing (NLP) with their vast scale and capabilities. This survey delves into knowledge distillation techniques for these models, paving the way for broader AI accessibility and efficiency.
Processed Text:

Large Language Models (LLMs) have unlocked new realms of possibility, from generating human-like text to offering sophisticated problem-solving capabilities. Their core significance lies in their emergent abilities, where models display capabilities beyond their explicit training objectives, enabling them to tackle a diverse array of tasks with remarkable proficiency. These models excel in understanding and generation, driving applications from creative content creation to complex problem-solving.

The potential of these models extends far beyond current applications, promising to revolutionize industries, augment human creativity, and more. For simplicity, we use 'proprietary' to represent both versatile yet closed-source LLMs like GPT-4 and open-source yet massive LLMs like LLaMA-2-70B, which encapsulate rich knowledge with a large number of parameters.
redefine our interaction with technology. Proprietary large language models (LLMs) like GPT-4 and Gemini, despite their impressive capabilities, have limitations. Key issues include limited accessibility and higher costs (OpenAI et al., 2023). These models often involve significant usage fees and restricted access, making them less accessible for individuals and smaller organizations.

Data privacy and security are also concerns when using proprietary LLMs (Wu et al., 2023a). They typically require sending sensitive data to external servers, raising privacy and security issues. This is particularly problematic for users handling confidential information.

Furthermore, the general-purpose design of proprietary LLMs might not always meet the specific needs of niche applications. The constraints of accessibility, cost, and adaptability thus pose challenges.
Open-source language models like LLaMA (Touvron et al., 2023) and Mistral (Jiang et al., 2023a) offer significant advantages over proprietary LLMs. Their key benefits include accessibility and adaptability. Unlike proprietary models, these open-source alternatives do not impose licensing fees or restrictive usage policies, making them widely available to a diverse range of users, from individual researchers to smaller organizations. This openness promotes collaboration and inclusivity in AI research, driving innovation and varied applications. Furthermore, the customizable nature of open-source LLMs enables more tailored solutions to address specific needs that larger, generic models may overlook. Nevertheless, open-source LLMs also face challenges due to their relatively limited scale.
Open-source models often face significant limitations compared to proprietary ones. A major issue is their smaller scale, which can lead to lower performance on real-world tasks involving multiple instructions (Zheng et al., 2023a). These models, due to fewer parameters, may struggle to grasp the extensive knowledge base of larger models like GPT-4.

Additionally, pre-training investment in open-source models is usually less substantial. This can result in a narrower range of pre-training data, potentially limiting their understanding and handling of diverse or specialized topics (Liang et al., 2022; Sun et al., 2024a).

Furthermore, open-source models often undergo fewer fine-tuning steps due to resource constraints. Fine-tuning is vital for optimizing a model's performance for specific tasks or industries. The lack of extensive fine-tuning can hinder the model's effectiveness in specialized applications.
Knowledge Distillation (KD) techniques have gained traction to bridge the performance gap between proprietary and open-source Language Learning Models (LLMs). This approach, similar to learning from a skilled teacher, uses advanced proprietary models like GPT-4 or Gemini as a guiding framework. Unlike traditional knowledge distillation algorithms, data augmentation (DA) has become a common method. It involves enhancing the competencies of open-source LLMs by mimicking the performance characteristics of their proprietary counterparts.
processed text:

Knowledge Distillation (KD) serves as a paradigm for LLM knowledge transfer using a minimal seed of information to prompt the model for skill or domain-specific data generation. It continues to play a crucial role in compressing LLMs, improving efficiency without substantial performance loss. Recently, utilizing open-source LLMs as self-improvement teachers has shown promise, significantly boosting their capabilities (Yuan et al., 2024a; Chen et al., 2024a). Figure 1 visually represents these three primary roles of KD in LLMs: 1) Capability enhancement, 2) traditional compression, and 3) self-improvement.
compression for efficiency, self-improvement via self-generated knowledge (in-context learning, in-instruction following), improved alignment with user intents (human values/principles, chain-of-thought thinking patterns), and NLP task specialization (semantic understanding, code generation). These skills are vital for LLMs' diverse applications, from casual conversations to complex problem-solving. In vertical domains like healthcare, law, or science, knowledge distillation enables open-source models to enhance performance by learning from proprietary models.
Knowledge distillation has been extensively trained and fine-tuned in various areas. Its benefits are multifaceted and transformative. Through a range of distillation techniques, the gap between proprietary and open-source models is significantly narrowed and even filled. This process not only simplifies computational requirements but also improves the environmental sustainability of AI operations. As open-source models become more proficient with less computational overhead, they enhance accessibility and equity in AI. Smaller entities and individual researchers gain access to state-of-the-art capabilities, promoting wider participation and diversity in AI advancements. This democratization of technology leads to more robust, versatile, and accessible AI solutions, driving innovation and growth across industries.
processed text:

The urgency for a comprehensive survey on knowledge distillation in Large Language Models (LLMs) arises from the rapid advancements in AI and the growing complexity of these models. As AI permeates various sectors, efficiently transferring knowledge from proprietary LLMs to open-source ones becomes increasingly crucialâ€”not just a technical goal, but a practical necessity. This demand is fueled by the rising need for accessible, cost-effective, and adaptable AI solutions across diverse vertical domains such as Law, Healthcare, Finance, Science, and more. Key aspects include:

- Student models (LlamaGPT, Vicuna, OPT, etc.)
- Seed knowledge and generated datasets
- Demonstrations and raw data inputs
- Context setting, following, and alignment agents in NLP tasks
- Specialization in multi-modal skills
- Teacher LLMs (GPT-4, Claude, Gemini)
- Instructions for skill domain knowledge elicitation and distillation algorithms
- Train divergence and similarity features.
Processed Text:

**Title:** Survey on Knowledge Distillation of Large Language Models

**Objective:** This survey aims to synthesize current methodologies, challenges, and breakthroughs in knowledge distillation. It serves as a guide for researchers and practitioners, helping them distill complex AI capabilities into more manageable forms. The survey also identifies gaps in current techniques and proposes directions for future research.

**Key Components:**

1. **Reward Model (RM!(Â·)):** A model that outputs rewards.
2. **Supervised Fine-tuning (X, Y):** A process involving input data (X) and corresponding labels (Y).
3. **Preference Rank Optimization (y, 1, 2, 3):** A method for ranking preferences (y) with multiple options (1, 2, 3).
4. **Data Curation (X, Y, raw data, synthesized feedback):** Involves raw data (X, Y) and feedback used for data synthesis.
5. **Self-Knowledge:** The process of extracting knowledge from the model's own outputs.
6. **Feature Extraction (Sec. 3.1, Sec. 3.2):** Two sections detailing feature extraction methods.
7. **Demonstration Expansion (X, Y):** Using demonstrations to expand features in data sets (X, Y).
8. **Labeling Expansion:** Expanding labeling methods beyond basic options.
9. **Fig. 2:** An overview of the survey, detailing steps in Knowledge Distillation of Large Language Models (KD of LLMs). The figure abbreviates 'Section' as 'Sec.' for clarity.
Future research in knowledge distillation for Large Language Models (LLMs) is structured into several detailed sections. This structure begins with an introduction, followed by Â§2 offering a fundamental overview of knowledge distillation. It contrasts conventional methods with those arising in the LLM era, emphasizing data augmentation's role.

Â§3 investigates techniques for extracting knowledge from teacher LLMs and primary distillation algorithms. This covers strategies ranging from supervised fine-tuning to intricate approaches involving divergence, similarity, reinforcement learning, and ranking optimization.

Lastly, Â§4 concentrates on skill distillation, examining how student models can be refined for better context comprehension, alignment with user intentions, and performance across various Natural Language Processing (NLP) tasks. Discussions include aspects of natural language understanding.
Processed Text:

This text covers key areas in AI and NLP, including Natural Language Understanding (NLU), generation (NLG), information retrieval, recommendation systems, and text evaluation. It delves into domain-specific vertical distillation in fields like law, healthcare, finance, and science, highlighting practical applications and potential impacts. The survey identifies open problems and research gaps in knowledge distillation. A conclusion and discussion synthesize insights, reflecting on implications for the broader AI and NLP community and suggesting future research directions. Figure 2 provides an overview of this survey.

The concept of knowledge distillation in AI and deep learning involves transferring knowledge from large, complex models to smaller ones.
Process Text:

In the field of machine learning, a method called knowledge distillation is used. This involves transferring knowledge from a larger model (teacher) to a smaller, more efficient one (student). This approach is crucial for overcoming computational challenges and resource limitations when deploying large-scale models in practical applications. Historically, this technique has been applied to move knowledge from complex neural networks to compact, efficient architectures, primarily to enable deployment in resource-constrained environments like mobile devices or edge computing platforms. These earlier methods focused on ad-hoc neural architecture selection and task-specific training objectives.

Notable recent advancements include Knowledge Labeling AnnoLLM (He et al., 2023a) and PandaLM (Wang et al., 2023b).
CoT-Distill, Orca, Orca 2, Baize, Mammoth, Mixed Distill, ExpansionSelf-Instruct, Alpaca, Code Alpaca, Self-Align, WizardLM, WizardCoder, WizardMath, AugGPT, TDG, UltraChat, Phi-1, Phi-1.5, Phi-2, Magicoder, WaveCoder, ZeroGen, SunGen, InPars, BabyLlama, MiniLLM, GKD, QuantGPT, LLM-QAT, FeedbackCAI, WizardMath, UltraFeedback, Zephyr
Processed Text:

Self-Knowledge and Self-Instruct Methods: CycleAlign, RLAIF, Lion, PERsD, GKD, Self-Knowledge, Self-Instruct, RLCD, ImpDistill, LMSI, ReST, Self-Rewarding, Baize, STaR.

Supervised Fine-Tuning Models: Alpaca, Vicuna, WizardLM, Baize, STaR.

Distillation Techniques: DistilGPT, f-Distill, MiniLLM, TED, GKD, BabyLlama.

Reinforcement Learning Methods: CAI, UltraFeedback, WizardMath, MiniLLM.
Gu et al. (2024), Agarwal et al. (2024) - GKD, Kwon et al. (2023) - GPT3 Reward, Tunstall et al. (2023) - Rank Optimization Zephyr, Hong et al. (2023) - CycleAlign, Wang et al. (2022a) - Context Following, Instruction Following, Self-Instruct, Taori et al. (2023) - Alpaca, Chiang et al. (2023) - Vicuna, Xu et al. (2023a) - WizardLM, Mukherjee et al. (2023) - Orca, Mitra et al. (2023) - Orca 2, Luo et al. (2023b) - WizardMath, Peng et al. (2023a) - Llama-GPT4, Chiang et al. (2023) - Multi-turn DialogueVicuna, Xu et al. (2023b) - Baize, Ding et al. (2023b) - UltraLLaMA, Li et al. (2023b) - CAMEL, Wang et al. (2023c) - OpenChat, Kang et al. (2023a) - RAG Capability KARD, Luo et al. (2023c) - SAIL, Asai et al. (2023) - Self-RAG, Ye et al. (2023) - AlignmentThinking PatternSelfee, Wang et al. (2023d) - AFT, Cheng et al. (2023) - AdaptLLM, Zhang et al. (2023a) - KnowPAT, Bai et al. (2022a) - PreferenceCAI, GPT-3
Reward, ILF, ALMoST, RLEF, RLAIF, Zephy, UltraFeedback, ValueCAI, Align Honesty, SANDBOX, Self-Align, RLCD, AgentTool, UsingToolformer, Graph-ToolFormer, Gorilla, ToolAlpaca, ToolLLM, CRAFT, Confucius, MLLM-Tool, Î±-UMi, PlanningFireAct, AgentTuning, Lumos, AUTOACT, TPTU-v2, NLUAugGPT, GPT Annotation, TDG, SunGen
2023a), Mix Distill (Chenglin et al., 2023), Annollm (He et al., 2023a), UDG (Wang et al., 2021a), ZeroGen (Ye et al., 2022), NLGInheritSumm (Xu et al., 2023c), RECOMP (Xu et al., 2024b), MaRio (Ramnath et al., 2023), ID (Jung et al., 2023), GPT-3 Labeling (Wang et al., 2021b), BioGPT (Guo et al., 2023a), ChatGPT NMT (Yang and Nicolai, 2023), Information RetrievalQUILL (Srinivasan et al., 2022), Promptgator (Dai et al., 2023b), InPars (Bonifacio et al., 2022), AugTriever (Meng et al., 2023), RankVicuna (Pradeep et al., 2023a), RankZephyr (Pradeep et al., 2023b), ExaRanker (Ferraretto et al., 2023), Recommendation NDR (Mysore et al., 2023), InstrcutRec (Zhang et al., 2023b), ONCE (Liu et al., 2023c), Text Generation EvaluationPandaLM (Wang et al., 2023b), Prometheus (Kim et al., 2024), InstructScore (Xu et al., 2023d), TigerScore (Jiang et al., 2023c), Auto-J (Li et al., 2024a), CodeCodeAlpaca (Chaudhary, 2023), CodeLlama (Roziere et al., 2023).
Phi-1 (Gunasekar et al., 2023), PERsD (Chen et al., 2023a), MFTCoder (Liu et al., 2023d), WaveCoder (Yu et al., 2024), Code Clean (Jain et al., 2023), Multi-ModalityLLaVA (Liu et al., 2023e), SVIT (Zhao et al., 2023b), LVIS-Instruct4V (Wang et al., 2023e), Shikra (Chen et al., 2023c), LSKD (Park et al., 2023), DetGPT (Pi et al., 2023; Zhao et al., 2023c), LRV (Liu et al., 2023f), NExT-GPT (Wu et al., 2023b), Valley (Luo et al., 2023d), ILuvUI (Jiang et al., 2023d), StableLLaVA (Li et al., 2023c), PointLLM (Xu et al., 2023e)

Involved training a smaller student network for Verticalization Distillation, as detailed in Figure 7.

Applications include Medical & Healthcare (Zhang et al., 2023c; Chen et al., 2023d), Finance (Zhang and Yang, 2023), Science (Xie et al., 2023a; Zhang et al., 2024), and Misc. (Dan et al., 2023; Guo et al., 2023b).
In AI and DL, knowledge distillation techniques often involve mimicking a larger teacher network's output using methods like soft target training, where the student learns from the softened softmax output. For comprehensive insights, refer to Gou et al., 2021. However, the emergence of Large Language Models (LLMs) has transformed this field. Today, knowledge distillation in LLMs focuses on knowledge elicitation and transfer rather than just architecture compression. This shift is primarily due to the extensive and profound knowledge held by LLMs like GPT-4 and Gemini. The complex parameters of these models make traditional compression methods, such as pruning (Han et al., 2016) or quantization (Liu et al., 2023a), challenging. Unlike the past, where the objective was to replicate the teacher model's output or minimize model size, current distillation aims at different goals.
focus is to extract specific knowledge from Large Language Models (LLMs). The approach involves using heuristic and meticulously designed prompts to elicit particular knowledge or capabilities from LLMs (Ding et al., 2023b; Chaudhary, 2023). These prompts are tailored to explore LLMs' understanding across various domains, including natural language processing (He et al., 2023a) and complex cognitive tasks such as reasoning (Hsieh et al., 2023) and problem-solving (Qiao et al., 2024). This prompt-based method provides a more adaptable and dynamic distillation process. It enables targeted knowledge extraction, focusing on specific skills or domains of interest. This technique is especially effective in leveraging LLMs' emergent abilities, where models demonstrate capabilities beyond their initial training objectives.
distillation. It emphasizes the transfer of abstract qualities like reasoning patterns, preference alignment, and value alignment. This contrasts with earlier output replication methods. Current techniques involve replicating outputs and emulating thought processes and decision-making patterns of teacher models. Strategies include chain-of-thought prompting, training student models to learn teacher's reasoning for enhanced problem-solving and decision-making. 2.2 Relation to Data Augmentation (DA): In the LLMs era, Data Augmentation (DA) is crucial. It aids in processes like knowledge distillation by enhancing data diversity and model robustness.
Knowledge Distillation (KD) differs from conventional Data Augmentation (DA) techniques like paraphrasing or back-translation. Unlike these methods that mechanically expand training datasets, DA in Large Language Models (LLMs) generates novel, context-rich data for specific domains and skills. The interplay between DA and KD in LLMs is mutually beneficial and foundational.

KD uses a seed of knowledge and employs DA to prompt LLMs to generate explicit data that embodies particular skills or domain expertise. This method effectively bridges the knowledge and capability gap between proprietary and open-source models. By using DA, LLMs produce targeted, high-quality datasets, not just larger in volume but also diverse and specific, enhancing the distillation process's effectiveness.
Distilled models aim to replicate not just the output behavior but also the underlying understanding and cognitive strategies of teacher models. Data Augmentation (DA) acts as a catalyst, allowing these models to gain and refine abilities that would otherwise necessitate massive datasets and computational power. This approach emphasizes qualitative learning aspects rather than quantitative growth.

The strategic application of DA within Knowledge Distillation (KD) processes signifies a significant shift towards efficiency, sustainability, and accessibility in leveraging Large Language Models (LLMs). It enables open-source models to approximate the contextual proficiency, ethical alignment, and deep semantic insights typically associated with proprietary models. This democratizes access to advanced AI capabilities and stimulates innovation across a wider range of applications and users.

2.3 Survey Scope: (No changes needed as it seems to be a heading or section identifier.)
Building on previous discussions, this survey thoroughly examines knowledge distillation in Large Language Models (LLMs) using a structured taxonomy (Figure 3). The survey's focus spans three main areas: KD Algorithms, Skill Distillation, and Verticalization Distillation. Each area covers various subtopics and methodologies.

KD Algorithms: This section dives into the technical foundations and techniques of knowledge distillation. It covers the processes of extracting knowledge from teacher models (like proprietary LLMs) and integrating this knowledge into student models (such as open-source LLMs). Strategies under 'knowledge' include labeling (Hsieh et al., 2023) and expansion (Taori, etc.).
Processed Text:

The research explores knowledge identification, expansion, and curation for effective distillation. Techniques include curation (Gunasekar et al., 2023), feature understanding (Agarwal et al., 2024), feedback mechanisms (Tunstall et al., 2023), and self-knowledge generation (Wang et al., 2022a). Distillation approaches cover supervised fine-tuning (SFT) (Wang et al., 2022a), divergence minimization (Agarwal et al., 2024), reinforcement learning techniques (Cui et al., 2023a), and rank optimization strategies (Tunstall et al., 2023). These methods show how Knowledge Distillation (KD) allows open-source models to acquire knowledge from proprietary ones. The 'Skill Distillation' aspect focuses on enhanced competencies and capabilities through KD, with discussions on context following (Taori et al., 2023; Luo et al., 2023c), including instruction following subtopics.
Retrieval-Augmented Generation (RAG) Capability. The survey explores alignment aspects including thinking patterns, persona/preference modeling, and value alignment. It also examines 'agent' skills like Tool Using and Planning. NLP task specialization is analyzed through various lenses such as natural language understanding (NLU), natural language generation (NLG), information retrieval, recommendation systems, text generation evaluation, and code generation. The survey further delves into multi-modality, showcasing how Knowledge Distillation (KD) enhances Large Language Models' (LLMs) ability to handle multiple input forms. Verticalization Distillation assesses KD's application across diverse fields like Law and Medical & Healthcare.
2023a), Finance (Zhang and Yang, 2023), Science (Zhang et al., 2024), among others. This exploration showcases practical implications of KD techniques and their transformative impact on domain-specific AI solutions. The survey provides a comprehensive analysis of KD in LLMs, guiding researchers and practitioners through methodologies, challenges, and opportunities.

Declaration: This survey offers a broad overview of knowledge distillation techniques applied to LLMs, focusing on algorithms, skill enhancement, and domain-specific applications. Acknowledging the vast and rapidly evolving nature of this field, it aims to introduce key studies and developments, though not exhaustively covering every pertinent detail due to its dynamic nature.
2.4 Distillation Pipeline in LLM Era
- Seed: Knowledge source (Teacher)
- Skill/Domain: Area of expertise
- TeacherLLM: Large Language Model (Teacher)
- KnowledgeElicitation: Extracting knowledge from TeacherLLM
- StudentModel: Less complex model to receive knowledge
- DistillationAlgorithm: Methods used for transfer
- GeneratedKnowledge: Knowledge transferred
- LearningObjective: Goal of the distillation process
- train: Training phase

Figure 4 illustrates a general pipeline for distilling knowledge from a large language model (TeacherLLM) to a student model. This structured process aims to transfer advanced capabilities of models like GPT-4 or Gemini into more accessible and efficient open-source alternatives. The pipeline consists of four main stages:
1. Knowledge source selection (Seed)
2. Identifying the area of expertise (Skill/Domain)
3. Extracting knowledge from TeacherLLM (KnowledgeElicitation)
4. Transferring knowledge to StudentModel using DistillationAlgorithm, with a focus on achieving the LearningObjective during the train phase.
1. Directing Teacher LLM: The first step involves guiding the teacher Language Learning Model (LLM) towards a specific skill or domain using carefully crafted instructions or templates. These prompts aim to highlight the LLM's expertise in areas such as healthcare, law, reasoning, or language understanding.
2. Seed Knowledge Input: After defining the target area, seed knowledge is introduced into the teacher LLM. This seed knowledge usually consists of a small dataset or specific data clues relevant to the desired skill or domain. It serves as a stimulus, encouraging the teacher LLM to produce more comprehensive and detailed outputs based on this initial information. The seed knowledge is vital as it establishes a foundation for the teacher model to build upon and expand.
III. Generation of Distillation Knowledge:
   - In response to seed knowledge and steering instructions, the teacher LLM generates comprehensive question-and-answer (QA) dialogues or narrative explanations.
   - These outputs primarily align with natural language processing/understanding capabilities.
   - In specialized cases, outputs may include logits or hidden features, though this is less frequent due to complexity and specific requirements.

IV. Training the Student Model:
   - The generated knowledge examples are used to train the student model.
   - This training is guided by a loss function tailored to the learning objective.
Learning Objectives: The loss function gauges the student model's ability to replicate or adapt knowledge from the teacher model. By minimizing this loss, the student model learns to mimic the target skills or domain knowledge of the teacher, thereby acquiring similar capabilities. This involves iteratively adjusting the student model's parameters to minimize the discrepancy between its outputs and those of the teacher model, ensuring effective knowledge transfer.

In essence, this process can be abstracted into two formulations. The first formulation represents the knowledge elicitation process: D(kd) = {Parse(o, s) | o âˆ¼ pT(o|I âŠ• s), âˆ€s âˆˆ S}, where âŠ• denotes fusing two pieces of text, I denotes an instruction or template for a task, skill, or domain to guide the LLM and elicit knowledge, s âˆˆ S denotes an example of seed knowledge upon which the LLM can build to generate novel knowledge, and Parse(o, s) stands for parsing the distillation.
processed text:

In this section, we delve into the algorithms that have become prominent in the era of knowledge distillation with Language Models (LLMs). As per Section 2.4, these algorithms are categorized into two main steps: 'Knowledge' for extracting information from teacher LLMs, and 'Distillation' for integrating this knowledge into student models.

The learning objective is defined as L=X ILI(D(kd) I;Î¸S), (2) where D(kd) represents the datasets built for distillation, Î¸S parameterizes the student model, and LI(Â·;Â·) denotes a specific learning objective. Note that this process can involve multiple tasks or skills being distilled into one student model.
3.1 Knowledge Elicitation Methods

This section delves into strategies for extracting knowledge from teacher Large Language Models (LLMs). These methods are categorized into Labeling, Expansion, Data Curation, Feature, Feedback, and Self-Knowledge. Figure 5 visualizes these knowledge elicitation techniques.

3.1.1 Labeling

Labeling involves using a teacher LLM to label the output y for a given input x as seed knowledge. This is done based on an instruction I or demonstrations c, where c consists of pairs (x1, y1), ..., (xn, yn). This approach to extracting knowledge from teacher LLMs is simple yet effective, used across various tasks and applications. It only necessitates the collection of an input dataset and its feeding into LLMs to generate desired outputs. The generation of y can be controlled via the predefined I and c. The process can be mathematically represented as follows: D(lab) = {x, y | x ~ X}
y~pT(y|IâŠ•câŠ•x)}. (3) The input x can originate from NLP task datasets, often used for distillation purposes. Many studies have attempted to utilize the capabilities of large language models (LLMs) as teachers for annotating dataset samples across various tasks.

In natural language understanding, LLMs are employed to categorize text (Gilardi et al., 2023; Ding et al., 2023a; He et al., 2023a). For natural language generation, LLMs aid in creating sequences for outputs (Hsieh et al., 2023; Jung et al., 2023; Wang et al., 2021b).

Text generation evaluation tasks use LLMs to label evaluated results (Li et al., 2024b; Wang et al., 2023b), while reasoning tasks employ LLMs for labeling Chains of Thought (CoT) explanations (Hsieh et al., 2023; Li et al., 2022; Ho et al., 2023; Magister et al., 2023; Fu et al., 2023; Ramnath et al., 2023; Li et al., 2023d; Liu et al., 2023g).

Instead of focusing on individual tasks, numerous studies leverage LLMs for diverse applications.
processed text: Current research focuses on training models to follow instructions for various NLP tasks. Collections of these tasks, coupled with instructional templates, serve as valuable training data. For example, FLAN-v2 (Longpre et al., 2023) provides extensive publicly available task sets with instructions, labeled by responses generated by teacher LLMs in Orca (Mukherjee et al., 2023; Mitra et al., 2023). However, these instructional templates lack diversity and may not fully capture human-like queries. Real conversations between humans and chat models offer large-scale data with genuine queries and responses labeled by powerful LLMs, like ShareGPT. Furthermore, researchers like Xu et al. (2023b) and Anand et al. (2023) label real questions from forums such as Quora and Stack Overflow. The labeling process can be guided by various methods.
Chain-of-thought (CoT) prompts are a common instruction type for guiding labeling. They've been used effectively by Hsieh et al. (2023), Fu et al. (2023), and Magister et al. (2023). Mukherjee et al. (2023) enhance this method by adding system messages to encourage detailed, step-by-step responses. Yue et al. (2023a) and Chenglin et al. (2023) combine chain-of-thought and program-of-thought rationales for labeling. Xu et al. (2023b) introduce a self-chat technique where two teacher LLMs mimic real conversations to generate multi-turn dialogues from Quora and Stack Overflow.

Despite its simplicity and effectiveness, this approach has limitations. It's constrained by the scale and variety of input data, especially in real-world applications involving user conversations. Privacy concerns also arise when handling such data.
Processed Text:

Various expansion methods have been proposed to tackle the limitations of large language models (LLMs). These methods, detailed in studies by Wang et al. (2022a), Taori et al. (2023), Chaudhary (2023), Si et al. (2023), Ji et al. (2023a), Luo et al. (2023b,a), Wu et al. (2023c), Sun et al. (2024b), Xu et al. (2023a), and Guo et al. (2023c), leverage the in-context learning capability of LLMs to generate a diverse, large-scale dataset similar to given demonstrations.

Unlike the labeling approach, where input x is drawn from the existing dataset, the expansion approach generates both x and y using teacher LLMs. This process can be encapsulated as: D(exp) = {(x, y) | x ~ pT(x|IâŠ•c), y ~ pT(y|IâŠ•x)}.
Meta-Information Demonstrations Instruction Output Feedback
Labeling: Teacher generates output from input
Expansion: Teacher generates similar samples via in-context learning
Data Curation: Teacher synthesizes data based on meta-information (topic or entity)
Feature: Extract internal knowledge (logits, features) from teacher using provided data
Feedback: Teacher offers corrections, expansions, preferences on student's generations
Self-Knowledge: Student generates outputs, filtered for quality or evaluated by itself.
x and y represent new input-output pairs produced by the teacher Large Language Model (LLM). Input x is derived from a set of initial input-output demonstrations c. Output y is then generated in response to the new input x, guided by an instruction I. These demonstrations can be predefined or updated dynamically by incorporating newly generated samples. Expansion techniques are commonly used to extract extensive instruction-following knowledge from teacher LLMs. Wang et al. (2022a) pioneered an iterative bootstrapping method, Self-Instruct, which employs LLMs to generate a diverse range of instructions based on several demonstrations sampled from 175 manually-written instructions. The newly generated instructions are then reintroduced into the initial pool, enhancing subsequent expansion iterations. Later, Taori et al. (2023) applied this expansion method to a more potent teacher LLM, text-davinci-003, to distill 52K high-quality data.
Wu et al. (2023c) and Sun et al. (2024b) use a teacher LLM to create instructions for specific topics. Xu et al. (2023a) introduce the Evol-Instruct method to broaden instruction sets in two ways: increasing complexity and enhancing diversity. This method, applicable across domains, has been employed to expand coding (Luo et al., 2023a) and math (Luo et al., 2023b) distillation.

Expansion techniques can substantially enrich NLP task datasets with comparable samples, boosting performance. For example, AugGPT (Dai et al., 2023a) employs a teacher LLM to rephrase sentences in training data into multiple semantically diverse, yet conceptually similar versions, improving classification accuracy. Similarly, TDG (He et al., 2023b) presents the Targeted Data Generation (TDG) framework.
Automatically identifies challenging subgroups within data and generates new samples for these subgroups using LLMs through in-context learning. The expansion method utilizes LLMs' strengths in in-context learning to create more diverse and extensive datasets with both inputs and outputs. However, the quality and diversity of the generated data heavily depend on the teacher LLMs and initial seed demonstrations. This reliance can result in a dataset with inherent biases from LLMs (Yu et al., 2023a; Wei et al., 2023) and homogeneity issues, where generations may be prone to similarity, ultimately limiting the desired diversity (Ding et al., 2023b). Furthermore, the expansion process can unintentionally amplify any biases present in the seed data.

The Data Curation approach emerges as a solution for achieving high-quality and scalable data generation in knowledge distillation from LLMs.
Data Curation addresses the limitations of Labeling and Expansion methods by producing high-quality or large-scale data using extensive meta-information as seed knowledge (Ding et al., 2023b; Gunasekar et al., 2023; Li et al., 2023a; Mar, 2023; Liu et al., 2023d; Wei et al., 2023; Yu et al., 2024; Ye et al., 2022; Gao et al., 2023a; Yang and Nicolai, 2023). Unlike Labeling's potential for noisy data from task datasets and Expansion's risk of homogeneous data in large quantities derived from seed demonstrations, Data Curation synthesizes data from scratch. It integrates various meta-information, like topics or knowledge points, to create controllable x and y.
processed text:

Data Curation can be meticulously controlled for large, high-quality datasets. The formulation is D(cur)={(x, y)|xâˆ¼pT(x|IâŠ•m), yâˆ¼pT(y|IâŠ•x)}. Here, m represents diverse meta-information guiding x's synthesis, and I guides teacher LLMs to generate x or y. Meta-information sources vary among studies. UltraChat (Ding et al., 2023b) curates high-quality, diverse data using distilled knowledge. They gather extensive meta-info across three domains: Questions about the World, Creation and Generation, and Assistance on Existing Materials. For instance, under Questions about the World, they cover 30 topics like "Technology" and "Food and Drink." Teacher LLMs then use this meta-information to distill a wide range of instructions and conversations.
Substantial scale of 1.5 million instances sets UltraChat apart with its lexical and topical diversity. The UltraLLaMA model, fine-tuned on this data, outperforms other open-source models. Another series focuses on distilling smaller, high-quality datasets similar to "textbooks." Phi-1 (Gunasekar et al., 2023) generates "textbook quality" data in the coding domain by synthesizing clear, self-contained, instructive, and balanced content from LLMs. This is guided by random topics or function names to boost diversity. The distilled data includes 1 billion tokens of Python textbooks with natural language explanations and code snippets, along with 180 million tokens of Python exercises with solutions. Notably, the Phi-1 model, despite its smaller size, outperforms nearly all open-source models on coding benchmarks like HumanEval and MBPP while being 10 times more efficient.
times smaller in size. This method involves leveraging meta-information such as Python knowledge points, raw code collections, or labels to generate instructional data. In NLU tasks, labels serve as meta-information for synthesizing samples, while in information retrieval tasks, documents are used to generate potential queries and build large-scale retrieval pairs. Overall, Data Curation through teacher LLMs is a promising technique for creating high-quality, diverse datasets that are significantly smaller in size.
processed text:

The effectiveness of large-scale models, such as phi-1, in specialized domains highlights the potential of this method. The creation of synthetic datasets is emerging as a vital technical skill and focal point in AI (Li et al., 2023a). Traditional knowledge elicitation methods, often used for powerful black-box models that are costly and less reproducible due to API calls, contrast with white-box distillation. This approach is more transparent and accessible, utilizing output distributions, intermediate features, or activations from teacher Large Language Models (LLMs), collectively termed Feature knowledge. While white-box KD has primarily been examined for smaller encoder-based LMs with fewer than 1 billion parameters (Gou et al., 2021), recent studies are delving into this for generative LLMs (Timiryasov and Tastet).
2023: Research by Liang et al., Gu et al., Agarwal et al., Liu et al., Wen et al., Wan et al., Zhao and Zhu, Qin et al., Boizard et al., Zhong et al. The standard approach to acquiring feature knowledge involves teacher Language Learning Models (LLMs) annotating the output sequence y with internal representations. These annotations are then distilled into the student model using methods like Kullback-Leibler Divergence (KLD).

The process of extracting feature knowledge can be outlined as: D(feat) = {(x, y, Ï• feat(x, y;Î¸T))|xâˆ¼ X, yâˆ¼ Y}. Here, Y is the output set, which could come from teacher LLMs, the student model, or directly from the dataset. Ï•feat(Â·;Î¸T) signifies the operation of extracting feature knowledge (like output distribution) from the teacher LLM. The simplest way to extract teacher's feature knowledge is by labeling a fixed dataset of sequences with token-level probability.
Distillation techniques are employed in various models, including Sanh et al. (2019), Wen et al. (2023), and Liang et al. (2023a) for task-aware layer-wise distillation. This method aligns the student's hidden representations with the teacher's at each layer, focusing on task-relevant knowledge. Gu et al. (2024) and Agarwal et al. (2024) propose a novel approach where the student generates sequences ('self-generated sequences') and learns using feedback from the teacher. This is advantageous when the student model lacks capacity to mimic the teacher's distribution. Additionally, methods like Tao et al. (2022a), Liu et al. (2023a), and Kim et al. (2023b) focus on LLM-quantization, preserving the original output distribution from teacher Large Language Models (LLMs).
Quantizing large language models (LLMs) while preserving performance is a key focus. Feature knowledge can serve as a robust resource for multi-teacher knowledge distillation. Timiryasov and Tastet (2023) employ an ensemble of GPT-2 and LLaMA as teacher models to extract output distributions. FuseLLM (Wan et al., 2024a) innovatively merges various LLMs' capabilities through a weighted fusion of their output distributions, consolidating them into one LLM. This method could significantly boost the student model's abilities, outperforming any single teacher LLM. In essence, feature knowledge provides a more transparent alternative to opaque methods, enabling deeper understanding and control over the distillation process. Leveraging feature knowledge from teacher LLMs, such as output distributions and intermediate layer features, white-box approaches facilitate richer knowledge transfer. This method, while promising, especially in...
Processed Text:

Smaller models aren't ideal for black-box LLMs due to inaccessible internal parameters. Student models distilled from white-box LLMs might perform worse than their black-box counterparts, as the latter (like GPT-4) are typically more potent.

Previous research largely concentrates on one-way knowledge transfer from teacher to student for imitation, neglecting feedback from the teacher on student generation. Teacher feedback usually provides guidance on student outputs via preferences, evaluations, or corrective data. A common method involves the teacher ranking student generations and distilling this preference into the student model through Reinforcement Learning from AI Feedback (RLAIF) (Bai et al., 2022a). The generalized formulation for eliciting feedback knowledge is: D(fb)={(x, y, Ï• fb(x, y;Î¸T))|xâˆ¼ X, yâˆ¼pS(y|x)}, (7)
Where 'y' denotes the student model's output in response to input 'x', and Ï•fb(Â·;Î¸T) signifies feedback from teacher Language Learning Models (LLMs). This process evaluates the student's output 'y' based on input 'x', providing assessment, correction, or guidance. This feedback knowledge can be distilled into the student for generating feedback or refining responses. Numerous methods have been studied to extract this advanced knowledge (Bai et al., 2022a; Luo et al., 2023b; Cui et al., 2023a; Kwon et al., 2023; Jiang et al., 2023b; Chen et al., 2023a; Gu et al., 2024; Agarwal et al., 2024; Chen et al., 2024b; Guo et al., 2024; Ye et al., 2023; Hong et al., 2023; Lee et al., 2023a). Preference, as previously mentioned, is a significant form of feedback knowledge from teacher models. Various types of preference knowledge can be distilled from these models.
Bai et al. (2022a) introduced RLAIF for extracting harmlessness preferences from LLMs. This process involves an SFT-trained LLM generating response pairs for each prompt, ranking them for harmlessness to create a preference dataset. This dataset is then distilled into a Preference Model (PM), which guides the RL training of a more harmless LLM policy.

Wizard-Math (Luo et al., 2023b) focuses on mathematical reasoning. It uses ChatGPT as a teacher to provide process supervision and evaluate the correctness of each step in generated solutions.

Cui et al. (2023a) developed UltraFeedback, a large-scale preference dataset for distilling better preference models. This dataset compiles various instructions and models to produce comparative data. GPT-4 is then used to score candidates based on aspects of preference, including instruction-following, truthfulness, and honesty.
Processed Text:

Teachers can provide detailed feedback beyond just grading student work. In Lion (Jiang et al., 2023b), a teacher model identifies challenging instructions for the student, generating tougher ones to enhance the student's skills. PERsD (Chen et al., 2023a) demonstrates a method where the teacher offers personalized refinement feedback on incorrect code snippets, guided by specific execution errors. SelFee (Ye et al., 2023) uses ChatGPT to generate feedback and revise student answers based on this feedback. Conversely, FIGA (Guo et al., 2024) modifies the student's response by comparing it to the correct answer. Additionally, the teacher model's distribution over student generations can serve as feedback itself. MiniLLM (Gu et al., 2024) and GKD (Agarwal et al., 2024) introduce a novel strategy for this approach.
The student model first produces sequences, then the teacher model generates an output distribution as feedback. This approach utilizes the teacher's expertise to directly influence and enhance the student model's learning process. In Self-Knowledge, the same model functions both as teacher and student, iteratively improving itself by refining its own previously generated outputs. This method eliminates the necessity for an external, potentially proprietary, advanced teacher model like GPT-series LLMs. It also enables the model to transcend the limitations inherent in conventional teacher-student methods. Self-knowledge can be formulated as: D(sk)={(x, y, Ï• sk(x, y))|xâˆ¼ S, yâˆ¼pS(y|IâŠ•x)}, where Ï•sk(Â·) symbolizes a generalized function applied to the self-generated outputs y.
Processed Text:

Recent research explores methodologies to enhance self-knowledge in learning systems, including filtering, rewarding, and evaluation mechanisms. These could be external or student-driven. Innovative approaches have shown potential for more efficient and autonomous learning. Notable among these is Self-Instruct (Wang et al., 2022a), which employs GPT-3 for data augmentation via the Expansion approach, creating additional data samples to enrich the dataset. This fine-tuned dataset then optimizes the original model.
model refinement by filtering out infeasible responses and using them for training. In Self-Align, they address the issue of brief or roundabout answers from fine-tuned models by providing detailed instructions and employing context distillation to transfer this knowledge back to the model. RLCD, on the other hand, uses contrasting prompts to create preference pairs, which are then used to train a preference model guiding the improvement of an unaligned LLM via reinforcement learning. Various methods also utilize filtering techniques to enhance self-generated data, such as Impossible Distillation, which filters out inapplicable responses for training purposes.
processed text:

The tasks involve summarization with filters for entailment, length, and diversity. LMSI (Huang et al., 2023a) creates multiple CoT reasoning paths and answers per question, retaining only those leading to the most consistent answer. Self-knowledge can iteratively improve as the student model enhances. Gulcehre et al. (2023) propose a Reinforced Self-Training (ReST) framework, alternating between Grow and Improve stages. In the Grow stage, the student model generates multiple output predictions. The Improve stage ranks and filters these using a scoring function, then fine-tunes the language model with an offline RL objective on this curated dataset.
2024a presents a framework akin to iterative DPO. This involves fine-tuning a language model to distinguish self-generated responses from human-annotated data, treating these as "negative knowledge" to enhance alignment with the target distribution. Self-Rewarding (Yuan et al., 2024a) introduces an innovative method using the language model itself as a reward model. It employs LLM-as-a-Judge prompting for self-generated responses, enabling autonomous reward assignment. This iterative process enhances instruction following and reward modeling capabilities.

This section delves into techniques for efficiently transferring knowledge from teacher LLMs to student models. Explored methods range from Supervised Fine-Tuning, Divergence and Similarity strategies that boost imitation, to advanced approaches like Reinforcement Learning.
Supervised Fine-Tuning (SFT), also known as Sequence-Level KD (SeqKD), is a straightforward yet effective method for knowledge distillation from powerful black-box Language Models (LLMs). It involves finetuning the student model to maximize the likelihood of sequences generated by the teacher LLMs, aligning the student's predictions with those of the teacher.

The divergence types used in this process are Forward KL Divergence, Reverse KL Divergence, and Jensen-Shannon (JS) Divergence, as outlined in Table 1. The similarity functions employed in knowledge distillation include L2-Norm Distance, L1-Norm Distance, Cross-Entropy Loss, and Maximum Mean Discrepancy (MMD), as summarized in Table 2.
LSFT is an objective function formulated as minimizing Exâˆ¼X,yâˆ¼pT(y|x)[âˆ’logpS(y|x)], where y is the output sequence produced by the teacher model. This technique has been widely used and proven effective in numerous studies. Researchers have successfully employed SFT to train student models using sequences generated by teacher LLMs (Taori et al., 2023; Chiang et al., 2023; Wu et al., 2023c; Xu et al., 2023a; Luo et al., 2023b). SFT has also been explored in many self-distillation works (Wang et al., 2022a; Huang et al., 2023c; Xu et al., 2023b; Zelikman et al., 2022). This section focuses on algorithms for distilling feature knowledge from white-box teacher LLMs, including distributions and hidden state features. These algorithms can be categorized into two main groups.
processed text:

Divergence-based methods in machine learning minimize the difference between the probability distributions of teacher and student models. This is represented by a general divergence function D, as shown in equation (10): LDiv = E xâˆ¼X,yâˆ¼Y[D(pT(y|x), pS(y|x))]. The type of divergence used varies, with specific forms outlined in Table 1.

The standard Knowledge Distillation (KD) objectives typically minimize the approximated forward Kullback-Leibler divergence (KLD) between the teacher and student distributions (Sanh et al., 2019). Figure 6 compares Forward and Reverse KL Divergences in approximating a target distribution. The Forward KL divergence method covers all modes of the target distribution but is less precise, exhibiting "mode-covering" behavior. In contrast, the Reverse KL divergence method focuses on precision.
The research indicates that models tend to prioritize the most prominent mode, displaying "mode-seeking" behavior (Wen et al., 2023; Timiryasov and Tastet, 2023; Liang et al., 2023a; Chen et al., 2024d). This forces the student model to cover all modes of the teacher's distribution. However, when a student model struggles to learn all modes of a highly complex teacher, it may exhibit "mode-covering" behavior (Figure 6 blue curve), potentially leading to hallucinations and low-quality generations.

As an alternative, mode-seeking divergences like reverse KL prioritize tokens where the teacher assigns high probabilities (Figure 6 green curve). This method can reduce the risk of low-quality outputs, promoting more accurate generations. Nevertheless, it often comes at the expense of reduced diversity. Gu et al. (2024) utilize reverse KL divergence to prevent students from overestimating.
Agarwal et al. (2024) and Sason & VerdÃº (2016) studied the impact of divergence functions in Language Model (LLM) distillation, revealing task-dependent optimal divergence. Forward KL divergence is favored for tasks like Machine Translation due to fewer output modes, while reverse KL divergence suits tasks such as dialogue generation and instruction tuning, which require multiple modes and varied responses. Task characteristics thus play a crucial role in choosing the best divergence function for peak performance.

Similarity-based methods in knowledge distillation focus on aligning the student model's hidden states or features with the teacher's. They employ different similarity metrics to gauge and enhance the congruence of internal representations between models.
Processed Text:

Two models are compared using a similarity-based objective. This ensures the student model not only generates similar outputs to the teacher but also processes information in a comparable way. The formula for this objective is LSim= E xâˆ¼X,yâˆ¼Y[LF(Î¦T(fT(x, y)),Î¦S(fS(x, y)))], where fT(x, y) and fS(x, y) are the feature maps of the teacher and student models respectively. Transformation functions Î¦T and Î¦S are applied to these feature maps to ensure they match in shape for direct comparison. The similarity function LF is then used to compare these transformed feature maps. Table 2 lists common choices for LF. Few studies have used similarity-based methods in the Knowledge Distillation (KD) of Large Language Models (LLMs). Among them, Liang et al. (2023a) introduced Task-Aware Layer-Wise Distillation (TED), a method that employs task-aware filters. These filters selectively capture the most relevant information for a specific task from the teacher model. The main goal is to minimize this process.
discrepancy in filtered representations between teacher and student models in encoder-based LMs (Sun et al., 2019, 2020; Jiao et al., 2020; Hou et al., 2020; Zuo et al., 2022; Liang et al., 2021). Despite their common use in similarity-based approaches, RL methods for LLM knowledge distillation are less prevalent. Nevertheless, expected growth in research for these techniques due to their effectiveness.

3.2.3 Reinforcement Learning: This section delves into advanced knowledge distillation methods using reinforcement learning (RL). RL is particularly beneficial for utilizing teacher feedback to train student models (Bai et al., 2022a; Cui et al., 2023a; Luo et al., 2023b; Agarwal et al., 2024; Chen et al., 2024b; Ma et al., 2023a; Pang et al., 2023; Du et al., 2023a). The RL-based distillation process generally comprises two stages: Distilled Reward Model Training.
Involves training a reward model, rÏ•, using feedback data D(fd) generated by teacher language models (LLMs). Preference data, a common type of feedback, is used to train the student reward model (Bai et al., 2022a; Cui et al., 2023a; Lee et al., 2023a; Kim et al., 2023a). This preference data typically consists of input-output pairs (x, yw, yl), where ywandylrepresent "winning" and "losing" outputs relative to the teacher's preferences.

The loss function for the reward model is defined as: LRM(rÏ•,D(fd)) =âˆ’ E (x,yw,yl)âˆ¼D(fd)[logÏƒ(rÏ•(x, yw)âˆ’rÏ•(x, yl))] (12)

This formulation guides the reward model to accurately differentiate between more and less preferable outputs based on the teacher's criteria. Unlike learning instance-level rewards, RLMEC (Chen et al., 2024b) employs a different strategy by training a generative reward model. This model is trained on erroneous solution rewriting data distilled from a teacher LLM. The resulting distilled reward model can generate token-level rewards for reinforcement learning training.
In the second stage, the student model, symbolized by policy Ï€Î¸, is optimized to maximize expected rewards based on the trained reward model. Simultaneously, it minimizes divergence from a reference policy Ï€ref, usually the initial policy of the student model trained via SFT, influenced by a factor Î². The RL objective is expressed as: max Ï€Î¸E xâˆ¼X,yâˆ¼Ï€Î¸(y|x)[rÏ•(x, y)]âˆ’Î²DKL[Ï€Î¸(y|x)âˆ¥Ï€ref(y|x)]. This RL framework ensures the student model learns explicit content from the teacher and effectively adopts the teacher's preference patterns. The use of RL, particularly with the PPO (Schulman et al., 2017) algorithm, provides a robust method for aligning the student model's outputs with the teacher. Alternatively, the teacher LLM can directly assign rewards during RL, bypassing the need to train a reward model (Lee et al., 2023a; Kwon et al., 2023). This approach may offer superior performance.
Performance, however, comes with a higher computational cost when using larger distilled reward models. Ranking optimization offers a stable and computationally efficient alternative to reinforcement learning for integrating preference feedback into language models (Rafailov et al., 2023; Song et al., 2023a; Yuan et al., 2023b). Unlike traditional RL methods, ranking optimization directly integrates ranking data from a fixed preference dataset during fine-tuning. This approach updates the policy to boost the relative likelihood of preferred responses over less favored ones. The direct optimization of preferences, without requiring output sampling, enhances stability and efficiency. Recent studies have explored using ranking optimization to distill teacher's preferences into student models (Tunstall et al., 2023; Hong et al., 2023; Yuan et al., 2024a). Zephyr (Tunstall et al., 2023) employs a direct method.
Preference Optimization (DPO) simplifies reinforcement learning's objective, involving reward maximization with a KL-divergence constraint, into single-stage policy training. DPO aims to maximize the expectation: E(x,yw,yl)~D(fd) [logÏƒ(Î²logÏ€Î¸(yw|x)/Ï€ref(yw|x) - Î²logÏ€Î¸(yl|x)/Ï€ref(yl|x))], where yw is preferred over yl by the teacher LLM.

Hong et al. (2023) use two ranking-based optimization objectives for preference distillation: Rank Responses to align Human Feedback (RRHF) and Preference Ranking Optimization (PRO). RRHF, defined as LRRHF = Î£ ri<rj max(0, pi - pj), focuses on a ranking loss. Here, ri and rj are reward scores from the teacher LLM for responses yi and yj, respectively, and pi and pj are their conditional log probabilities.
processed text:

PRO (Song et al., 2023a) introduces a training objective, LPRO, for ranking responses based on teacher preferences. For an instruction x and teacher-preferred sequence y1â‰»y2â‰»...â‰»yn, LPRO is defined as: LPRO = -âˆ‘(n-1)/k=1 logexp(pk)/Pn i=kexp(pi). Here, pk denotes the conditional log probabilities for yk under the student policy Ï€Î¸. This method contrasts response likelihoods iteratively, optimizing the student LM to favor the most preferred response and rank the others in order of decreasing preference.

Next, we delve into skill distillation in large language models (LLMs), building on the knowledge elicitation and distillation algorithms discussed earlier. Our examination will cover a broad range of skills.
range of skills in Large Language Models (LLMs): Context Following, Alignment, Agent, NLP Task Specialization, and Multi-Modality.

Context Following: evaluates a model's ability to understand and respond effectively to input information.

Alignment: assesses the model's capacity to align its output with the teacher's responses.

Agent: emphasizes the autonomous nature of language models.

NLP Task Specialization: showcases LLMs' versatility in adapting to various Natural Language Processing tasks.

Multi-Modality: involves knowledge transfer from teacher LLMs to multi-modal models.

Table 3 provides a summary of representative works, detailing skills, seed knowledge, teacher LLM, student model, knowledge elicitation method, and training objectives.

4.1 Context Following: This section focuses on distilling context following skills from LLMs, which involves...
transferring complex context handling abilities from LLMs to smaller models, including few-shot demonstrations, detailed instructions, dialogue history, and retrieval-augmented information. Research focuses on equipping smaller models with these advanced, context-aware skills. This discussion explores skill distillation types based on various contexts and explains their integration into compact, efficient models.

4.1.1 Instruction Following: This capability lets LLMs comprehend and execute user instructions, improving human-AI interaction by accurately understanding and executing tasks as instructed. Acquiring this skill involves creating instruction-like prompt-response pairs and using Supervised Fine Tuning (SFT) for model training.
Processed Text:

1. Methods: Skill Seed Knowledge
   - Teacher LLM, Student Model
   - Knowledge Elicitation, Objective Context Following
   - Self-Instruct (Wang et al., 2022a) uses 175 human-curated tasks with GPT3
   - Alpaca (Taori et al., 2023) employs 175 human-curated tasks with GPT3 and LLaMA Expansion + Self-Knowledge SFT

2. Other Methods:
   - LaMini-LM (Wu et al., 2023c): Uses IF3.5K Wikipedia Categories + Mixed Dataset, ChatGPT, various models, Expansion SFT
   - WizardLM (Xu et al., 2023a): Uses Alpaca Data, ChatGPT, LLaMA, Expansion SFT
   - Lion (Jiang et al., 2023b): Uses Alpaca Cata, ChatGPT, LLaMA, Labeling + Expansion + Feedback
   - BabyLlama (Timiryasov and Tastet, 2023): Utilizes 10M-word BabyLM dataset, GPT-2 + small LLaMA 58M-parameter LLaMA Feature D&S
   - MiniLLM (Gu et al., 2024): Uses Dolly Dataset, GPT2 + OPT + LLaMA, Feature D&S
   - Self-Align (Sun et al., 2024b): Employs Human-written Principles, LLaMA, Expansion + Self-Knowledge SFT
Self-Rewarding Models:

1. Yuan et al. (2024a): Human-written Samples, LLaMA, Self-Knowledge, SFT + RL, STaR (Zelikman et al., 2022)
2. Zelikman et al. (2022): Arithmetic + CommonsenseQA + GSM8K, GPT-J, Self-Knowledge, SFT
3. Peng et al. (2023a): Alpaca Dataset, GPT4, LLaMA Labeling, SFT
4. Li et al. (2023e): Alpaca/WizardLM Dataset, ChatGPT, LLaMA Labeling, SFT, Reflection-Tuning
5. Li et al. (2024d): Alpaca/WizardLM Dataset, ChatGPT, LLaMA Labeling, SFT, Selective Reflection-Tuning
6. Chiang et al. (2023): Human Conversation, ChatGPT + GPT4, LLaMA Labeling, SFT
7. Geng et al. (2023): Human Conversation, ChatGPT, LLaMA Labeling, SFT
8. Xu et al. (2023b): Quora + Stack Overflow, ChatGPT, LLaMA Expansion + Self-Knowledge, SFT
9. Ding et al. (2023b): Wikidata + Text Material + C4, ChatGPT, LLaMA Curation, SFT
10. Mukherjee et al. (2023): FLAN-v2, ChatGPT + GPT4, LLaMA Labeling, SFT
11. Mitra et al. (2023): FLAN-v2 + Few-Shot/Math/Synthetic, GPT4, LLaMA Labeling, SFT
Processed Text:

- et al., 2023: Human Conv, Flan/Code/Math Collection ChatGPT LLaMA Labeling SFT CoT-Distill
- Hsieh et al., 2023: e-SNLI + ANLI + CQA + SVAMP PaLM T5 Labeling SFT KnowPAT
- Zhang et al., 2023a: CPKG + QA Data ChatGPT + ChatGLM + Vicuna-7B LLaMA Labeling SFT
- Li et al., 2024e: Controversial Topics ChatGPT LLaMA Labeling SFT
- Gunasekar et al., 2023: - GPT3.5 phi-1 Curation SFT Phi-1
- Li et al., 2023a: 20k Topics from Web GPT3.5 phi-1 Curation + Labeling SFT
- Luo et al., 2023c: Alpaca Data + Web Content GPT4 LLaMA Label SFT KARD
- Kang et al., 2023b: MedQAUSMLE ChatGPT T5 + OPT Label SFT + D&S
- Asai et al., 2023: Open-Instruct GPT4 LLaMA Labeling SFT Alignment
- Wang et al., 2023c: Human Conversation ChatGPT + GPT4 LLaMA Labeling SFT + RL Zephyr
- Tunstall et al., 2023: Mixed Datasets GPT4 Mistral Labeling + Feedback SFT + RO
- Kim et al., 2023a: Preference (Details not provided)
Processed Text:

1. Human-written Prompts LLaMA, Expansion + Labeling SFT + RL RLCD (Yang et al., 2024) - IF/Preference
2. Human-written Prompts PaLM 2, Labeling + Feedback RL GPT3 Reward (Kwon et al., 2023) - Preference
3. Human-written Prompts GPT3, Labeling RL ILF (Scheurer et al., 2023) - Preference
4. Task-specific Datasets GPT3 + FeedME, Labeling RL ULTRAFEEDBACK (Cui et al., 2023a) - Preference
5. Mixed Datasets GPT4 LLaMA, Labeling RL Constitutional AI (Bai et al., 2022a) - Preference/Value
6. Human-written Prompts Self-defined Student Model, Labeling + Expansion + Feedback SFT + RL SANDBOX (Liu et al., 2023b) - Value Simulation
7. LLaMA Data Curation SFT + RL Agent Toolformer (Schick et al., 2023) - Tool
8. CCNet GPT-J, Labeling SFT Graph-ToolFormer (Zhang, 2023) - Tool
9. ChatGPT GPT-J + LLaMA, Labeling SFT Gorilla (Patil et al., 2023) - Tool Online
API Documentation:
- GPT4 LLaMA Expansion, SFT, Tools by Yang et al. (2023b), Image Content, ChatGPT, LLaMA Curation + Expansion
- Alpaca (Tang et al., 2023a), Tool from Public-apis Repository, ChatGPT, LLaMA Curation, SFT
- LLM (Qin et al., 2023a), Tool for Real-world APIs, ChatGPT, LLaMA Curation, SFT
- MLLM-Tool (Wang et al., 2024), HuggingFace Model Cards, GPT4, LLaMA Curation, SFT
- FireAct (Chen et al., 2023b), Planning for Mixed QA Dataset, GPT4, LLaMA Labeling, SFT
- AgentTuning (Zeng et al., 2023a), Planning for 6 Agent Tasks, GPT4 + ChatGPT, LLaMA Labeling + Expansion, SFT
- Lumos (Yin et al., 2023a), Planning for Mixed Interactive Tasks, GPT4, LLaMA Labeling, SFT
- AUTOACT (Qiao et al., 2024), Planning for Mixed QA Tasks, LLaMA, LLaMA Labeling, SFT
- NLP Task Specialization AugGPT (Dai et al., 2023a), NLU Amazon/Symptoms/PubMed20k Dataset, ChatGPT, BERT, Label, SFT
- TDG (He et al., 2023b), NLU SST + QQP + MNLI, GPT3, BERT, Expansion, SFT
- SunGen (Gao et al., 2023a), NLU Text Classification Tasks, GPT2, DistilBERT, Curation, SFT
- UDG (Wang et al.), NLP Task Unknown Dataset, GPT4/3/2, LLaMA/BERT, Labeling/Curation, SFT
Processed Text:

- NLU Tasks: GPT3, BERT, Expansion, SFT, InheritSumm (Xu et al., 2023c)
- NLG: Pile + ArXiv + CNN/DM + WikiHow, GPT3.5, ZCode++ Label SFT, DIMSUM+ (Jung et al., 2023)
- NLG/NLU/IF: XSum+WMT14 en-de+GSM8K+FLAN2021, T5-XL, Feature + Feedback D&S + RL
- IR Datasets: T5 4-layer Transformer, Internal Knowledge D&S
- Recommendation: GPT3 MPnet-110M, InstrcutRec (Zhang et al., 2023b), ChatGPT LLaMA Labeling SFT, ONCE (Liu et al., 2023c)
PandaLM, Alpaca Data, ChatGPT, LLaMA, Labeling, SFT

Alpaca Evaluation: GPT4, LLaMA, Labeling, SFT

Prometheus (Kim et al., 2024) Evaluation: 50 Seed Rubrics, GPT4, LLaMA, Labeling, SFT

InstructScore (Xu et al., 2023d) Evaluation: Mixed Dataset, GPT4, LLaMA, Labeling, SFT

WizardMath (Luo et al., 2023b): Math, GSM8k + MATH, ChatGPT, LLaMA, Expansion + Feedback, SFT + RL

Mammoth (Yue et al., 2023a): Math/TP, Mixed Math Dataset, GPT4, LLaMA, Labeling, SFT

Mixed Distill (Chenglin et al., 2023): Math/TP, SVAMP + GSM8K + ASDIV + StrategyQA, ChatGPT, LLaMa, Labeling, SFT

WizardCoder (Luo et al., 2023a): Code, Alpaca Data, ChatGPT, StarCoder Expansion, SFT

Magicoder (Wei et al., 2023): Code, Existing Source Codes, ChatGPT, LLaMa, Curation, SFT

WaveCoder (Yu et al., 2024): Code, Existing Source Codes, GPT4, LLaMa, Curation, SFT

Code Alpaca (Chaudhary, 2023): Code, Code Instructions, ChatGPT, LLaMA, Expansion + Self-Knowledge, SFT

Code Llama (Rozi `ere et al., 2023): Code, Human-written Instructions, LLaMA, LLaMA Expansion + Self-Knowledge, SFT
Processed Text:

Here's a summary of notable skill distillation works:

1. **Code**: Utilizes Code datasets and employs Supervised Fine-Tuning (SFT) with ChatGPT and LLaMA.
2. **Datasets**: Employs COCO, Visual Genome + COCO, LVIS for Vision-Language tasks, utilizing GPT4 and LLaMA with SFT.
3. **Macaw-LLM**: Handles multiple modalities (Image/Video with Caption), using ChatGPT and LLaMA for Labeling with SFT.
4. **MIMIC-IT**: Manages Multiple Modalities (Image/Video Dataset), leveraging ChatGPT and LLaMA for Labeling via SFT.
5. **ChatBridge**: Focuses on Multiple Modalities (Task-Specific/Multimodal-Chat Data), combining GPT4 + ChatGPT with LLaMA for Labeling using SFT.

These works involve Instruction Following (IF), Multi-turn Dialogue (MD), and Think Pattern (TP) techniques, sometimes incorporating Retrieval-Augmented Generation (RAG), Natural Language Understanding (NLU), Natural Language Generation (NLG), and Information Retrieval (IR). Supervised Fine-Tuning (SFT) is a common method used across these projects.
Processed Text:

Large Language Models (LLMs), such as GPT-4, provide an efficient method for generating diverse and controlled Supervised Fine-Tuning (SFT) data. This is due to their capabilities in in-context learning and instruction following. Most relevant studies utilize OpenAI's GPT series models to produce prompt-response data pairs. These pairs are then used to train student LLMs via supervised fine-tuning. Notable works include Wang et al., 2022a; Taori et al., 2023; Chiang et al., 2023; Wu et al., 2023c; Xu et al., 2023a; Mukherjee et al., 2023; Mitra et al., 2023; Luo et al., 2023b; Peng et al., 2023a.

Self-Instruct (Wang et al., 2022a) capitalizes on the in-context learning capability of these models.
Processed Text:

GPT-3 expands a seed pool of 15 tasks into 52,000 task-agnostic instructions, covering a wide range of general directions. A filtering and post-processing phase is implemented to discard redundant or similar instructions. Training with this enhanced dataset allows GPT-3 to follow instructions effectively, achieving comparable performance to InstructGPT in zero-shot instruction tasks and with expert-crafted instructions for unseen tasks.

Following the self-instruct method, Taori et al. (2023) train an Alpaca model using the Llama 7B model on 52,000 instruction-following demonstrations. These are generated in a style similar to self-instruct but leveraging the more reliable text-davinci-003 model. To broaden the variety of instructional data, Wu et al. (2023c) introduce Topic-Guided Instruction Generation. This technique gathers 3,500 common topics from Wikipedia to guide the generation process.
Processed Text:

Xu et al. (2023a) developed Evol-Instruct to boost smaller models' ability to handle complex instructions. This method evolves simple instructions into more intricate forms through a multi-step process, enhancing difficulty levels and topic diversity. They used the OpenAI ChatGPT API for four rounds of evolution, generating a 250k complex instruction dataset. Trained on this, WizardLM (LLAMA 7B) surpassed ChatGPT in high-difficulty test sections, boasting a 7.9% higher win rate. Zhao et al. (2023e) continued...
Processed Text:

Preliminary studies indicate the effectiveness of enhancing instruction complexity. Instruction Fusion (Guo et al., 2023c) builds upon this by leveraging teacher LLMs to boost complexity through merging two separate evolved instructions. This "evolving" instructions concept has also been applied to hone specific abilities, including coding (Luo et al., 2023a) and mathematics (Luo et al., 2023b).

Unlike methods that depend on ChatGPT-generated instructions, which may lack diversity and diverge from actual human instructions, Vicuna (Chiang et al., 2023) and Koala (Geng et al., 2023) demonstrate remarkable performance. They utilize human conversations and natural instructions sourced from community-driven discussions on platforms like ShareGPT. These interactions offer a platform for users to share their ChatGPT experiences.

However, it's crucial to acknowledge that models trained on such organic conversations might adopt the style but may not wholly encapsulate human instruction nuances.
Gudibande et al. (2023; Mukherjee et al., 2023) and Mitra et al. (2023) developed methods to enhance student models' understanding of the reasoning process. Orca and Orca 2 introduce a system message, such as "explain like I'm five, think step-by-step," to encourage GPT-4 to provide explanation traces that reveal the teacher's reasoning process. Orca 2 further trains student models to identify the most effective solution strategy for each task, guided by Orca's performance. This approach significantly improves smaller models' ability to follow instructions involving reasoning.

High-quality instructions are essential for instruction following training, as shown in Zhou et al. (2023a) and Li et al. (2024f). UltraChat (Ding et al.,) emphasizes the importance of data quality in this context.
Processed Text:

The UltraLLaMA model, distilled from large-scale data with high-quality, diverse instructions from teacher LLMs using various meta-information, outperforms other open-source models. The Phi series models (Gunasekar et al., 2023; Li et al., 2023a; Mar, 2023) emphasize data quality and use synthetic methods to create "textbook quality" data for better learning experiences in smaller models. Remarkably, Phi-2, with only 2.7 billion parameters, surpasses Mistral and Llama-2 models with 7B and 13B parameters across various benchmark evaluations.

Another approach enhances the quality of existing instruction data, including both instructions and responses. SelFee (Ye et al., 2023) employs ChatGPT to iteratively improve these aspects.
ExpertLLaMA (Xu et al., 2023f) enhances response quality by incorporating specialized Expert Identity descriptions into vanilla instructions. Reflection-Tuning (Li et al., 2023e) refines both instructions and responses iteratively, focusing on specific criteria. DEITA (Liu et al., 2023h) suggests scoring instructions across complexity, quality, and diversity for superior distillation data. MUFFIN (Lou et al., 2023) scales instructions based on input diversity. Selective Reflection-Tuning (Li et al., 2024d) integrates a student model into the data enhancement process via a unique selection module, enabling the model to choose learning data. In essence, distilling instruction data from teachers offers a promising method for training affordable and reproducible instruction-following language models.
Current small models have improved instruction-following abilities, including diversity, complexity, and explanation. However, student models trained on expanded instruction data from ChatGPT often mimic its style without replicating factual accuracy (Gudibande et al., 2023). To achieve a more capable instruction-following capability, a stronger teacher LLM is needed along with access to diverse, high-quality instruction data, similar to what's used in Orca (Mukherjee et al., 2023; Mitra et al., 2023). This data incorporates extensive task instructions from the Flan 2022 Collection (Longpre et al., 2023).

Multi-turn dialogue, unlike instruction following which focuses on single-instance command execution, involves understanding and maintaining context through ongoing interactions. This skill is crucial for models to engage in human-like conversations and respond coherently over successive turns.
Processed Text:

Several studies focus on training small chat models by extracting multi-turn knowledge from larger language models (LLMs). Platforms like ShareGPT facilitate this by providing a large collection of multi-turn conversations for model training. Models such as Vicuna (Chiang et al., 2023) are trained exclusively on ShareGPT data, demonstrating high performance in multi-turn dialogues, as indicated by their MT-Bench scores. In research by Wang et al. (2023c), GPT-3.5 and GPT-4 generate mixed responses using ShareGPT data, with GPT-4 responses receiving higher rewards.
aiming to incentivize student models for superior responses. Ye et al. (2023) improve ShareGPT's multi-turn data quality via self-feedback on model responses, iteratively refining them based on feedback. Enhancing student models' multi-turn capabilities involves expanding conversational datasets through self-chat and training smaller models. Xu et al. (2023b) started with Quora/Stack Overflow questions as seeds, gathering 111.5k dialogues via self-chat. They then used parameter-efficient tuning to train Baize, a chat model. Ding et al. (2023b) built UltraChat, a dataset of 1.5 million high-quality multi-turn dialogues, by distilling instructions and...
Processed Text:

UltraChat covers a broad spectrum of topics and instructions. Using UltraChat, they fine-tune a LLaMA model, leading to the development of a robust chat model called UltraLLaMA. This model surpasses other open-source chat models such as Vicuna and Baize in performance. 

UltraChat is also utilized alongside an AI preference-aligned chat model named Zephyr (Tunstall et al., 2023). Zephyr improves intent alignment via the use of distilled direct preference optimization (dDPO).

Large Language Models (LLMs) often struggle with up-to-date knowledge, sometimes generating factually incorrect responses due to their exclusive dependence on stored knowledge. Retrieval-Augmented Generation (RAG) is a method aimed at mitigating this problem by incorporating additional context.
processed text:

Several methods have been suggested to enhance Retrieval-Augmented Generation (RAG) capabilities in large language models (LLMs). SAIL (Luo et al., 2023c) initiates by retrieving search results for each training case via search APIs, forming search-augmented instructions that incorporate both the instruction and grounding data. To prompt the language model to focus on pertinent retrieval outcomes, they input each retrieved passage alongside the ground truth response into an entailment model for relevance labeling. These labeled results, along with the search-augmented instructions, are then used by teacher LLMs (such as GPT-4) to generate responses. After fine-tuning on this dataset, the student model adeptly refines search results and produces precise answers. KARD (Kang et al., 2023b) distills rationales from the teacher LLM in response to questions.
During training, rationales are employed to fine-tune two models: a student Language Model (LM) and a Reranker. The student LM uses rationales to fetch relevant knowledge 'd', then gets fine-tuned with rationales, questions, and knowledge. However, during inference, only questions are accessible. To tackle this, the Reranker is trained to imitate how the retriever ranks passages using the rationale, minimizing the KL divergence between Retriever(d|r) and Reranker(d|x). Yet, incorporating a fixed number of passages in language models, disregarding their necessity or relevance, can diminish versatility and result in unhelpful responses. To endow student LMs with adaptable Retrieval-Augmented Generation (RAG) capabilities, Self-Rag distills this adaptability from larger Language Learning Models into a compact critic model. This critic model assesses the necessity of retrieval and appraises the quality of retrieved results by producing outputs.
Processed Text:

Self-Rag triggers retrieval when creating reflection tokens. GPT-4 evaluates the need for retrieval using few-shot examples, task input x, and output y to forecast a reflection token r via p(r|I, x, y). Traditional methods primarily align student model responses directly with teacher model responses (Taori et al., 2023). While effective, these models may mimic the teacher's response style rather than the reasoning process (Mukherjee et al., 2023). To improve distillation from teacher models, methods are suggested that not only imitate responses but also novel thinking patterns (Ye et al., 2023; Mukherjee et al., 2023; Mitra et al., 2023; Wang et al., 2023d; Cheng et al., 2023; Zhang et al., 2023a).
SelFee (Ye et al., 2023) introduces a method for training a fine-tuned model to continuously revise its own answer until it generates a high-quality response in one inference. It uses both the final response and feedback chain during training. This approach, combining response with revision process, demonstrates promising performance improvements. Reflection-Tuning (Li et al., 2023e, 2024d) later adopts this reflection pattern for learning. Recognizing the absence of reasoning imitation in prior methods, Orca (Mukherjee et al., 2023) proposes Explanation tuning. This method aims to learn reasoning steps, including explanation traces and step-by-step thought processes, from a teacher model, rather than just basic styles.
processed text:

The effectiveness of distilling with a specific thinking pattern is verified through experiments. Orca2 (Mitra et al., 2023) introduces the idea of equipping student models with diverse solution strategies for different tasks, inspired by model capability discrepancies between smaller and larger ones. This training method enhances student models' reasoning abilities.

Apart from learning with revision or reflection processes, a new thinking pattern involves generating both responses and preferences. Zhang et al. (2023a) propose learning domain-specific QA knowledge alongside corresponding preferences using LLMs. DEBATunE (Li et al., 2024e) aims to improve the controllability of LLMs in generating statements on contentious topics through structured multi-round debates between agents, yielding salient and in-depth statements.
4.2.2 Preference: Prior approaches emphasize student models' accuracy, potentially disregarding human preference alignment. Achieving this level of alignment allows models to assist in diverse tasks without fulfilling more stringent requirements. Initial techniques predominantly employ human feedback.
