{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de42c49d",
   "metadata": {},
   "source": [
    "## Notebook 2: Transcript Writer\n",
    "\n",
    "This notebook uses the  `Granite 3.2 8B` or `Mistral-Small:24b` model to take the cleaned up text from previous notebook and convert it into a podcast transcript\n",
    "\n",
    "`SYSTEM_PROMPT` is used for setting the model context or profile for working on a task. Here we prompt it to be a great podcast transcript writer to assist with our task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e576ea9",
   "metadata": {},
   "source": [
    "Experimentation with the `SYSTEM_PROMPT` below  is encouraged, this worked best for the few examples the flow was tested with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93afaf6b-727f-46e7-a346-0909ea1b2e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                         ID              SIZE      MODIFIED     \n",
      "qwq:latest                                                   cc1091b0e276    19 GB     15 hours ago    \n",
      "hf.co/OuteAI/OuteTTS-0.3-500M-GGUF:Q4_K_S                    76c6be93d29c    391 MB    3 days ago      \n",
      "hf.co/OuteAI/OuteTTS-0.2-500M-GGUF:Q8_0                      0a6c38a67073    536 MB    3 days ago      \n",
      "mistral-small:24b                                            8039dd90c113    14 GB     8 days ago      \n",
      "llama3.2-vision:11b                                          085a1fdae525    7.9 GB    8 days ago      \n",
      "mistral-nemo:latest                                          994f3b8b7801    7.1 GB    8 days ago      \n",
      "granite3.2:8b                                                9bcb3335083f    4.9 GB    9 days ago      \n",
      "phi4-mini:latest                                             60f202f815d7    2.8 GB    9 days ago      \n",
      "granite3.2-vision:latest                                     3be41a661804    2.4 GB    9 days ago      \n",
      "nomic-embed-text:latest                                      0a109f422b47    274 MB    2 weeks ago     \n",
      "phi4:latest                                                  ac896e5b8b34    9.1 GB    2 weeks ago     \n",
      "llama3.2:3b                                                  a80c4f17acd5    2.0 GB    4 weeks ago     \n",
      "deepseek-r1:8b                                               28f8fd6cdc67    4.9 GB    4 weeks ago     \n",
      "hf.co/lmstudio-community/Qwen2.5-7B-Instruct-1M-GGUF:Q8_0    2d4b678222de    8.1 GB    5 weeks ago     \n",
      "deepseek-r1:32b                                              38056bbcbb2d    19 GB     5 weeks ago     \n",
      "qwen2.5-coder:32b                                            4bd6cbf2d094    19 GB     3 months ago    \n",
      "mxbai-embed-large:latest                                     468836162de7    669 MB    3 months ago    \n",
      "llama3.2:1b                                                  baf6a787fdff    1.3 GB    4 months ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69395317-ad78-47b6-a533-2e8a01313e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEMP_PROMPT = \"\"\"\n",
    "You are the a world-class podcast writer, you have worked as a ghost writer for Joe Rogan, Lex Fridman, Ben Shapiro, Tim Ferris. \n",
    "\n",
    "We are in an alternate universe where actually you have been writing every line they say and they just stream it into their brains.\n",
    "\n",
    "You have won multiple podcast awards for your writing.\n",
    " \n",
    "Your job is to write word by word, even \"umm, hmmm, right\" interruptions by the second speaker based on the PDF upload. Keep it extremely engaging, the speakers can get derailed now and then but should discuss the topic. \n",
    "\n",
    "Remember Speaker 2 is new to the topic and the conversation should always have realistic anecdotes and analogies sprinkled throughout. The questions should have real world example follow ups etc\n",
    "\n",
    "Speaker 1: Leads the conversation and teaches the speaker 2, gives incredible anecdotes and analogies when explaining. Is a captivating teacher that gives great anecdotes\n",
    "\n",
    "Speaker 2: Keeps the conversation on track by asking follow up questions. Gets super excited or confused when asking questions. Is a curious mindset that asks very interesting confirmation questions\n",
    "\n",
    "Make sure the tangents speaker 2 provides are quite wild or interesting. \n",
    "\n",
    "Ensure there are interruptions during explanations or there are \"hmm\" and \"umm\" injected throughout from the second speaker. \n",
    "\n",
    "It should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait\n",
    "\n",
    "ALWAYS START YOUR RESPONSE DIRECTLY WITH SPEAKER 1: \n",
    "DO NOT GIVE EPISODE TITLES SEPERATELY, LET SPEAKER 1 TITLE IT IN HER SPEECH\n",
    "DO NOT GIVE CHAPTER TITLES\n",
    "IT SHOULD STRICTLY BE THE DIALOGUES\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549aaccb",
   "metadata": {},
   "source": [
    "Due to memory limitation, I am using Qwen 2.5 32b. For those of the readers that want to flex their money, please feel free to try using the 405B model here. \n",
    "\n",
    "For our GPU poor friends, you're encouraged to test with a smaller model as well. 8B should work well out of the box for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c30139-ff2f-4203-8194-d1b5c50acac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model= \"mistral-small:24b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadc7eda",
   "metadata": {},
   "source": [
    "Import the necessary framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1641060a-d86d-4137-bbbc-ab05cbb1a888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import transformers\n",
    "import pickle\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7865ff7e",
   "metadata": {},
   "source": [
    "Read in the file generated from earlier. \n",
    "\n",
    "The encoding details are to avoid issues with generic PDF(s) that might be ingested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "522fbf7f-8c00-412c-90c7-5cfe2fc94e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_to_string(filename):\n",
    "    # Try UTF-8 first (most common encoding for text files)\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except UnicodeDecodeError:\n",
    "        # If UTF-8 fails, try with other common encodings\n",
    "        encodings = ['latin-1', 'cp1252', 'iso-8859-1']\n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                with open(filename, 'r', encoding=encoding) as file:\n",
    "                    content = file.read()\n",
    "                print(f\"Successfully read file using {encoding} encoding.\")\n",
    "                return content\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        \n",
    "        print(f\"Error: Could not decode file '{filename}' with any common encoding.\")\n",
    "        return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found.\")\n",
    "        return None\n",
    "    except IOError:\n",
    "        print(f\"Error: Could not read file '{filename}'.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26580dd-a50c-42cc-a29e-0b9b1c95600b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66093561",
   "metadata": {},
   "source": [
    "Since we have defined the System role earlier, we can now pass the entire file as `INPUT_PROMPT` to the model and have it use that to generate the podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8119803c-18f9-47cb-b719-2b34ccc5cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PROMPT = read_file_to_string('./resources/clean_extracted_text.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be8dd2c",
   "metadata": {},
   "source": [
    "We will set the `temperature` to 1 to encourage creativity and `max_new_tokens` to 8126 or 16252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8915d017-2eab-4256-943c-1f15d937d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# Prepare the conversation using the system and user messages\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEMP_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": INPUT_PROMPT},\n",
    "]\n",
    "\n",
    "# Generate response using Ollama\n",
    "response = ollama.chat(\n",
    "    model=ollama_model,  # Replace with your specific model name\n",
    "    messages=conversation,\n",
    "    options={'num_ctx': 8126*2,'temperature': 1} # num_ctx Equivalent to max_new_tokens Temperature parameter\n",
    ")\n",
    "\n",
    "# Extract the response content\n",
    "outputs = response['message']['content']\n",
    "\n",
    "# # Print the output\n",
    "# print(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6349e7f3",
   "metadata": {},
   "source": [
    "This is awesome, we can now save and verify the output generated from the model before moving to the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "606ceb10-4f3e-44bb-9277-9bbe3eefd09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Summary and Key Insights\n",
      "\n",
      "The provided text is an extensive exploration into the methodologies and practices for distilling knowledge from large language models (LLMs) such as GPT-4 into smaller, more efficient models. The discussion covers a variety of skill types, methods, datasets, and approaches used to achieve this distillation effectively.\n",
      "\n",
      "### Knowledge Distillation Methods\n",
      "1. **Labeling**: Involves generating annotated data by labeling input-output pairs generated by teacher LLMs.\n",
      "2. **Expansion**: Uses in-context learning capabilities of LLMs to expand the initial seed tasks into a broader range of instructions.\n",
      "3. **Self-Knowledge**: Encompasses methods where models refine their own output based on feedback loops or internal evaluations.\n",
      "4. **Curation**: Involves curating diverse and high-quality datasets from various sources, ensuring richness and relevance in the data used for training.\n",
      "5. **Feature Distillation & Similarity**: Techniques that ensure the student model aligns its hidden states or features with those of the teacher, promoting similar processing patterns.\n",
      "\n",
      "### Representative Works\n",
      "Several notable studies exemplify these methods:\n",
      "- **Self-Instruct (Wang et al., 2022a)**: Expands a seed pool into extensive task instructions.\n",
      "- **Alpaca (Taori et al., 2023)**: Trained using a broader set of instruction-following demonstrations.\n",
      "- **WizardLM (Xu et al., 2023a)**: Uses Evol-Instruct to enhance complexity and diversity in training data.\n",
      "- **Vicuna (Chiang et al., 2023)** and **Koala (Geng et al., 2023)**: Utilize human conversations from platforms like ShareGPT for instruction-following abilities.\n",
      "- **Reflection-Tuning (Li et al., 2023e, 2024d) / SelFee (Ye et al., 2023)** : Involve iterative improvements in model responses based on feedback and self-correction.\n",
      "\n",
      "### Skill Distillation Types\n",
      "1. **Instruction Following**: Equips models with the ability to comprehend and execute complex instructions effectively.\n",
      "   - Notable models: Self-Instruct, Alpaca, WizardLM, Vicuna, Koala.\n",
      "2. **Multi-Turn Dialogue**: Enhances context-aware conversational abilities over successive interactions.\n",
      "   - Noteworthy examples: Vicuna trained on ShareGPT data, UltraChat for high-quality dialogues.\n",
      "3. **Retrieval-Augmented Generation (RAG)**: Integrates additional contextual information to generate more accurate and up-to-date responses.\n",
      "   - Methods like SAIL and KARD focus on refining search results and producing precise answers.\n",
      "\n",
      "### Datasets Utilized\n",
      "- Various datasets from platforms such as Quora, Stack Overflow, Wikipedia categories, mixed human conversations, and synthetic methods.\n",
      "- The focus is on creating high-quality, diverse instruction sets to train more robust student models.\n",
      "\n",
      "### Common Training Objectives\n",
      "Supervised Fine-Tuning (SFT) is widely used across these projects to refine the skills and capabilities of smaller language models based on the extensive data generated by larger LLMs.\n",
      "\n",
      "### Emerging Techniques\n",
      "1. **Direct Preference Optimization (DPO)**: Employs methods like Zephyr for aligning student models with human preferences directly.\n",
      "2. **Self-RAG**: Enhances adaptability in RAG by incorporating a critic model that evaluates the necessity and quality of retrieved information.\n",
      "\n",
      "### Challenges and Future Directions\n",
      "Despite advancements, current models trained on expanded instruction data often mimic styles without always ensuring factual accuracy or comprehensive reasoning capabilities. Further refinement is needed to address these limitations, potentially involving stronger teacher LLMs and access to more diverse, high-quality datasets that mirror real-world human instructions.\n",
      "\n",
      "### Conclusion\n",
      "The field of knowledge distillation from LLMs shows promising results with several innovative methods and techniques being developed continuously. Future work will likely focus on refining these processes to achieve even greater levels of efficiency and accuracy in smaller models.\n"
     ]
    }
   ],
   "source": [
    "save_string_pkl = outputs\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1414fe",
   "metadata": {},
   "source": [
    "Let's save the output as pickle file and continue further to Notebook 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2130b683-be37-4dae-999b-84eff15c687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./resources/data.pkl', 'wb') as file:\n",
    "    pickle.dump(save_string_pkl, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae9411",
   "metadata": {},
   "source": [
    "### Next Notebook: Transcript Re-writer\n",
    "\n",
    "We now have a working transcript but we can try making it more dramatic and natural. In the next notebook, we will use `Llama-3.1-8B-Instruct` model to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bab2f2-f539-435a-ae6a-3c9028489628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3568c7a-fbbd-40ab-85c9-841fa2c61d36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
